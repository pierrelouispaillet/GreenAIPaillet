IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

SUBSCRIBE

        Cart 
        Create Account
        Personal Sign In 

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Institutional Sign In
IEEE logo - Link to IEEE main site homepage
ADVANCED SEARCH
Conferences > 2016 IEEE International Confe...
Evaluating the Energy Efficiency of Deep Convolutional Neural Networks on CPUs and GPUs
Publisher: IEEE
Cite This
PDF
Da Li ; Xinbo Chen ; Michela Becchi ; Ziliang Zong
All Authors
67
Paper
Citations
1
Patent
Citation
2678
Full
Text Views

    Alerts

Abstract
Document Sections

    I.
    Introduction
    II.
    Background
    III.
    Methodology
    IV.
    Overall Energy Efficiency Results on CPU & GPU
    V.
    Effect of NN and Batch Size Configuration

Show Full Outline
Authors
Figures
References
Citations
Keywords
Metrics
Abstract:
In recent years convolutional neural networks (CNNs) have been successfully applied to various applications that are appropriate for deep learning, from image and video processing to speech recognition. The advancements in both hardware (e.g. more powerful GPUs) and software (e.g. deep learning models, open-source frameworks and supporting libraries) have significantly improved the accuracy and training time of CNNs. However, the high speed and accuracy are at the cost of energy consumption, which has been largely ignored in previous CNN design. With the size of data sets grows exponentially, the energy demand for training such data sets increases rapidly. It is highly desirable to design deep learning frameworks and algorithms that are both accurate and energy efficient. In this paper, we conduct a comprehensive study on the power behavior and energy efficiency of numerous well-known CNNs and training frameworks on CPUs and GPUs, and we provide a detailed workload characterization to facilitate the design of energy efficient deep learning solutions.
Published in: 2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom)
Date of Conference: 08-10 October 2016
Date Added to IEEE Xplore : 31 October 2016
ISBN Information:
INSPEC Accession Number: 16427226
DOI: 10.1109/BDCloud-SocialCom-SustainCom.2016.76
Publisher: IEEE
Conference Location: Atlanta, GA, USA
I. Introduction

The history of neural network research can be traced back to the second half of the last century. In 1958, Frank Rosenblatt, a psychologist, proposed the concept of Perceptron and a theory on the operation of neurons in the human brain [2]. This theory has led to the emergence of a new field of artificial intelligence, called neural networks. About thirty years later, Yann LeCun et al. [3] successfully applied neural networks to recognize handwritten checks and ZIP codes in mail. However, the training of their neural network required approximately three days. Later, neural networks became a core computation in various applications, from wake-sleep algorithms [4] to the vanishing gradient problem [5]. However, the high computational requirements of the training phase continue to be a key factor hindering the advancement of algorithms and applications based on neural networks. Recent advancements in software and hardware, including the use of high throughput GPU s to accelerate neural network training, have alleviated this problem. It is now possible to train large and complex neural networks in reasonable time on relatively inexpensive hardware. This has led to the rapid growth of neural network-based deep learning algorithms.
Sign in to Continue Reading
Authors
Figures
References
Citations
Keywords
Metrics
More Like This
Speedup of Learning in Interval Type-2 Neural Fuzzy Systems Through Graphic Processing Units

IEEE Transactions on Fuzzy Systems

Published: 2015
moDNN: Memory Optimal Deep Neural Network Training on Graphics Processing Units

IEEE Transactions on Parallel and Distributed Systems

Published: 2019
Show More
References
References is not available for this document.
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

Â© Copyright 2023 IEEE - All rights reserved.
