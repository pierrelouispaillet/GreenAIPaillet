arXiv:2108.07258v3 [cs.LG] 12 Jul 2022

On the Opportunities and Risks of Foundation Models

Rishi Bommasani* Drew A. Hudson Ehsan Adeli Russ Altman Simran Arora Sydney von Arx Michael S. Bernstein Jeannette Bohg Antoine Bosselut Emma Brunskill
Erik Brynjolfsson Shyamal Buch Dallas Card Rodrigo Castellon Niladri Chatterji Annie Chen Kathleen Creel Jared Quincy Davis Dorottya Demszky Chris Donahue Moussa Doumbouya Esin Durmus Stefano Ermon John Etchemendy Kawin Ethayarajh Li Fei-Fei Chelsea Finn Trevor Gale Lauren Gillespie Karan Goel Noah Goodman Shelby Grossman Neel Guha Tatsunori Hashimoto Peter Henderson John Hewitt
Daniel E. Ho Jenny Hong Kyle Hsu Jing Huang Thomas Icard Saahil Jain Dan Jurafsky Pratyusha Kalluri Siddharth Karamcheti Geoff Keeling Fereshte Khani
Omar Khattab Pang Wei Koh Mark Krass Ranjay Krishna Rohith Kuditipudi Ananya Kumar Faisal Ladhak Mina Lee Tony Lee Jure Leskovec Isabelle Levent
Xiang Lisa Li Xuechen Li Tengyu Ma Ali Malik Christopher D. Manning Suvir Mirchandani Eric Mitchell Zanele Munyikwa Suraj Nair Avanika Narayan Deepak Narayanan Ben Newman Allen Nie Juan Carlos Niebles Hamed Nilforoshan Julian Nyarko Giray Ogut Laurel Orr Isabel Papadimitriou Joon Sung Park Chris Piech
Eva Portelance Christopher Potts Aditi Raghunathan Rob Reich Hongyu Ren Frieda Rong Yusuf Roohani Camilo Ruiz Jack Ryan Christopher Ré Dorsa Sadigh
Shiori Sagawa Keshav Santhanam Andy Shih Krishnan Srinivasan Alex Tamkin Rohan Taori Armin W. Thomas Florian Tramèr Rose E. Wang William Wang Bohan Wu Jiajun Wu Yuhuai Wu Sang Michael Xie Michihiro Yasunaga Jiaxuan You Matei Zaharia
Michael Zhang Tianyi Zhang Xikun Zhang Yuhui Zhang Lucia Zheng Kaitlyn Zhou Percy Liang*1

Center for Research on Foundation Models (CRFM) Stanford Institute for Human-Centered Artificial Intelligence (HAI)
Stanford University
AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) trained on broad data (generally using self-supervision at scale) that can be adapted to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotic manipulation, reasoning, human interaction) and technical principles (e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities, and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.

1Corresponding author: pliang@cs.stanford.edu

*Equal contribution.

1

2

Center for Research on Foundation Models (CRFM)

Contents

Contents

2

1 Introduction

3

1.1 Emergence and homogenization

3

1.2 Social impact and the foundation models ecosystem

7

1.3 The future of foundation models

9

1.4 Overview of this report

12

2 Capabilities

21

2.1 Language

22

2.2 Vision

28

2.3 Robotics

34

2.4 Reasoning and search

40

2.5 Interaction

44

2.6 Philosophy of understanding

48

3 Applications

53

3.1 Healthcare and biomedicine

54

3.2 Law

59

3.3 Education

67

4 Technology

73

4.1 Modeling

74

4.2 Training

81

4.3 Adaptation

85

4.4 Evaluation

91

4.5 Systems

97

4.6 Data

101

4.7 Security and privacy

105

4.8 Robustness to distribution shifts

109

4.9 AI safety and alignment

114

4.10 Theory

118

4.11 Interpretability

123

5 Society

129

5.1 Inequity and fairness

130

5.2 Misuse

136

5.3 Environment

140

5.4 Legality

146

5.5 Economics

149

5.6 Ethics of scale

152

6 Conclusion

161

Acknowledgments

161

References

161

On the Opportunities and Risks of Foundation Models

3

1 INTRODUCTION
This report investigates an emerging paradigm for building artificial intelligence (AI) systems based on a general class of models which we term foundation models.2 A foundation model is any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks; current examples include BERT [Devlin et al. 2019], GPT-3 [Brown et al. 2020], and CLIP [Radford et al. 2021]. From a technological point of view, foundation models are not new — they are based on deep neural networks and self-supervised learning, both of which have existed for decades. However, the sheer scale and scope of foundation models from the last few years have stretched our imagination of what is possible; for example, GPT-3 has 175 billion parameters and can be adapted via natural language prompts to do a passable job on a wide range of tasks despite not being trained explicitly to do many of those tasks [Brown et al. 2020]. At the same time, existing foundation models have the potential to accentuate harms, and their characteristics are in general poorly understood. Given their impending widespread deployment, they have become a topic of intense scrutiny [Bender et al. 2021].
1.1 Emergence and homogenization
The significance of foundation models can be summarized by two words: emergence and homogenization. Emergence means that the behavior of a system is implicitly induced rather than explicitly constructed; it is both the source of scientific excitement and anxiety about unanticipated consequences. Homogenization indicates the consolidation of methodologies for building machine learning systems across a wide range of applications; it provides strong leverage towards many tasks but also creates single points of failure. To better appreciate emergence and homogenization, let us reflect on their rise in AI research over the last 30 years.

Fig. 1. The story of AI has been one of increasing emergence and homogenization. With the introduction of machine learning, how a task is performed emerges (is inferred automatically) from examples; with deep learning, the high-level features used for prediction emerge; and with foundation models, even advanced functionalities such as in-context learning emerge. At the same time, machine learning homogenizes learning algorithms (e.g., logistic regression), deep learning homogenizes model architectures (e.g., Convolutional Neural Networks), and foundation models homogenizes the model itself (e.g., GPT-3).
Machine learning. Most AI systems today are powered by machine learning, where predictive models are trained on historical data and used to make future predictions. The rise of machine learning within AI started in the 1990s, representing a marked shift from the way AI systems were built previously: rather than specifying how to solve a task, a learning algorithm would induce it based on data — i.e., the how emerges from the dynamics of learning. Machine learning also
2We chose the term foundation models to capture the unfinished yet important status of these models — see §1.1.1: naming for further discussion of the name.

4

Center for Research on Foundation Models (CRFM)

represented a step towards homogenization: a wide range of applications could now be powered by a single generic learning algorithm such as logistic regression.
Despite the ubiquity of machine learning within AI, semantically complex tasks in natural language processing (NLP) and computer vision such as question answering or object recognition, where the inputs are sentences or images, still required domain experts to perform “feature engineering” — that is, writing domain-specific logic to convert raw data into higher-level features (e.g., SIFT [Lowe 1999] in computer vision) that were more suitable for popular machine learning methods.
Deep learning. Around 2010, a revival of deep neural networks under the moniker of deep learning [LeCun et al. 2015] started gaining traction in the field of machine learning. Deep learning was fueled by larger datasets, more computation (notably, the availability of GPUs), and greater audacity. Deep neural networks would be trained on the raw inputs (e.g., pixels), and higher-level features would emerge through training (a process dubbed “representation learning”). This led to massive performance gains on standard benchmarks, for example, in the seminal work of AlexNet [Krizhevsky et al. 2012] on the ImageNet dataset [Deng et al. 2009]. Deep learning also reflected a further shift towards homogenization: rather than having bespoke feature engineering pipelines for each application, the same deep neural network architecture could be used for many applications.
Foundation models. Foundation models have taken shape most strongly in NLP, so we focus our story there for the moment. That said, much as deep learning was popularized in computer vision but exists beyond it, we understand foundation models as a general paradigm of AI, rather than specific to NLP in any way. By the end of 2018, the field of NLP was about to undergo another seismic change, marking the beginning of the era of foundation models. On a technical level, foundation models are enabled by transfer learning [Thrun 1998] and scale. The idea of transfer learning is to take the “knowledge” learned from one task (e.g., object recognition in images) and apply it to another task (e.g., activity recognition in videos). Within deep learning, pretraining is the dominant approach to transfer learning: a model is trained on a surrogate task (often just as a means to an end) and then adapted to the downstream task of interest via fine-tuning.
Transfer learning is what makes foundation models possible, but scale is what makes them powerful. Scale required three ingredients: (i) improvements in computer hardware — e.g., GPU throughput and memory have increased 10× over the last four years (§4.5: systems); (ii) the development of the Transformer model architecture [Vaswani et al. 2017] that leverages the parallelism of the hardware to train much more expressive models than before (§4.1: modeling); and (iii) the availability of much more training data.
The importance of the availability of data and the ability to harness it cannot be underestimated. Transfer learning with annotated datasets has been common practice for at least a decade, for example, pretraining on the ImageNet dataset [Deng et al. 2009] for image classification in the computer vision community. However, the non-trivial cost of annotation imposes a practical limit on the benefits of pretraining.
In self-supervised learning on the other hand, the pretraining task is derived automatically from unannotated data.3 For example, the masked language modeling task used to train BERT [Devlin et al. 2019] is to predict a missing word in a sentence given its surrounding context (e.g., I like
sprouts). Self-supervised tasks are not only more scalable, only depending on unlabeled data, but they are designed to force the model to predict parts of the inputs, making them richer and potentially more useful than models trained on a more limited label space.
3Interestingly, self-supervised learning was dominant in the early days of deep learning [Hinton et al. 2006], but was for a decade largely overtaken by pure supervised learning as labeled datasets became larger.

On the Opportunities and Risks of Foundation Models

5

There had been considerable progress in self-supervised learning dating back to word embeddings [Turian et al. 2010; Mikolov et al. 2013; Pennington et al. 2014], which associated each word with a context-independent vector, provided the basis for a wide range of NLP models. Shortly thereafter, self-supervised learning based on autoregressive language modeling (predict the next word given the previous words) [Dai and Le 2015] became popular. This produced models that represented words in context, such as GPT [Radford et al. 2018], ELMo [Peters et al. 2018], and ULMFiT [Howard and Ruder 2018].4
The next wave of developments in self-supervised learning — BERT [Devlin et al. 2019] GPT-2 [Radford et al. 2019], RoBERTa [Liu et al. 2019], T5 [Raffel et al. 2019], BART [Lewis et al. 2020a] — quickly followed, embracing the Transformer architecture, incorporating more powerful deep bidirectional encoders of sentences, and scaling up to larger models and datasets.
While one can view this last wave of technical developments purely through the lens of selfsupervised learning, there was a sociological inflection point around the introduction of BERT. Before 2019, self-supervised learning with language models was essentially a subarea in NLP, which progressed in parallel to other developments in NLP. After 2019, self-supervised learning with language models became more of a substrate of NLP, as using BERT has become the norm. The acceptance that a single model could be useful for such a wide range of tasks marks the beginning of the era of foundation models.
Foundation models have led to an unprecedented level of homogenization: Almost all state-ofthe-art NLP models are now adapted from one of a few foundation models, such as BERT, RoBERTa, BART, T5, etc. While this homogenization produces extremely high leverage (any improvements in the foundation models can lead to immediate benefits across all of NLP), it is also a liability; all AI systems might inherit the same problematic biases of a few foundation models [Bolukbasi et al. 2016; Caliskan et al. 2017; Abid et al. 2021, inter alia]) — see §5.1: fairness, §5.6: ethics for further discussion.
We are also beginning to see a homogenization across research communities. For example, similar Transformer-based sequence modeling approaches are now applied to text [Devlin et al. 2019; Radford et al. 2019; Raffel et al. 2019], images [Dosovitskiy et al. 2020; Chen et al. 2020d], speech [Liu et al. 2020d], tabular data [Yin et al. 2020], protein sequences [Rives et al. 2021], organic molecules [Rothchild et al. 2021], and reinforcement learning [Chen et al. 2021b; Janner et al. 2021]. These examples point to a possible future where we have a unified set of tools for developing foundation models across a wide range of modalities [Tamkin et al. 2021b].
Besides the homogenization of approaches, we also see the homogenization of actual models across research communities in the form of multimodal models — e.g., foundation models trained on language and vision data [Luo et al. 2020; Kim et al. 2021a; Cho et al. 2021; Ramesh et al. 2021; Radford et al. 2021]. Data is naturally multimodal in some domains—e.g., medical images, structured data, clinical text in healthcare (§3.1: healthcare). Thus, multimodal foundation models are a natural way of fusing all the relevant information about a domain, and adapting to tasks that also span multiple modes (Figure 2).
Foundation models have also led to surprising emergence which results from scale. For example, GPT-3 [Brown et al. 2020], with 175 billion parameters compared to GPT-2’s 1.5 billion, permits in-context learning, in which the language model can be adapted to a downstream task simply by providing it with a prompt (a natural language description of the task), an emergent property that was neither specifically trained for nor anticipated to arise.

4The prescient work of Collobert and Weston [2008] is related: they trained on a scalable task akin to masked language modeling jointly with downstream tasks, rather than producing a single foundation model that can be adapted after the fact to downstream tasks.

6

Center for Research on Foundation Models (CRFM)

Fig. 2. A foundation model can centralize the information from all the data from various modalities. This
one model can then be adapted to a wide range of downstream tasks.
Homogenization and emergence interact in a potentially unsettling way. Homogenization could potentially provide enormous gains for many domains where task-specific data is quite limited — see the opportunities presented in several such domains (e.g., §3.1: healthcare, §3.2: law, §3.3: education); on the other hand, any flaws in the model are blindly inherited by all adapted models (§5.1: fairness, §5.6: ethics). Since the power of foundation models comes from their emergent qualities rather than their explicit construction, existing foundation models are hard to understand (§4.4: evaluation, §4.10: theory, §4.11: interpretability) and they have unexpected failure modes (§4.7: security, §4.8: robustness). Since emergence generates substantial uncertainty over the capabilities and flaws of foundation models, aggressive homogenization through these models is risky business. Derisking is the central challenge in the further development of foundation models from an ethical (§5.6: ethics) and AI safety (§4.9: ai-safety) perspective.
1.1.1 Naming.
We introduce the term foundation models to fill a void in describing the paradigm shift we are witnessing; we briefly recount some of our reasoning for this decision. Existing terms (e.g., pretrained model, self-supervised model) partially capture the technical dimension of these models, but fail to capture the significance of the paradigm shift in an accessible manner for those beyond machine learning. In particular, foundation model designates a model class that are distinctive in their sociological impact and how they have conferred a broad shift in AI research and deployment. In contrast, forms of pretraining and self-supervision that technically foreshadowed foundation models fail to clarify the shift in practices we hope to highlight.

On the Opportunities and Risks of Foundation Models

7

Fig. 3. Before reasoning about the social impact of foundation models, it is important to understand that
they are part of a broader ecosystem that stretches from data creation to deployment. At both ends, we
highlight the role of people as the ultimate source of data into training of a foundation model, but also as the
downstream recipients of any benefits and harms. Thoughtful data curation and adaptation should be part
of the responsible development of any AI system. Finally, note that the deployment of adapted foundation
models is a decision separate from their construction, which could be for research.
Additionally, while many of the iconic foundation models at the time of writing are language models, the term language model is simply too narrow for our purpose: as we describe, the scope of foundation models goes well beyond language. We also considered terms such as general-purpose model and multi-purpose model that capture the important aspect that these models can serve multiple downstream tasks, but both fail to capture their unfinished character and the need for adaptation. Terms such as task-agnostic model would capture the manner of training, but fail to capture the significant implication to downstream applications.
We chose the new term foundation models to identify the models and the emerging paradigm that are the subject of this report. In particular, the word “foundation” specifies the role these models play: a foundation model is itself incomplete but serves as the common basis from which many task-specific models are built via adaptation. We also chose the term “foundation" to connote the significance of architectural stability, safety, and security: poorly-constructed foundations are a recipe for disaster and well-executed foundations are a reliable bedrock for future applications. At present, we emphasize that we do not fully understand the nature or quality of the foundation that foundation models provide; we cannot characterize whether the foundation is trustworthy or not. Thus, this is a critical problem for researchers, foundation model providers, application developers who rely on foundation models, policymakers, and society at large to address.
1.2 Social impact and the foundation models ecosystem Foundation models are scientifically interesting due to their impressive performance and capabilities, but what makes them critical to study is the fact that they are quickly being integrated into realworld deployments of AI systems with far-reaching consequences on people. For example, Google search, which boasts 4 billion users, now depends on foundation models like BERT [Devlin et al. 2019] as one of its signals.5
5https://blog.google/products/search/search-language-understanding-bert/

8

Center for Research on Foundation Models (CRFM)

We must thus pause and ask: What is the nature of this social impact? In this report, we address many aspects of this question: the potential exacerbation of social inequities (§5.1: fairness), the economic impact due to increased capabilities (§5.5: economics), the environmental impact due to increased computation demands (§5.3: environment), potential concerns of amplifying disinformation (§5.2: misuse), legal ramifications due to powerful generative capabilities (§5.4: legality), ethical issues resulting from homogenization, and the broader political economy in which foundation models are developed and deployed (§5.6: ethics). Given the protean nature of foundation models and their unmapped capabilities, how can we responsibly anticipate and address the ethical and societal considerations they raise? A recurring theme is that it is easier to reason about the social impact of specific systems deployed to specific users than it is to reason about the social impact of foundation models, which could be adapted to any number of unforeseen downstream systems.
Before attempting to answer these questions, we need to lay some groundwork. First, let us distinguish between research on foundation models and deployment of foundation models. Most of what is publicly known is foundation models research — through academic papers, demonstrations, and progress on leaderboards. While the production of knowledge can play a vital role in shaping the future, the direct social impact is through the actual deployment of these models, which is governed by proprietary practices on often private data. Sometimes the deployment is through new products — e.g., GitHub’s Copilot6 based on OpenAI’s Codex model [Chen et al. 2021f], but often, it is through upgrades to existing products (e.g., Google search using BERT). Research models are often not extensively tested and might have unknown failure modes; warning labels should be placed on research models that are not fit to deploy. On the other hand, deployed foundation models that actually affect people’s lives should be subject to much more rigorous testing and auditing.
To further understand the research and deployment of foundation models, we must zoom out and consider the full ecosystem that these foundation models inhabit, from data creation to actual deployment. It is important to note that the foundation model is only one component (though an increasingly important component) of an AI system. Simplifying, we can think about the ecosystem of a foundation model in terms of sequence of stages, extending the training and adaptation stages from before.7 Appropriately, as we’re interested in social impact, people occupy both ends of the pipeline. This ecosystem view allows us to see that different questions about foundation models (e.g., whether a foundation model is ethical) should actually be answered with respect to different stages.
(1) Data creation: Data creation is fundamentally a human-centric process: all data is created by people and most data is at least implicitly about people. Sometimes data is created by people for other people in the form of emails, articles, photos, etc., and sometimes it is a measurement of people (e.g., genomic data) or a measurement of the environment people live in (e.g., satellite images). It is important to note that all data has an owner and is created with a purpose (where that purpose may or may not include training a foundation model).
(2) Data curation: Data is then curated into datasets. There is no single natural distribution of data; even the most permissive Internet crawl requires some selection and post-filtering. Ensuring data relevance and quality while respecting legal and ethical constraints is critical but challenging. While this is recognized in industry, it is underappreciated in AI research (§4.6: data).

6https://copilot.github.com/ 7In practice, the end of the pipeline is followed by monitoring, and feedback is used to readjust the previous stages.

On the Opportunities and Risks of Foundation Models

9

(3) Training: Training foundation models on these curated datasets8 is the celebrated centerpiece in AI research, though it is only one of many stages.
(4) Adaptation: In the context of machine learning research, adaptation is about creating a new model based on the foundation model that performs some task (e.g., document summarization). For deployment, adaptation is about creating a system, which requires potentially many different modules, custom rules (e.g., restrictions on the output space) or classifiers (e.g., for toxicity classification), and combination with other complementary signals (e.g., a question answering model’s generated answers would be validated against relevant documents). For example, a problematic model capable of generating toxic content might be tolerable if appropriate precautions are taken downstream. The extra application-specific logic is crucial for mitigating harms.
(5) Deployment: The direct social impact of an AI system occurs when it is deployed to people. Though we would not want to deploy potentially harmful foundation models trained on questionable data, there might still be value in permitting them in research to advance scientific understanding, though one must still exercise caution. More generally, it is standard practice in large-scale deployments to conduct gradual releases, where deployment happens to an increasing fraction of users; this can partially mitigate any potential harms.
While this report is about foundation models, it is important to note that many of the impacts come from decisions made in other stages in the pipeline, and thoughtful monitoring and intervention is needed at every stage. While large organizations might own the entire pipeline, each stage could be performed by a different organization, e.g., a company which specializes in creating custom foundation models for various domains that application-developers can use.
Think ecosystem, act model. While the social impact depends on the whole ecosystem, it is still important to be able to reason about the social implications of a foundation model, given that many researchers’ and practitioners’ purview is restricted to the training stage. This is difficult because foundation models are unfinished intermediate objects that can be adapted to many downstream applications, sometimes by an entirely different entity for unforeseen purposes. What we need are two things: (i) surrogate metrics for a representative set of potential downstream evaluation (§4.4: evaluation), and (ii) a commitment to documenting these metrics [Mitchell et al. 2019] similar to data sheets for materials such as metals and plastics, which can be adapted to many downstream use cases.
Characterizing the potential downstream social impact of foundation models is challenging and demands a deep understanding of both the technological ecosystem and of society. One cannot fully assess the harms (§5.1: fairness) of a foundation model without recognizing how it will be deployed, and one cannot just define automatic metrics without considering the rich social and historical context.
1.3 The future of foundation models
Foundation models have demonstrated raw potential, but we are still in the early days. Despite their deployment into the real world, these models are very much research prototypes that are poorly understood. Even the professional norms — what Robert Merton calls the ethos of science [Merton 1979] — around foundation models are underdeveloped. For example, there is lack of agreement on basic questions such as when models are “safe” to release or how the community should react in response to methodological misconduct. Given that the future of foundation models is thus filled with uncertainty, a big question is: who will determine this future?
8A foundation model (e.g., Codex) can also be trained with another model (e.g., GPT-3) as a starting point.

10

Center for Research on Foundation Models (CRFM)

Disciplinary diversity. The technology behind foundation models is based on decades of research in machine learning, optimization, NLP, computer vision, and other fields. These technical contributions have come from both academia and industrial research labs. However, research on building foundation models themselves has occurred almost exclusively in industry — big tech companies such as Google, Facebook, Microsoft, or Huawei, or startups such as OpenAI or AI21 Labs, though AI2 is a notable exception [Peters et al. 2018; Zellers et al. 2019b].
The furious pace of technological progress and the entrenchment due to centralization raise powerful concerns that demand the attention of humanists and social scientists in addition to technologists. We should not rely on post-hoc audits of ethical and social consequences, conducted only after the technical architecture and deployment decisions have been made. We instead need to infuse social considerations and ethical design deeply into the technological development of foundation models and their surrounding ecosystem from the start. Academic institutions are unique in that they host the widest set of disciplines under one roof, thus bringing together computer scientists, social scientists, economists, ethicists, legal scholars, etc. Given the importance of disciplinary diversity in understanding and solving problems that combine technical, ethical, legal, social, and political dimensions [Hong and Page 2004; Solomon 2006; Steel et al. 2018], we therefore see academia as playing a crucial role in developing foundation models in such a way to promote their social benefit and mitigate their social harms, as well as determining the contexts under which actions in each of the stages of the ecosystem (§1.2: ecosystem) ranging from data curation to deployment should be strictly prohibited.
Incentives. The political economy in which foundations models are designed, developed, and deployed provides an inevitable incentive structure for decision-making at every stage. How people and institutions respond to incentives is an elementary lesson of economics. Market-driven commercial incentives can align well with social benefit: making foundation models more accurate, reliable, safe, and efficient while searching for a wide variety of potential use cases can produce a great deal of social utility. However, commercial incentives can also lead to market failures and underinvestment in domains where shareholders are unable to capture the value of innovation. Just as the pharmaceutical industry has little incentive to devote significant resources to the research and development of malaria treatments, because poor people cannot afford medications,9 the tech industry has little incentive to devote significant resources to technologies designed for improving the condition of poor and marginalized people [Reich et al. 2021]. What’s more, commercial incentives can lead companies to ignore social externalities [Acemoglu 2021; Reich et al. 2021] such as the technological displacement of labor, the health of an informational ecosystem required for democracy, the environmental cost of computing resources, and the profit-driven sale of technologies to non-democratic regimes. Finally, there is little incentive for any given company to create an open, decentralized ecosystem for developing foundation models that encourages broad participation.
In contrast, the long-standing and deeply-seated research mission of universities is the production and dissemination of knowledge and creation of global public goods [Kerr 2001; Rhoten and Calhoun 2011; Nussbaum 2010]. We believe that academia is distinctively positioned to shape the development of foundation models to ensure that we capture directions with potentially large social benefit that might not otherwise be prioritized by industry.
Loss in accessibility. Unfortunately, academia has not been able to participate in the fullest way possible due to the loss in accessibility. One of the often overlooked effects of the deep learning revolution was the increase in reproducibility and open science: it increasingly became the norm
9See https://www.gatesfoundation.org/about/our-role.

On the Opportunities and Risks of Foundation Models

11

to publicly release code and datasets, and packages such as TensorFlow [Abadi et al. 2016] and PyTorch [Paszke et al. 2019] made it much easier for people to collaborate and build off of each other’s work. Initiatives like the ML Reproducibility Challenge10 as well as reproducibility checklists adopted by major conferences [Pineau et al. 2020], alongside platforms like CodaLab Worksheets11 helped advance community standards for reproducibility. This resulted in a surge in technological innovation and progress.
Foundation models start to roll back this positive trend. Some models (e.g., GPT-3) are not released at all (only API access to a limited pool of people). Even datasets (e.g., for GPT-2) are not released. While trained models may be available (e.g., BERT), the actual training of foundation models is unavailable to the vast majority of AI researchers, due to the much higher computational cost and the complex engineering requirements.
Some meaningful research can still be done by training smaller models within reach of an academic budget, and indeed the surprisingly regularity predicted by scaling laws [Kaplan et al. 2020] make this a viable strategy for cases where the differences due to scale are quantitative (e.g., accuracy goes up). However, due to the emergent nature of these foundation models, some functionalities like in-context learning have only been demonstrated in models of sufficient size, so scale is needed to even ask the right questions.
It is also possible to productively study pre-existing models that have been released; indeed, this has led to a large subcommunity within NLP for probing these models [Rogers et al. 2020; Manning et al. 2020]. Having access to existing models can be useful for powering downstream applications or identifying defects (e.g., bias), but this might not be enough for us to design better architectures or training objectives for foundation models that can fix these defects (e.g., mitigate the bias). It is worth reflecting on how much of NLP research today is based on BERT, a particular (and somewhat arbitrary) foundation model. Given the need to infuse social awareness and ethical design into the construction of these models, it is possible that we need to build foundation models that look quite different from what exists today. This will demand intense experimentation at scale.
Community efforts such as EleutherAI12 and Hugging Face’s BigScience project13 are attempting to train large foundation models, but the gap between the private models that industry can train and the ones that are open to the community will likely remain large if not grow. Further, today startups (OpenAI, Anthropic, AI21 Labs, etc.) are much more well-resourced than academia and can therefore still afford to train the largest foundation models (e.g., OpenAI’s GPT-3). However, big tech companies are on a completely different level in terms of resources, especially in terms of the infrastructure, users, and data that come from their market position. The fundamental centralizing nature of foundation models means that the barrier to entry for developing them will continue to rise, so that even startups, despite their agility, will find it difficult to compete, a trend that is reflected in the development of search engines [Radinsky 2015].
One way to close the resource gap is for the government to invest in public infrastructure. We can look to Big Science projects such as the Hubble Space Telescope and the Large Hadron Collider as inspiration, where substantial investment made possible fundamental scientific discoveries which wouldn’t have been possible. One can imagine a similar infrastructure for computing, from which academic research on foundation models would greatly benefit. In the US, the nascent National Research Cloud initiative14 is a step in this direction.

10https://paperswithcode.com/rc2020 11https://worksheets.codalab.org/ 12https://www.eleuther.ai/ 13https://bigscience.huggingface.co/ 14https://hai.stanford.edu/policy/national-research-cloud

12

Center for Research on Foundation Models (CRFM)

Another complementary approach is to rely on volunteer computing, in which any of the billions of computing devices (nodes) can connect to a central server and contribute computation. The Folding@home project has successfully implemented this approach for simulating protein dynamics [Beberg et al. 2009]. Recently, the Learning@home project is attempting to harness volunteer computing for training foundation models [Ryabinin and Gusev 2020]. The high latency connections between nodes and the high bandwidth requirements for training foundation models make this an open technical challenge.
Summary. There are tremendous economic incentives to push the capabilities and scale of foundation models, so we anticipate steady technological progress over the coming years. But the suitability of a technology relying largely on emergent behavior for widespread deployment to people is unclear. What is clear that we need to be cautious, and that now is the time to establish the professional norms that will enable the responsible research and deployment of foundation models. Academia and industry need to collaborate on this: industry ultimately makes concrete decisions about how foundation models will be deployed, but we should also lean on academia, with its disciplinary diversity and non-commercial incentives around knowledge production and social benefit, to provide distinctive guidance on the development and deployment of foundation models that is both technically and ethically grounded.
1.4 Overview of this report
In March 2021, we created an informal community at Stanford University of students, faculty, and researchers interested in some aspect of foundation models.15 From the very beginning, the community included not just AI researchers, but those eager to apply foundation models to their domain (e.g., healthcare and law), as well as those who were interested in societal concerns (e.g., ethics and economics). As discussions progressed, we noticed that there were many gaps in mutual understanding — how the technology worked, how industry develops foundation models, how to think about the ethical concerns, etc., and existing literature only covered bits and pieces. We wanted to therefore provide a fuller picture of foundation models, identify opportunities and risks, and establish a constructive vision for the future responsible development of foundation models.
The writing of this report was an experiment: we had over 100 people from different backgrounds come together to write a single report covering a wide range of aspects of foundation models. A large part of this report is a survey of existing work, but through many discussions, we have unified it in one report to highlight all the interdisciplinary connections.
Structure. The report is divided into 26 sections, each discussing one aspect of foundation models. The sections are grouped into four parts: capabilities (§2: capabilities), applications (§3: applications), technology (§4: technology), and society (§5: society), although there are many connections across sections. These connections highlight an integrated approach in which the technologies and capabilities are developed in a way that is sensitive to real societal concerns, while being inspired by and grounded out in applications.
While we have sought to capture most of the important topics surrounding foundation models, this report will inevitably be incomplete, especially as the field evolves quickly. For example, many applications (e.g., natural sciences, music, finance, agriculture) are not included, though they are as likely to be affected as the applications we have chosen to discuss. It would also be interesting to

15This community led to the founding of the Center for Research on Foundation Models (CRFM), a new interdisciplinary initiative at the Stanford Institute for Human-Centered AI (HAI).

On the Opportunities and Risks of Foundation Models

13

2. Capabilities

Paper Roadmap

Language
2.1

Vision
2.2

3. Applications

Robotics
2.3

Reasoning
2.4

Interaction
2.5

Philosophy
2.6

4. Technology

Healthcare
3.1

Law

Education

3.2

3.3

Modeling
4.1

Training
4.2

Adaptation
4.3

Evaluation
4.4

Systems
4.5

Data
4.6

Security
4.7

Robustness
4.8

AI Safety & Alignment
4.9

Theory
4.10

Interpretability 4.11

5. Society

Inequity
5.1

Misuse
5.2

Environment
5.3

Legality
5.4

Economics
5.5

Ethics
5.6

Fig. 4. This report is divided into four parts: capabilities, applications, technology, and society, where each part contains a set of sections, and each section covers one aspect of foundation models.
study how foundation models relate to research in neuroscience, cognitive science, and psychology to explain intelligence and aid efforts in computational social science to understand society.

14

Center for Research on Foundation Models (CRFM)

Author Contributions Percy Liang initiated and conceptualized the framing and structure of the overall report. He and Rishi Bommasani worked together to lead the decentralized writing effort and provided guidance on individual sections. Drew A. Hudson created all the figures in the report, discussing their structure and content with the authors of each section. Each of the 26 sections of this report was written by a subset of authors, whose names are listed at the beginning of each section. There were, however, many discussions that spanned multiple sections, so the actual contributions to each section generally came from a broader set. Finally, we note that not all the views expressed in this report are held by all the authors.
1.4.1 Overview of capabilities.
Foundation models acquire various capabilities that can power applications. We have chosen to discuss five potential capabilities: the ability to process different modalities (e.g., language, vision), to affect the physical world (robotics), to perform reasoning, and to interact with humans (interaction). Finally, we conclude with a philosophical discussion of potential limits on their capabilities.
§2.1: Language. NLP as a field has blazed the trail for foundation models. While these models dominate standard benchmarks, there is a clear gap between the capabilities these models acquire currently and those that characterize language as a complex system for human communication and thought. In response to this, we emphasize the full range of linguistic variation (e.g., different styles, dialects, languages), which poses an opportunity and challenge given some variants are data-limited. Further, child language acquisition is more sample efficient than the training of foundation models; we examine how signals beyond text and grounding may help to bridge this gap. Both of these characteristics of language provide clear directions for future foundation models research.
§2.2: Vision. Computer vision led the adoption of deep learning in AI [Russakovsky et al. 2015], demonstrating that models pretrained on large annotated datasets can transfer to numerous downstream settings. Now, pretraining on web-scale raw data instead of curated datasets, foundation models are on the rise in computer vision [e.g., Radford et al. 2021]. These models have shown promising results for standard tasks in the field, like image classification and object detection, and training on multimodal and embodied data beyond images may enable progress on significant challenges (e.g., 3D geometric and physical understanding, commonsense reasoning). We also discuss some of the key challenges in modeling (e.g., the ability to scale effectively to videos) and evaluation (e.g., the measurement of higher-order capabilities) along with the applications (e.g., ambient intelligence for healthcare) and societal considerations (e.g., surveillance) that will determine the impact of foundation models for computer vision going forward.
§2.3: Robotics. A longstanding goal of robotics research is to develop “generalist” robots capable of performing myriad tasks across physically diverse environments. Unlike language and vision, which have led the way with foundation models both due to the abundance of raw data to train these models on and the availability of virtual applications to apply these models to, robotics faces fundamental challenges due to being anchored to the physical world. The principal challenge in developing new types of foundation models for robotics — different in nature than their language and vision counterparts — is acquiring sufficient data of the right form that is conducive to learning: we explore how plentiful data (e.g., generic videos of humans, amongst others) that is not specific to particular environments and across modalities (e.g., language, vision) may help to bridge this gap. These new robotic foundation models could allow for easier task specification and learning, ushering in new applications (e.g., better robotic assistance for household tasks) and heightening the importance of robustness and safety (e.g., formal safety evaluation).

On the Opportunities and Risks of Foundation Models

15

§2.4: Reasoning and search. Reasoning and search problems such as theorem proving and program synthesis have been long-standing challenges in AI. The combinatorial search space renders traditional search-based methods intractable. However, humans are known to operate intuitively even in the most mathematical of domains [Lakoff and Núñez 2000], and indeed existing work such as AlphaGo have already shown that deep neural networks can be effective in guiding the search space. But humans also transfer knowledge across tasks, facilitating much more efficient adaptation and the ability to reason more abstractly. Foundation models offer the possibility of closing this gap: their multi-purpose nature along with their strong generative and multimodal capabilities offer new leverage for controlling the combinatorial explosion inherent to search.
§2.5: Interaction. Foundation models show clear potential to transform the developer and user experience for AI systems: foundation models lower the difficulty threshold for prototyping and building AI applications due to their sample efficiency in adaptation, and raise the ceiling for novel user interaction due to their multimodal and generative capabilities. This provides a synergy we encourage going forward: developers can provide applications that better fit the user’s needs and values, while introducing far more dynamic forms of interaction and opportunities for feedback.
§2.6: Philosophy of understanding. What could a foundation model come to understand about the data it is trained on? Focusing on the case of natural language, we identify different positions on the nature of understanding and explore their relevance for our central question. Our tentative conclusion is that skepticism about the capacity of future foundation models to understand natural language may be premature, especially where the models are trained on multi-modal data.
1.4.2 Overview of applications.
At present, foundation model research is largely confined to computer science and AI, with the impact of foundation models and the applications they support largely being centered in the tech industry. Moving forward, foundation models present clear potential to transform and extend the reach of AI across many sectors beyond the tech industry, suggesting a more pervasive effect on people’s lives. While there is a multitude of applications and domains to consider, we we have chosen three applications — healthcare, law, and education — because they represent foundational pillars of our society. For foundation models to significantly contribute to these application domains, models will require specific capabilities (§2: capabilities) as well as technical innovation (§4: technology) to account for the unique considerations in each domain. Further, since these domains are critical to societal function (§5: society), applying foundation models in these domains requires engaging with deeply sociotechnical matters such as those those pertaining to data (§4.6: data), privacy (§4.7: security), interpretability (§4.11: interpretability), fairness (§5.1: fairness) and ethics (§5.6: ethics).
§3.1: Healthcare and biomedicine. Healthcare tasks (e.g., patient care via disease treatment) and biomedical research (e.g., scientific discovery of new therapies) require expert knowledge that is limited and expensive. Foundation models present clear opportunities in these domains due to the abundance of data across many modalities (e.g., images, text, molecules) to train foundation models, as well as the value of improved sample efficiency in adaptation due to the cost of expert time and knowledge. Further, foundation models may allow for improved interface design (§2.5: interaction) for both healthcare providers and patients to interact with AI systems, and their generative capabilities suggest potential for open-ended research problems like drug discovery. Simultaneously, they come with clear risks (e.g., exacerbating historical biases in medical datasets and trials). To responsibly unlock this potential requires engaging deeply with the sociotechnical

16

Center for Research on Foundation Models (CRFM)

matters of data sources and privacy as well as model interpretability and explainability, alongside effective regulation of the use of foundation models for both healthcare and biomedicine.
§3.2: Law. Legal applications require that attorneys read and produce long coherent narratives that incorporate shifting contexts and decipher ambiguous legal standards. Foundation models may provide benefits in this domain: ample data exists in the form of legal documents and their generative capabilities are well-suited to the many generative tasks required in law, but significant improvements are required for foundation models to be able to reliably reason over various sources of information to generate truthful long-form documents. As is the care in healthcare (§3.1: healthcare), the sample efficiency of adaptation for foundation models is of heightened value given the costs of expert time and knowledge in the legal domain, which may allow for the re-allocation of expertise towards pressing problems of justice and government service. The responsible development of foundation models for law will require specific consideration of privacy, and highlights core limitations of existing foundation models that will require fundamental advances with respect to provenance for their behavior and guarantees for the factuality of their generation.
§3.3: Education. Education is a complex and subtle domain; effective teaching involves reasoning about student cognition and should reflect the learning goals of students. The nature of foundation models presents promise here that has yet to be realized in the sphere of AI for education: while certain many streams of data in education are individually too limited to train foundation models, the ability to leverage relevant data from outside the domain (e.g., the Internet) and make use of data across multiple modalities (e.g., textbooks, mathematical formula, diagrams, video-based tutorials) jointly offers hope for foundation models that are broadly applicable to educational tasks. If foundation models lead to a significant improvement in education-relevant capabilities, there is clear potential for new applications that align with the open-ended generative (e.g., problem generation) and interactive (e.g., feedback to teachers) aspects of foundation models; the sample efficient adaptation of foundation models suggests greater ability for adaptive and personalized learning. In this event, renewed consideration is required of hallmarks of applying technology to education (e.g., student privacy), along with certain concerns becoming more critical (e.g., inequity in access to technology in education, technology-aided plagiarism).
1.4.3 Overview of technology.
Now we discuss the technology behind building better model architectures, training and adaptation procedures, and of course scaling up the systems. One crucial but often overlooked topic is data — where does it come from and what is its composition? In addition, we want foundation models to be robust to distribution shifts and secure against attackers. Finally, we wish to understand why foundation models work from both a mathematical perspective as well as an empirical perspective.
§4.1: Modeling. What structural properties give rise to a foundation model? In the modeling section, we explore the underlying architectures behind foundation models and identify 5 key attributes. First, we start by discussing expressivity of the computational model — to capture and assimilate real-world information, and scalability — to adeptly handle large quantities of highdimensional data. These properties are successfully realized by existing architectures such as the transformer network [Vaswani et al. 2017] that underpins most foundation models to date. We then proceed to attributes may be essential for the next generation of models, including: multimodallity — to consume, process and potentially produce content from different sources and domains, memory capacity — to effectively store and retrieve the acquired knowledge, and finally, compositionality, to foster successful generalization to novel settings and environments. We believe that realizing the

On the Opportunities and Risks of Foundation Models

17

full potential envisioned for foundation models will hinge on modelling advances to fulfill these desiderata.
§4.2: Training. Training objectives mathematically specify how models should learn and acquire capabilities from their training data. The current status quo for training foundation models involves modality-specific objectives (e.g., masked language modeling [Devlin et al. 2019] for text and SimCLR [Chen et al. 2020c] for images) that are often chosen heuristically. We envision that future training objectives for foundation models will reflect two changes: principled selection derived from systematic evidence and evaluation (§4.4: evaluation), and domain-generality to provide rich, scalable, and unified training signal across data sources and modalities. We also discuss important design trade-offs, including generative vs discriminative training, the choice of input data representation, and the potential of future training objectives that involve explicit representations of goals.
§4.3: Adaptation. Foundation models are intermediary assets; they are unfinished and generally should not be used directly, instead requiring adaptation for specific downstream tasks. The de facto approach for adaptation has been fine-tuning, with recent work suggesting that lightweight fine-tuning alternatives and prompting-based methods may achieve favorable accuracy-efficiency tradeoffs. Moving forward, we envision a more expansive view of adaptation that goes beyond just specializing foundation models to perform the task of interest: adaptation will alleviate deficiencies of stand-alone foundation models (e.g., temporal adaptation to reflect changes over time in the world) or introduce constraints (e.g., GDPR compliance relating to the right to be forgotten; §4.7: security); this broader perspective on adaptation coincides with a need for new evaluation protocols (§4.4: evaluation) that systematically evaluate adaptation methods while controlling for resources (e.g., runtime, memory) and access requirements involved in adaptation.
§4.4: Evaluation. Evaluation offers context to foundation models by providing a means to track progress, understand models, and document their capabilities and biases. Foundation models challenge the ability of standard evaluation paradigms in machine learning to achieve these goals since they are one step removed from specific tasks. To envision new paradigms in evaluation that suit foundation models, we discuss (a) evaluating foundation models directly to measure their inherent capabilities and inform how foundation models are trained, (b) evaluating task-specific models by controlling for adaptation resources and access, and (c) broader evaluation design to provide richer context beyond measures of accuracy (e.g., robustness (§4.8: robustness), fairness (§5.1: fairness), efficiency (§4.5: systems), environmental impact (§5.3: environment)). Reform of evaluation practices will allow for evaluation that adequately serves both the diverse goals and stakeholders involved in the foundation model paradigm.
§4.5: Systems. While the training data (§4.6: data) determines the theoretical information available for foundation models, and model architectures (§4.1: modeling) and training objectives (§4.2: training) determine how much of this information can be extracted, computer systems determine what is practically achievable. Systems are a key bottleneck for scaling in terms of data and model size, both of which appear to reliably track with improvements in capabilities. To ensure that we can train the next generation of foundation models efficiently (with respect to time and cost), we will require the co-design of algorithms, models, software, and hardware. This co-design is already starting to happen to in various forms, from carefully tuned parallelism strategies to new architectures such as retrieval-based and mixture-of-expert models. Beyond training, we consider what will be required to deploy applications on top of foundation models (e.g., efficient inference).

18

Center for Research on Foundation Models (CRFM)

§4.6: Data. Data is the lifeblood of foundation models; the training data of these models largely determines what these capabilities these models can acquire. The centrality of data is not unique to foundation models; recent calls for data-centric AI [Press 2021; Ré 2021] indicate the pervasive importance of managing, understanding, and documenting data used to train machine learning models. For foundation models specifically, the current modus operandi is for training data to be selected using unspecified or unclear principles with a general lack of transparency regarding the nature of training data. We believe an alternative approach is needed to re-imagine the data ecosystem surrounding foundation models: we draw upon work on data visualization and management to propose a data hub for foundation models. We articulate how this proposal relates to many of the relevant data-centric considerations for foundation models: selection, curation, documentation, access, visualization and inspection, quality assessment, and legal regulation.
§4.7: Security and privacy. Security and privacy for foundation models is largely uncharted at present. Fundamentally, foundation models are a high-leverage single point of failure, making them a prime target for attack: existing work demonstrates a variety of security vulnerabilities (e.g., adversarial triggers to generate undesirable outputs) or privacy risks (e.g., memorization of training data) for these models. Further, the generality of foundation models compounds these concerns, intensifying the risk for function creep or dual use (i.e., use for unintended purposes). For security, we view foundation models as akin to operating systems in traditional software systems; we discuss steps towards secure foundation models which, if achieved, would provide a strong abstraction layer to build upon for reliable ML applications. For privacy, by leveraging knowledge transfer from public data, foundation models may enable more sample efficient adaptation to sensitive data distributions, i.e., privacy-preserving applications may incur less degradation in accuracy when built using foundation models.
§4.8: Robustness to distribution shifts. A major limitation of standard machine learning is that it produces models that are not robust to distribution shifts, where the training distribution does not match the test distribution (for the downstream task). Existing work shows that adapting a foundation model trained on a broad range of unlabeled data improves the robustness of adapted models across a wide variety of shifts. This opens a new set of promising directions for improving training and adaptation of foundation models for robustness. However, we do not believe that foundation models are a panacea for robustness — challenges such as extrapolation across time and spurious correlations are not likely to be fully addressed.
§4.9: AI safety and alignment. Ensuring foundation models are reliable (§4.5: systems), robust (§4.8: robustness), and interpretable (§4.11: interpretability) is increasingly important when considering the potential real-world applications of these models. In addition to critical and immediate considerations, we also consider the relationship between foundation models and larger-scale risks, hazards, and harms that have the potential for increased relevance as model capabilities continue to advance. For example, we consider the importance of aligning foundation models such that they are not deployed with misspecified goals or values. We also discuss the relevance of forecasting the emergent behaviors of foundation models (e.g., the ability to deceive or plan strategically), which may complicate attempts to adapt them to particular tasks, and may require new approaches for interpretability (§4.11: interpretability) or evaluation (§4.4: evaluation).
§4.10: Theory. Learning theory provides a broad foundation for the variety of contexts encountered in applied machine learning; theory offers both understanding, principles, and guarantees to complement empirical findings. At present, the study of foundation models is largely empirical: the theory of standard supervised learning, while relatively mature, is inadequate to fully explain foundation models. Specifically, the discrepancy between the training phase and the adaptation

On the Opportunities and Risks of Foundation Models

19

phase within the foundation model regime pinpoints the insufficiency of existing theory, since these phases correspond to (potentially) completely different tasks and data distributions. Nevertheless, we endeavor that advances in theory to address this discrepancy, even in simple, limited settings, will provide useful insights.
§4.11: Interpretability. Interpretability provides clarity to foundation models: the opacity of the deep neural networks that underpin foundation models, alongside the expected ubiquity of foundation models, heightens the need to understand these models and their capabilities. Interpretability methods at present generally are designed for interpreting and explaining the behavior of task-specific models; the nature of foundation models (i.e., the wide array of tasks these models are beneficial for and the unexpected emergent properties they acquire) introduces new challenges for interpretability research. To frame the discussion of interpretability for foundation models, we propose the one model-many models paradigm, which aims to determine the extent to which the one model (the foundation model) and its many models (its adapted derivatives) share decision-making building blocks. In addition to interpreting the decision-making components involved, we further discuss explainability in the context of foundation models (e.g., the validity ofpost hoc explanations generated by models) as well as the mechanisms that drive model behavior (which may clarify the extent to which understanding foundation models can extend to understanding their adapted derivatives). Given the critical role we ascribe interpretability in the study of foundation models, we conclude with an assessment of the societal impact of interpretability and non-interpretability.
1.4.4 Overview of society.
We believe the rapid development of foundation models, adapted and deployed to various applications, will have wide-ranging consequences on the health of societies. What makes these models so exciting and also so troubling is their task agnosticity. Societal impact is easier (but still non-trivial) to understand and reason about when we talk about specific systems deployed to users, but how can we take into account the societal impact of all possible systems and use cases when developing foundation models?
§5.1: Inequity and fairness. In many contexts, machine learning has been shown to contribute to, and potentially amplify, societal inequity. Foundation models may extend this trend, i.e., furthering the unjust treatment of people who have been historically discriminated against. However, understanding the relationship between inequity and foundation models requires reckoning with the abstraction of foundation models; foundation models are intermediary assets that are adapted for applications that impact users. Therefore, we delineate intrinsic biases, i.e., properties in foundation models that portend harm, and extrinsic harms, i.e., harms arising in the context of specific applications built using foundation models. We taxonomize various sources (e.g., training data, lack of diversity among foundation model developers, the broader sociotechnical context) that give rise to these biases and harms, emphasizing the importance, and technical difficulty, of source tracing to understand ethical and legal responsibility. We do not view unfairness as inevitable in the foundation model paradigm: to address unfair outcomes that arise from foundation models, we dually consider proactive interventions (e.g., technical methods like counterfactual data augmentation) and reactive recourse (e.g., mechanisms for feedback propagation and attribution of moral/legal responsibility).
§5.2: Misuse. We define foundation model misuse as the use of foundation models as they are technically intended (e.g., to generate language or video), but with the goal of causing societal harm (e.g., to generate disinformation, to develop deepfakes for harassment). We argue that advances in foundation models will result in higher-quality machine-generated content that will be easier to

20

Center for Research on Foundation Models (CRFM)

create and personalize for misuse purposes. For example, disinformation actors may use them to quickly generate collections of articles targeted across different demographic groups (e.g., nationality, political party, religion, etc.). While these new capabilities may limit existing human detection methods for harmful content (e.g., tracking similar text across different sources), foundation models may themselves provide promising potential as automated misuse detectors.
§5.3: Environment. Foundation models are the byproducts of computationally expensive training regimes, with the existing trajectory favoring even more intensive models; the energy required for this training coincides with the release of more carbon into the atmosphere and the degradation of the environment. At present, current discussion centers these enormous single-time training costs and the potential to amortize these costs across repeated use. We seek to clarify these discussions by identifying assumptions that shape the calculus of environmental impact for foundation models. Further, we envision that the ecosystem surrounding foundation models requires a multi-faceted approach: (a) more compute-efficient models, hardware, and energy grids all may mitigate the carbon burden of these models, (b) environmental cost should be a clear factor that informs how foundation models are evaluated (§4.4: evaluation), such that foundation models can be more comprehensively juxtaposed with more environment-friendly baselines, and (c) the cost-benefit analysis surrounding environmental impact necessitates greater documentation and measurement across the community.
§5.4: Legality. Foundation models rest on tenuous legal footings at present; how the law bears on both the development and use of these models is largely unclear. Legal and regulatory frameworks for foundation models specifically, alongside those for AI technology more generally, will be needed to influence, constrain, and even foster practices in research, development, and deployment. Centering on the legal landscape of the United States, where existing consideration of algorithmic tools remains broadly uncertain, we highlight the pertinent issues of liability for model predictions and protections from model behavior. With respect to both issues, we describe how legal standards will need to be advanced to address these given the intermediary status of foundation models (as opposed to that of user-facing task-specific models).
§5.5: Economics. Foundation models are likely to have substantial economic impact due to their novel capabilities and potential applications in a wide variety of industries and occupations. We consider the implications of the development and use of foundation models for the future of the US and global economy with a focus on productivity, wage inequality, and concentration of ownership.
§5.6: Ethics of scale. In addition to running the risk of increasing inequity, as discussed in §5.1: fairness, the widespread adoption of foundation models poses other ethical, political and social concerns. We discuss ethical issues related to the scale of application of foundation models, such as homogenization and the concentration of power, as well as the norms and release strategies appropriate to address them.

On the Opportunities and Risks of Foundation Models

21

2 CAPABILITIES
Foundation models acquire capabilities, some that surprisingly emerge from their learning process, that power downstream applications (§3: applications). Specifically, we discuss linguistic (§2.1: language) and visual (§2.2: vision) capabilities alongside the ability to affect the physical world (§2.3: robotics), perform reasoning and search (§2.4: reasoning), and interact with humans (§2.5: interaction). In addition, we discuss how self-supervision (the technical approach used to learn most current foundation models) philosophically relates to the ability to understand (§2.6: philosophy).

22

Center for Research on Foundation Models (CRFM)

2.1 Language Authors: Isabel Papadimitriou, Christopher D. Manning

2.1.1 The nature of human language.
Language is the basis of most human communication and interaction. However, it is not just a means for humans to achieve shared goals: language is central to human thought, to how social and emotional relations are formed, to how we identify ourselves socially and personally, and to how humans record knowledge and develop societal intelligence. Spoken or signed languages arise in every human society, and the languages of the world are both incredibly diverse in the ways that they express and structure the information they convey, while also exhibiting surprising concordance in the richness of what makes a language [Comrie 1989]. Languages are remarkably complex yet efficient systems, acquired consistently by children in a short amount of time, and which evolve and encompass the changing needs and conditions of linguistic communities. Due to this centrality of language in human activities, language understanding and generation is a critical element of research in artificial intelligence. Natural language processing (NLP) is the subfield of artificial intelligence concerned with language and, together with the related fields of automatic speech recognition (ASR) and text-to-speech (TTS), has the goal of giving computers the ability to understand and generate human language in much the same way human beings can.
To date in 2021, NLP has been the field most profoundly affected by foundation models. The first generation of foundation models showcased an impressive variety of linguistic abilities, as well as a surprising amount of adaptability to a large range of linguistic situations. Since the introduction of the early foundation models ELMo [Peters et al. 2018] and BERT [Devlin et al. 2019] in 2018, the field of NLP has become largely centered around using and understanding foundation models. The field has shifted to using foundation models as the primary tool, moving towards more generalized language learning as a central approach and goal. In this section, we go over the recent successes of foundation models in NLP, detail how foundation models have changed the overall process and mentality for training machine learning models for language, and discuss some of the theoretical and practical challenges facing foundation models as they are applied to a broader set of languages and more realistic and complex linguistic situations.
2.1.2 Impact of foundation models on NLP.
Foundation models have had a huge impact on the field of NLP, and are now central to most NLP systems and research. On a first level, many foundation models are skilled language generators: for example, Clark et al. [2021] demonstrate that non-experts have difficulty distinguishing short-form English text that was written by GPT-3 from that written by humans. However, the feature of foundation models that has been most impactful in NLP is not their raw generation abilities but their surprising generality and adaptability: a single foundation model can be adapted in different ways in order to achieve many linguistic tasks.
The field of NLP has historically focused on defining and engineering systems for challenging linguistic tasks, with the vision that models that are good at these tasks will lead to competent language systems for downstream applications. NLP tasks include classification tasks for a whole sentence or document (e.g., sentiment classification, like predicting whether a movie review is positive or negative), sequence labeling tasks, in which we classify each word or phrase in a sentence or document (e.g., predicting if each word is a verb or a noun, or which spans of words refer to a person or an organization), span relation classification, (e.g., relation extraction or parsing, like whether a person and location are linked by a “current residence” relation, or a verb and a noun by a “subject-verb” relation) and generation tasks, producing new text that is conditioned

On the Opportunities and Risks of Foundation Models

23

Fig. 5. Only a tiny percentage of the world’s languages are currently represented in foundation models.
There are over 6,000 languages in the world, with estimates varying due to the inherent uncertainty of what
constitutes a separate language [Nordhoff and Hammarström 2011]. This map shows the languages of the
world, with each dot representing one language and its color indicating the top-level language family. Data is
from Glottolog [Hammarström et al. 2021]. We label a few of the languages on the map as examples.
strongly on an input (e.g., producing a translation or summary of a text, recognizing or producing speech, or responding in a conversation) [Jurafsky and Martin 2009]. In the past, NLP tasks had distinct research communities that developed task-specific architectures, often based on pipelines of different models, each performing a linguistic sub-task such as token segmentation, syntactic parsing, or coreference resolution.
By contrast, the dominant modern approach for performing each task is to use a single foundation model and adapt it slightly using relatively small amounts of annotated data specific to each task (sentiment classification, named entity tagging, translation, summarization) to create an adapted model. This has proved to be an extremely successful approach: for the vast majority of the tasks described above, a foundation model that is slightly adapted for a task greatly outperforms previous models or pipelines of models that were built specifically to perform that one task. To take just one example, the best system for answering open-ended science questions in 2018, before foundation models, could get 73.1% on the NY Regents 8th grade science exam. A year later in 2019, an adapted foundation model scored 91.6% [Clark et al. 2019].
The emergence of foundation models that are largely trained to generate language has constituted an important shift in the role of language generation in NLP. Until around 2018, the problem of generating general-purpose language was considered very difficult and essentially unapproachable except through other linguistic sub-tasks [Paris et al. 2013]. Instead, NLP research was mostly focused on linguistically analyzing and understanding text. Now, it is possible to train highly coherent foundation models with a simple language generation objective, like “predict the next word in this sentence”. These generative models now constitute the primary vehicle through which machine learning for language is done — including the analysis and understanding tasks that were once considered prerequisites for generation. The successful generation exhibited by foundation

24

Center for Research on Foundation Models (CRFM)

models has also led to a flowering of research for language generation tasks like summarization and dialogue generation. The rise of the foundation model paradigm has begun to play a similar role in spoken language as well as written. Modern automatic speech recognition (ASR) models like wav2vec 2.0 are trained on large datasets of speech audio alone, and then adapted on audio with associated transcriptions for the task of ASR [Baevski et al. 2020].
Due to the changes brought about by the foundation model paradigm, the focus of research and practice in NLP has shifted from making bespoke architectures for different tasks to exploring how to best leverage foundation models. Research into adaptation methods has blossomed (see §4.3: adaptation for a detailed look at adaptation), and the surprising successes of foundation models have also caused a shift in research interest towards analyzing and understanding foundation models (see §4.11: interpretability for interpretability and analysis of foundation models).
2.1.3 Language variation and multilinguality.
Though foundation models are surprisingly versatile with the linguistic knowledge they obtain from pretraining, there are limits to this adaptability: it is not clear how successful current foundation models are at handling language variation. Language varies greatly. Apart from the fact that there are thousands of different languages in the world, language varies even within one language or within one speaker. To point out a few examples, informal conversation manifests differently from written language, the grammatical constructions that people reach for when speaking to friends are very different from those used when speaking to someone with authority, and communities of speakers within a language use different dialects. Social and political factors are embedded in how language variation is viewed and valued, and in how much different varieties are represented in NLP research (see for example Blodgett and O’Connor [2017] on the failures of NLP for African American English, and §5.1: fairness for a deeper discussion on inequities in foundation models). Due to their large capacity for learning linguistic information and flexibly adapting that knowledge, foundation models hold promise for expanding NLP to encompass more linguistic diversity. It remains an open research question to understand whether it is possible to make foundation models that robustly and equitably represent language with both its major and subtle variations, giving equal weight and acuity to what makes each linguistic variety distinct [research posing and addressing this question includes Ponti et al. 2019; Bender 2011; Joshi et al. 2020].
Following the success of foundation models for English, multilingual foundation models have been released to extend that success to non-English languages. For most of the over 6,000 languages in the world, the text data available is not enough to train a large-scale foundation model. To give one example, there are over 65 million speakers of Fula, a West African language, but few if any resources available for NLP in Fula [Nguer et al. 2020]. Multilingual foundation models address this by jointly training on multiple languages simultaneously. The multilingual foundation models to date (mBERT, mT5, XLM-R) are each trained on around 100 languages [Devlin et al. 2019; Goyal et al. 2021; Xue et al. 2020]. Joint multilingual training relies on the reasonable assumption that the shared structures and patterns between languages can lead to sharing and transfer from the high-resource languages to the low-resource ones, making foundation models possible for languages where we could not train a stand-alone model. Experiments using and analyzing multilingual foundation models have shown that there is indeed a surprising amount of transfer between and parallel encoding of the different languages in multilingual foundation models [Wu and Dredze 2019; Choenni and Shutova 2020; Pires et al. 2019; Libovicky` et al. 2019; Chi et al. 2020; Papadimitriou et al. 2021; Cao et al. 2019].
However, the extent to which these models are robustly multilingual is still an open question. It remains unclear how much models trained on this data can represent aspects of languages that are drastically different from English or for which few language resources are available [Wu and Dredze

On the Opportunities and Risks of Foundation Models

25

2020], and whether their apparent multilingual performance relies more on assimilation [Lauscher et al. 2020; Virtanen et al. 2019; Artetxe et al. 2020]. Multilingual models show better performance in languages that are similar to the highest-resource languages in their training data, and it has been shown that languages in multilingual models compete for model parameters, making it unclear how much variation can fit in a single model [Wang et al. 2020d]. A salient issue stems from the data that we use to train multilingual foundation models: in many multilingual corpora, English data is not only orders of magnitude more abundant than that of lower-resource languages, but it is often cleaner, broader, and contains examples showcasing more linguistic depth and complexity [Caswell et al. 2021] (see Nekoto et al. [2020] on building participatory and robust multilingual datasets). However, the answer does not simply lie in creating more balanced corpora: there are so many axes of language variation that it would be infeasible to create a corpus that is balanced and representative in all regards. The future, versatility, and equity of foundation models all depend on robustly handling language variation despite unbalanced data [e.g., Oren et al. 2019].
Current multilingual foundation models in their raw form, and naive unsupervised multilingual training as a method, may not model the subtleties of languages and language varieties to their full extent. Nevertheless, they remain useful for some multilingual applications, for example through adapting multilingual models for low-resource languages not in their original training set [Wang et al. 2020b]. Moreover, the results for the (non-public) GShard neural machine translation model show the largest gains over monolingual baselines for the lowest resource languages, with the gains increasing with model size [Lepikhin et al. 2021]. The research community should critically examine how foundation models deal with language variation, understand the limits of foundation models in bringing equity and representation to NLP, and not settle on promoting foundation models that erase language variation and mostly conform to the linguistic majority in their training data.
2.1.4 Inspiration from human language acquisition.
Though foundation models have constituted a huge source of progress in creating NLP systems that act more like humans, there are still significant ways in which the linguistic system that they acquire, as well as the learning process, differ from human language. Understanding the implications of this gap between machine and human language learning is a necessary part of developing a research community informed about the linguistic limits and possibilities of foundation models.
Human language acquisition is very efficient: foundation models like GPT-3 are trained on around three to four orders of magnitude more language data than most humans will ever hear or read, and certainly much more than children have been exposed to by the time they are mostly linguistically competent. One salient difference between foundation models and human language acquisition is that human language is grounded to the real world [Saxton 2017]. For example babies and caretakers point to objects during language development [Colonnesi et al. 2010], and babies learn the grounded meanings of words that refer to common objects before they learn a lot of the other aspects of the linguistic system [Bergelson and Swingley 2012]. Most foundation models used in NLP, on the other hand, learn from the distributional information of raw, ungrounded text, and (in contrast to human learners) Zhang et al. [2021] show that RoBERTa models express abstract syntactic features before usable meaning. Powerful ungrounded statistical learning is indeed also present in babies [Saffran et al. 1996], so it is no doubt an important factor in acquisition. Nevertheless, advancing grounded language learning for foundation models remains an important direction for approaching human acquisition efficiency [Dupoux 2018; Tan and Bansal 2020; Zellers et al. 2021a, inter alia] (see §2.2: vision and §2.3: robotics for the multimodal potential of foundation models, and §2.6: philosophy for a discussion of whether foundation models can understand language without grounding). Another important direction is examining the inductive biases in foundation

26

Center for Research on Foundation Models (CRFM)

Fig. 6. Language Acquisition for humans and foundation models. While there are certainly different inductive
biases between the human brain and foundation models, the ways that they learn language are also very
different. Most saliently, humans interact with a physical and social world in which they have varied needs
and desires, while foundation models mostly observe and model data produced by others.
models and how they relate to the inductive biases in the human mind, both those specific to language learning and those general to human cognition [Linzen and Baroni 2021]. Though the human brain may be more architecturally specialized for efficient language acquisition, foundation models are not blank-slate learners [Baroni 2021], and understanding and aligning these linguistic inductive biases is an important future direction for research in foundation models.
A significant factor in the efficiency of language acquisition is the fact that humans acquire a systematic and generalizable language system. Though there are many differing theories about what types of theoretical abstractions the human language system makes [e.g., Comrie 1989; Chomsky 2014; Croft 2001; Jackendoff 2011], it is generally agreed that humans learn language in a way that allows them to easily slot new knowledge into existing abstractions and productively create new grammatical sentences. For example, a ten-year-old child has acquired a lot of the abstractions about how their language works, though the actual words and constructions that they produce will change drastically over the next ten years. Foundation models, on the other hand, often do not acquire the systematic abstractions that we expect from humans. For example, when a foundation model produces a linguistic construction accurately one time there is no guarantee that future uses of that construction will be mostly consistent, especially after a significant domain shift in the subject matter [examples of work examining limitations of foundation models in systematicity include Lake and Baroni 2018; Kim and Linzen 2020; Bahdanau et al. 2018; Chaabouni et al. 2021]. NLP faces the challenge of developing some sort of systematicity in acquisition for foundation models, without regressing to systems that rely too heavily on rigid linguistic rules.
Language learning continues for a speaker’s whole lifetime: the grammar of human languages evolves, and humans flexibly adapt to novel linguistic situations [Sankoff 2018]. For example, as new terms and concepts arise in an adult’s life, they can use them relatively easily in grammatical

On the Opportunities and Risks of Foundation Models

27

sentences, and humans often adapt their grammatical patterns to fit in with different social groups [Rickford et al. 1994]. On the other hand, the linguistic system of foundation models is mostly set by the training data, and is relatively static [Lazaridou et al. 2021; Khandelwal et al. 2020]. Though adaptation methods can prime foundation models for different tasks (see §4.3: adaptation), it still remains unclear how to change the more basic linguistic foundation of a foundation model without a large amount of training. Making adaptable models that naturally mirror human-like linguistic accommodation and language evolution is an important research area for the future of foundation models.
Foundation models have drastically changed the research and practice of NLP. Foundation models have given rise to many new research directions for the community: understanding generation as a fundamental aspect of language, studying how to best use and understand foundation models, understanding the ways in which foundation models may increase inequities in NLP, examining whether foundation models can satisfactorily encompass linguistic variation and diversity, and finding ways to draw on human language learning dynamics. Most of the complex NLP tasks that the research community focused on before foundation models are now best handled, to an almost-human level, using one of a few publicly released foundation models. Nevertheless, there remain significant gaps between this performance and the needs for useful and safe deployment of foundation models in complex downstream settings.

28

Center for Research on Foundation Models (CRFM)

2.2 Vision
Authors: Shyamal Buch, Drew A. Hudson, Frieda Rong, Alex Tamkin, Xikun Zhang, Bohan Wu, Ehsan Adeli, Stefano Ermon, Ranjay Krishna, Juan Carlos Niebles, Jiajun Wu, Li Fei-Fei

Fig. 7. By harnessing self-supervision at scale, foundation models for vision have the potential to distill raw,
multimodal sensory information into visual knowledge, which may effectively support traditional perception
tasks and possibly enable new progress on challenging higher-order skills like temporal and commonsense
reasoning (§2.2.1: vision-capabilities). These inputs can come from a diverse range of data sources and
application domains, suggesting promise for applications in healthcare and embodied, interactive perception
settings (§2.2.2: vision-challenges). Image credits [Zamir et al. 2018; Haque et al. 2020].
Vision underlies one of the primary modes through which a living organism understands its environment. The ability to see enables the near-constant, long-range gathering of dense signals, a critical capability developed over an evolutionary time-scale in a diverse range of life forms [Parker 2003; Zhang and Shu 2021]. For a skill executed effortlessly by even simple living creatures, transferring the same abilities to machines has proved remarkably challenging, leading computer vision and robotics researcher Hans Moravec in 1988 to observe a paradox: in AI, (what were considered) hard problems are easy and likewise easy problems are hard, and among the “easiest” problems of them all is the visual acuity which we use each day to continually interpret complex scenes in a matter of milliseconds [Moravec 1988; Thorpe et al. 1996; Fei-Fei et al. 2007].
On the other end of this formidable challenge is the substantial scope of transformative applications which computer vision holds the key to: self-driving cars that can free commuters from gridlock (§2.3: robotics), life-saving AI tools that can assist overworked specialists by detecting rare medical events (§3.1: healthcare), next-generation tools for multimedia creation and editing (§2.5: interaction), among others. Reflecting on the applications and settings where human perception is instrumental offers a sense of the potential areas where computer vision can assist and transform.

On the Opportunities and Risks of Foundation Models

29

The field of computer vision and the challenges we define draw inspiration in many ways from human perception capabilities. Several classical theories [e.g., Biederman 1972; McClelland and Rumelhart 1981; Marr 1982] suggested that humans may perceive real world scenes by contextualizing parts as a larger whole, and pointed the way for computer vision techniques to progressively model the physical world with growing levels of abstractions [Lowe 1992; Girshick et al. 2014]. Gibson [1979] suggested that human vision is inherently embodied and interactive ecological environments may play a key role in its development. These ideas continue to motivate the ongoing development of computer vision systems, iterating towards a contextual, interactive, and embodied perception of the world.
In the context of computer vision, foundation models translate raw perceptual information from diverse sources and sensors into visual knowledge that may be adapted to a multitude of downstream settings (Figure 7). To a large extent, this effort is a natural evolution of the key ideas that have emerged from the field over the last decade. The introduction of ImageNet [Deng et al. 2009] and the advent of supervised pretraining led to a deep learning paradigm shift in computer vision. This transition marked a new era, where we moved beyond the classic approaches and task-specific feature engineering of earlier days [Lowe 2004; Bay et al. 2006; Rosten and Drummond 2006] towards models that could be trained once over large amounts of data, and then adapted for a broad variety of tasks, such as image recognition, object detection, and image segmentation [Krizhevsky et al. 2012; Szegedy et al. 2015; He et al. 2016a; Simonyan and Zisserman 2015]. This idea remains at the core of foundation models.
The bridge to foundation models comes from the limitations of the previous paradigm. Traditional supervised techniques rely on expensive and carefully-collected labels and annotations, limiting their robustness, generalization and applicability; in contrast, recent advances in self-supervised learning [Chen et al. 2020c; He et al. 2020] suggest an alternative route for the development of foundation models that could make use of large quantities of raw data to attain a contextual understanding of the visual world. Relative to the broader aims of the field, the current capabilities of vision foundation models are currently early-stage (§2.2.1: vision-capabilities): we have observed improvements in traditional computer vision tasks (particularly with respect to generalization capability) [Radford et al. 2021; Ramesh et al. 2021] and anticipate that the near-term progress will continue this trend. However, in the longer-term, the potential for foundation models to reduce dependence on explicit annotations may lead to progress on essential cognitive skills (e.g., commonsense reasoning) which have proven difficult in the current, fully-supervised paradigm [Zellers et al. 2019a; Martin-Martin et al. 2021]. In turn, we discuss the potential implications of foundation models for downstream applications, and the central challenges and frontiers that must be addressed moving forward (§2.2.2: vision-challenges).
2.2.1 Key capabilities and approaches.
At a high-level, computer vision is the core sub-field of artificial intelligence that explores ways to endow machines with the capacity to interpret and understand the visual world. It encompasses a multitude of tasks, sub-domains and downstream applications, where the community has made continual progress over the last several decades [Zamir et al. 2018]. A selection of example tasks16: (1) semantic understanding tasks, which aim to discover the properties and relations among entities within visual scenes; these include image classification, object detection, semantic segmentation, action recognition, and scene graph generation, among others [e.g., Krizhevsky et al. 2012; He et al. 2016a; Krishna et al. 2017; Russakovsky et al. 2015; Krizhevsky et al. 2009; Kay et al. 2017; Lin et al. 2014]. (2) geometric, motion and 3D tasks, seeking to represent the geometry, pose and structure
16This, of course, is a coarse selection: please see the categories at the annual conference on Computer Vision and Pattern Recognition (CVPR) for a more complete (but evolving) picture of the tasks in the field.

30

Center for Research on Foundation Models (CRFM)

of still or moving objects, and include tasks of depth estimation, structure-from-motion, surface normal detection, curvature line and keypoint estimation, to name a few [e.g., Laina et al. 2016; Agarwal et al. 2011; Wang et al. 2015a; Zamir et al. 2018; Ullman 1979]. (3) multimodal integration tasks, combining semantic and geometric understanding with other modalities such as natural language; these include, for instance, visual question answering, image captioning, and instruction following [e.g., Antol et al. 2015; Chen et al. 2015b; Anderson et al. 2018; Goyal et al. 2017b; Hudson and Manning 2019b; Johnson et al. 2017; Luo et al. 2020; Akbari et al. 2021; Huang et al. 2021c; Tsimpoukelli et al. 2021]. We highlight a subset of traditional core tasks in Figure 7.
The predominant paradigm for addressing these tasks, driven by the emergence of ImageNet [Deng et al. 2009] during the early 2010s, tends to center around a familiar core idea: First, pretrain a model on a large collection of carefully annotated data [Russakovsky et al. 2015] with a fully supervised training task, like image classification. Then, adapt the model downstream on taskspecific datasets and domains [Lin et al. 2014; Chen et al. 2015b; Antol et al. 2015] by fine-tuning to reach state-of-the-art performance [Krizhevsky et al. 2012; Simonyan and Zisserman 2015; He et al. 2016a; Xu and Saenko 2016]. This notion of pretraining followed by adaptation persists in the definitions we consider now for foundation models (§1: introduction). The limitations of this fully supervised paradigm motivate the transition to foundation models: the reliance on external supervised annotations constrains the upper bound capability of previous approaches to capture the diverse spectrum of visual inputs in a scalable, robust and generalizable manner. Recent developments in the domain of visual synthesis and unsupervised learning offer a compelling alternative. GANs, for instance, learn to generate visual content of high fidelity, realism and diversity, by featuring two competing networks of a generator and a discriminator that can supervise one another from image collections alone [e.g., Goodfellow et al. 2014; Hudson and Zitnick 2021]. Other neural models infer the visual properties of objects and scenes without explicitly annotated supervision, by employing variational auto-encoding, contrastive learning or other self-supervised techniques [e.g., Kingma and Welling 2014; Chen et al. 2020c; He et al. 2020]. For instance, He et al. [2021] build upon prior work on representation learning with masked image encoding [e.g., Pathak et al. 2016; Vincent et al. 2008] by, in part, combining recent advancements in flexible architectures (e.g., vision transformers [Dosovitskiy et al. 2021; Zhai et al. 2021]) with increased scaling.
With foundation models, the development of such self-supervision techniques has enabled training at greater scales of visual data [Changpinyo et al. 2021], both in terms of its scope as well as its potential diversity. Accordingly, we have seen early indicators of progress on traditional vision tasks in terms of both standard accuracy metrics and few-shot generalization. For image classification and object detection, self-supervised techniques have reported competitive performance to prior fully-supervised approaches [He et al. 2019; Chen et al. 2020c; Radford et al. 2021; Hénaff et al. 2021], without explicit annotations during training and greater sample efficiency during adaptation. For visual synthesis, notable examples include DALL-E [Ramesh et al. 2021] and CLIP-guided generation [Radford et al. 2021; Galatolo et al. 2021], where researchers leverage multimodal language and vision input to render compelling visual scenes. In the short-term, we anticipate that the capabilities of these foundation models will continue to improve along these directions, as training objectives are refined [Chen et al. 2020a; Hénaff et al. 2021; Selvaraju et al. 2021] and architectures are designed to incorporate additional modalities [Jaegle et al. 2021b].
Notably, current foundation models for computer vision are nascent relative to their NLP counterparts (§2.1: language): promising early efforts are still largely centered on RGB image inputs and a subset of core traditional vision tasks. However, the field continues to progress on broader challenges centered on embodied and interactive perception settings (critical for foundation models for robotics [Bohg et al. 2017, §2.3: robotics]). We note a subset of these higher-order goals in Figure 7, including physical scene understanding, reasoning over visual commonsense and temporal

On the Opportunities and Risks of Foundation Models

31

events, and perception for social affordances. Each of these have been goals for fully-supervised systems, but have proven challenging in part due to the difficulty of annotating these tasks at scale. For instance, standard systems for visual-question answering struggle to answer questions that require commonsense understanding, since these questions often require external knowledge beyond what is present in the pixels alone [Zellers et al. 2019a]. Perceiving human gaze and social affordances in a robust manner remain ongoing challenges for embodied vision systems in interactive agents [Martin-Martin et al. 2021]. By reducing the dependence on explicit annotations, foundation models may enable further progress towards these goals than was previously feasible. Related progress in language foundation models (§2.1: language), which have been able to capture a degree of commonsense over language events [Brown et al. 2020], also suggests a potential avenue towards achieving similar capability over multimodal visual inputs. While the exact roadmap for how to achieve these capabilities in foundation models remains an open problem, a combination of new efficient and flexible architectures (§4.1: modeling), large-scale training (§4.5: systems), self-supervision techniques (§4.2: training) and few-shot adaptation schemes (§4.3: adaptation) may open the door towards capabilities that have been difficult to reach so far.
2.2.2 Central research challenges.
Our discussion of research challenges is motivated by the downstream application domains where foundation models may further the integration and impact of vision models. We highlight a few such areas: (1) ambient intelligence for healthcare and home environments: building upon existing approaches for ambient intelligence in these settings [Haque et al. 2017; Lyytinen and Yoo 2002; Hong and Landay 2004], foundation models may offer the potential for better detection of finegrained human activities and medical events, as well as improved assistive interaction for clinicians, patients, and everyday consumers (see also §3.1: healthcare). (2) mobile and consumer applications: foundation models with stronger multimodal grounding may enable more capable interactivity of services in mobile settings, and fundamental improvements in generation capability from vision and language inputs can benefit computational photography and content editing applications [Delbracio et al. 2021; Ramesh et al. 2021; Park et al. 2019] (see also §2.5: interaction). (3) embodied, interactive agents: perception models have already proven effective as both inputs [Sermanet et al. 2018] and reward functions [Chen et al. 2021c; Shao et al. 2020] in robotics settings; foundation models trained on large collections of egocentric (real/simulated, human/robotic) visual data [Damen et al. 2018; Chen et al. 2021e] may potentially further this progress by capturing a wider distribution of visual scenes, objects, and actions (see also §2.3: robotics).
The extent to which foundation models may further impact these application settings hinges on the degree to which the capabilities outlined in §2.2.1: vision-capabilities are realized. To bridge the significant gaps between present, short-term, and long-term anticipated capabilities, we must address current limitations of foundation models for vision, including their training and evaluation. Below, a subset of corresponding key challenges:
Semantic systematicity and perceptual robustness. Humans have a remarkable capacity for generalizing visual understanding to unseen compositions, and reasoning about the physical and geometric properties of novel objects and scenes [Lake et al. 2015]. While current foundation models have shown promising capability for image synthesis and early results for generalization to fine-grained language inputs, these models still struggle to generalize to compositions of simple shapes and colors [Ramesh et al. 2021; Radford et al. 2021; Rong 2021]. Generalizability goes beyond semantics as well; visual scenes and objects have a natural regularity to their physical dynamics and geometric properties. Foundation models have shown early indications of understanding scene and object geometry [Ramesh et al. 2021]. Further, early efforts towards physical scene and

32

Center for Research on Foundation Models (CRFM)

geometric understanding in perception models may provide guidance for ongoing foundation model development [Yi et al. 2019; Bakhtin et al. 2019; Li et al. 2020b]. Indeed, the continued incorporation of multiple modalities (e.g., audio) in foundation models may prove beneficial towards these aims [Zhang et al. 2017; Gao et al. 2020b; Jaegle et al. 2021a]. However, the specific techniques to enable generalizing the initial observed capabilities robustly to a wide range of natural scenes and objects at the level of humans remains an open research challenge for foundation models.
Computational efficiency and dynamics modeling. Humans are surprisingly efficient at processing the continuous visual stream of objects, scenes, and events necessary to support an understanding of event dynamics [Zacks et al. 2001; Tversky and Zacks 2013]. Foundation models in language (§2.1: language) have shown initial steps towards modeling longer-term coherence of events; the analogous ability to capture long-range temporal correlations and causal coherence in visual input would stand to benefit downstream settings like robotics [Dai et al. 2019; Alyamkin et al. 2019; Goel et al. 2020b; Feng et al. 2019, §2.3: robotics]. However, relative to word token-level inputs in language, low-level computer vision inputs are extremely high-dimensional: a single 1080p frame contains over 2 million pixels. In this context, modeling the richer event dynamics in long-range video sequences seems like a daunting endeavor, especially with additional modalities (e.g., speech, optical flow, etc.) and increasing resolutions. Understandably, a naïve approach to fully processing every individual pixel is likely prohibitive. Current vision models [e.g., Radford et al. 2021; Sun et al. 2019a; Tan and Bansal 2019; Kim et al. 2021a] often address this by processing embeddings that summarize image patches or even groups of frames altogether, but this has the potential drawback of losing fine-grained details [Ramesh et al. 2021]. In addition to considerations of the raw input space, foundation models for vision may need to revisit the design of fundamental architecture primitives (§4.1: modeling) for efficient and effective modeling: alternatives to 3D convolutions may better address its cubic complexity [Fan et al. 2020; Sitzmann et al. 2019], while particle-based representations may prove more effective for modeling physical dynamics [Bear et al. 2021]. Further, deployment of these vision models to downstream application settings will also necessitate advancements in systems design (§4.5: systems). Taken together, the bottleneck of efficient and effective modeling for larger-scale, dynamic vision inputs remains a multi-faceted research direction that must be addressed going forward.
Training, environments, and evaluation. Equally critical to realizing the potential of foundation models are the supporting elements for training and evaluating them. Current foundation models for vision have largely focused on a small subset of modalities shown in Figure 7 (e.g., datasets of RGB images and text), since these are perhaps the most readily accessible [Changpinyo et al. 2021; Radford et al. 2021]. This motivates the development and use of additional large-scale training datasets which contain a diverse collection of inputs across a broad spectrum of modalities. While additional annotations may not strictly be necessary, the input quality impacts the learning efficiency of the models; techniques that leverage foundation models of other types (e.g., language) to help improve quality are a promising route forward [Zellers et al. 2021b]. We also want to consider settings beyond static datasets: classic studies have suggested that perceptual understanding in humans is linked to its embodiment and interactive, ecological settings [Gibson 1979]. As stepping stones towards longer-term capabilities of embodiment and interaction (§2.3: robotics), ongoing development of simulation environments that capture physical, visual, and ecological realism with multiple modalities and viewpoints may play an important role in providing scalable and high-fidelity visual inputs for this goal [Kolve et al. 2017a; Savva et al. 2019b; Gan et al. 2020; Shen et al. 2021a; Srivastava et al. 2021]. Finally, there is the question of metrics: how do we evaluate the faithfulness of generative foundation model outputs with respect to semantics? Standard metrics like Fréchet Inception Distance, suffer from known flaws [Bińkowski et al. 2018]; such issues

On the Opportunities and Risks of Foundation Models

33

parallel ones in natural language processing (e.g., metrics like BLEU do not correlate with causal judgements from humans). Having human judgements as part of evaluation may be one route, but incurs significant cost and may not be as scalable [Zhou et al. 2019; Khashabi et al. 2021]. The outstanding and open challenges surrounding the training (§4.2: training), data (§4.6: data), and evaluation (§4.4: evaluation) settings for vision foundation models are indeed quite nuanced, and will be a central area of research going forward.
Concluding remarks. In this section, we explored foundation models in the context of computer vision, from identifying roots in previous computer vision paradigms, to contextualizing its current and anticipated capabilities, to proposing research directions moving forward. We conclude with a brief discussion of some broader societal implications of foundation models for computer vision and their continued development (see also §5: society). The ubiquity of cameras in our society means that advances in computer vision techniques have great potential for disruptive impact; this carries a corresponding burden of responsibility for careful consideration of its risks. There is a well-documented history of learned bias in computer vision models, resulting in lower accuracies and correlated errors for underrepresented groups, with consequently inappropriate and premature deployment to some real-world settings [e.g., Buolamwini and Gebru 2018, §5.1: fairness]. Many of the same underlying issues continue to persist in current foundation models [Agarwal et al. 2021]. As data from additional sensor modalities (e.g., wearable or ambient sensors, Figure 7) become incorporated in these foundation models, concerns surrounding privacy and surveillance become paramount (see §5.6: ethics). Furthermore, generated deepfake images and misinformation pose greater risks as the semantic and generative capability of vision foundation models continues to grow [Dolhansky et al. 2020; Ramesh et al. 2021, §5.2: misuse]. While the intriguing open challenges and opportunities ahead for computer vision and foundation models are significant, addressing these and related risks concurrently remains essential.

34

Center for Research on Foundation Models (CRFM)

2.3 Robotics
Authors: Siddharth Karamcheti, Annie Chen, Suvir Mirchandani, Suraj Nair, Krishnan Srinivasan, Kyle Hsu, Jeannette Bohg, Dorsa Sadigh, Chelsea Finn

Fig. 8. Building new types of foundation models for robotics will require massive datasets spanning diverse
environments and behaviors. Simulation, robotic interaction, videos of humans, and natural language descrip-
tions could all be useful data sources for these models. Despite the challenges of acquiring data, developing
new foundation models for robotics has tremendous potential for a variety of problem formulations in task
specification and robot learning. Image credits: [Finn et al. 2016b; Szot et al. 2021].
A longstanding challenge of robotics research is to endow robots with the ability to handle the myriad conditions they will encounter in real-world settings. In this section, we discuss how the ideas underlying foundation models can potentially help bring about “generalist” robots that can, for example, cook a new meal in a new house, with a new kitchen. To make progress towards this goal, existing foundation models will not suffice. We need new types of models trained on a multitude of data sources, spanning grounded robotic interaction data to videos of humans performing tasks, amongst others. We focus on how such foundation models can apply to the problem of a robot controlling its own physical embodiment to successfully perform different tasks. This is a high-dimensional and closed-loop decision-making problem: the actions that a robot takes directly influence what it perceives next, which in turn influences the next robot action. This closed-loop aspect is not traditionally studied in language and computer vision, where large offline datasets are dominant and foundation models have already seen success. We focus on how the demonstrated benefits of foundation models — large-scale, self-supervised learning — can be leveraged in this new closed-loop data regime. The promise of a new type of robotic foundation model is in its ability to amplify the potential of robots to improve key facets of daily life ranging from manufacturing [Nof 1999; Sanneman et al. 2020], construction [Khoshnevis 2004; Bock 2007], autonomous driving [Thorpe et al. 1988; Badue et al. 2020], to household aid [Thrun and Mitchell 1995; Brooks 2002; Dillmann 2004; Goodrich and Schultz 2007; Gupta et al. 2018; Shridhar et al. 2020] and personal assistance [Dragan and Srinivasa 2013; Javdani et al. 2018], amongst others. Our discussion in this section primarily focuses on mobile manipulation robots for household tasks, but we expect its essence to be broadly applicable to the other use-cases of robotics listed above.
On the critical path towards building new types of foundation models for robotics is embracing opportunities in task specification and task learning, coupled with tackling challenges in data acquisition and safety and robustness. Consider the following robot learning paradigm: starting with

On the Opportunities and Risks of Foundation Models

35

a description of a task capturing what a user might like the robot to do (e.g., “make breakfast”) — learn a corresponding policy to generate the desired robot actions. While policies can be parameterized in different ways, a common choice is that of a function that maps the task representation and environment observation (e.g., a scene image from a fixed or egocentric camera, or inputs from alternative sensors like LIDAR) to robot actions [Andrychowicz et al. 2017; Nair et al. 2018]. As the robot acts in a task-conditioned manner, the subsequent states are fed back to the policy, generating more actions until the task has been satisfied.
Yet, implementing such a paradigm in practice is difficult. To begin, what is the right interface for describing one’s goals? For a given user in one context, “make breakfast” carries an implication of a full breakfast that consists of fried eggs, toast, and a glass of orange juice; for another user, “make breakfast” may imply idlis with sambar and a tumbler of filter coffee. In general, high-level context-dependent goals like these do not stand alone and can introduce a multitude of ambiguities. How does one specify a goal (and corresponding subgoals) with enough clarity to both resolve these ambiguities, and in so doing, allow a robot to make progress on the given task? Additionally, how might we craft general task representations that might aid generalization to similar objectives (e.g., fetching a glass of milk instead of orange juice). Going a step further, how do we build methods that aid robots in learning policies for new tasks and new environments (in this case, a brand new kitchen with new utensils, appliances, layouts, etc.)?
Recent breakthroughs in applying foundation models for language and vision (§2.1: language and §2.2: vision) suggest several potential benefits of large-scale, self-supervised pretraining for improving generalization. The ability to tap into diverse streams of data to learn meaningful representational priors (akin to those learned by models such as BERT and GPT-3) holds promise for learning powerful robotic foundation models for task specification. Diverse robotic interaction data can be used for learning action-conditional dynamics models or policies indexing general and semantically meaningful skills thereby holding promise for task learning. Yet while these opportunities exist, the key stumbling block is collecting the right data. Unlike language and vision data, robotics data is neither plentiful nor representative of a sufficiently diverse array of embodiments, tasks, and environments — we (as a field) still have not converged on the kinds of data that would be maximally useful for enabling generalist robotics (e.g., offline demonstrations, thirdperson recordings of humans, egocentric videos, autonomous experience, etc.) Coupled with issues in obtaining the right scale and diversity of data are questions of ensuring safety and robustness: how do we behave in a new environment without causing damage?
Building new types of foundation models for robotics thus consists of a dichotomy of opportunities and challenges: opportunities for task specification and learning balanced against challenges of data collection and safe deployment. This section explores both by presenting a picture of how robotic foundation models might help us develop generalist robots, in a way that not only meaningfully addresses the challenges associated with building such systems, but that also embraces the potential of multi-modality — incorporating perception, actuation, and language — as well as human-robot interaction for specification and learning.

2.3.1 Opportunities.
Robotic foundation models could take a variety of forms: problems in robotics do not easily conform to a one-size-fits-all model, since different problems have different input-output signatures — a contrast to domains like NLP where many problems can be cast into a general “text-in, text-out” signature. We focus on opportunities in generalizable task specification and learning across tasks, environments, and robot embodiments.

36

Center for Research on Foundation Models (CRFM)

Foundation models for task specification. Before robots can learn how to solve tasks in a general purpose way, they must understand what the desired task is: for example, to be useful in a new kitchen, a robot needs to know what we would like it to cook, as well as behaviors we would like it to avoid. Therefore, a necessary first step towards developing generalist robots is building a new type of foundation models for reliable task specification, i.e., the intuitive and effective communication of task objectives, preferences, and constraints. We formalize task specification as a process that transforms a human-provided task description into a quantitative metric that measures a robot’s task completion and progress — e.g., a reward function. This signal is crucial for optimizing robot behavior, diagnosing failures, and prompting human feedback. As the most natural way to describe a task can vary depending on the user, environment, or task, robotic foundation models for task specification should accept a variety of description modalities, such as goal states [Fu et al. 2018; Singh et al. 2019], natural language [MacGlashan et al. 2015; Karamcheti et al. 2017; Misra et al. 2017b; Co-Reyes et al. 2019; Shao et al. 2020], videos of humans [Shao et al. 2020; Chen et al. 2021c; Liu et al. 2018], pairwise or ranking comparisons [Biyik and Sadigh 2018], interactive corrections [Co-Reyes et al. 2019; Karamcheti et al. 2020] and physical feedback [Ross et al. 2011; Bajcsy et al. 2017].
An important requirement of general purpose models for task specification is the ability to transfer to new environments and tasks. Reliably transforming task descriptions into generalizable reward signals for robot learning remains an open problem [Taylor et al. 2016] — one that robotic foundation models will arguably be well suited for. When applied to task specification, such models should provide more robust (§4.8: robustness) reward signals by learning from large and broad datasets — even leveraging multiple of the description modalities listed above. One possible instantiation of a new foundation model for task specification might be to learn a mapping from arbitrary (language, current observation) pairs to reward signals by training on diverse language and vision datasets [Bahdanau et al. 2019; Fu et al. 2019; Chen et al. 2021c]. By learning informative priors from these broad, diverse datasets, such a model may be able to generalize to unseen language instructions and observations in unseen environments. In general, the potential for new foundation models to be able to deftly bridge modalities and generalize broadly make them appealing for general purpose task specification.
Foundation models for task learning. In addition to enabling more general task specification, robotic foundation models could make learning to solve new tasks more efficient and reliable. In this context, these new types of foundation models might take the form of a joint distribution over actions, sensor observations, rewards, and other properties of interest. Conditioning on different dimensions of this joint distribution recovers different inference problems, each corresponding to a different signature:
• Dynamics modeling: 𝑝(future observations | actions, past observations) [Finn and Levine 2017; Hafner et al. 2019; Wu et al. 2021d].
• Policy learning: 𝑝(actions | observations, goal) [Kaelbling 1993; Schaul et al. 2015; Ding et al. 2019].
• Inverse reinforcement learning: 𝑝(reward function | observations, actions) [Ng and Russell 2000; Ziebart et al. 2008; Finn et al. 2016a].
A plausible training objective for a robotic foundation model is to predict the different elements of the joint distribution described above in an autoregressive fashion [Janner et al. 2021; Chen et al. 2021b, §4.1: modeling]. However, these are not the only options. In particular, robot datasets contain large amounts of unlabeled data consisting of synchronized observations from many different sensor modalities (e.g., RGB and depth cameras, haptic sensors, microphones, etc.) and a sequence of actions that the robot has performed to generate these observations. Beyond the objectives above,

On the Opportunities and Risks of Foundation Models

37

a robotic foundation model could be trained to predict observations of one sensor modality from another or to predict whether two streams of sensory observations are from the same segment of time. These kinds of self-supervised objectives can leverage multi-modal correspondences to produce low-dimensional representations of high-dimensional data, and can even be combined with the above objectives to yield models, policies, and rewards on top of those representations.These objectives may facilitate the training of powerful robotic foundation models from unlabeled data — as long as the data exhibits diverse, meaningful behavior. §2.3.2: robotics-challenges discusses the challenges of collecting such data further.
In language and vision, foundation models have demonstrated the capability to learn broadly applicable priors from large, diverse datasets, that can be subsequently adapted to downstream tasks (§2.1: language, §2.2: vision). Robotic foundation models have the potential to similarly enable few-shot adaptation of perception and control to new environments, tasks, and embodiments though leveraging different data, self-supervised objectives, and modalities than have been studied with existing language and vision models. Consider our running kitchen example. To cook in a new kitchen, a robot needs to adapt to the specific environment — its spatial layout, the available equipment, etc. Priors learned from offline videos of humans, robotic interaction, text, and/or simulation might encode general aspects of kitchens, such as the fact that stoves are usually against walls and must be turned on in order to produce heat. Such commonsense knowledge, physical priors, and visual priors could make adaptation to new environments more sample efficient. Similarly, developing a new foundation model for robot task learning might enable the use of a large number of cooking videos in its training dataset to adapt a policy for a common skill, such as “fry an egg,” to a specific user’s preferences from a low number of demonstrations — allowing for sample efficient adaptation. Finally, with their potential to learn the cross-modal representations described earlier, robotic foundation models could help enable adaptation to new embodiments. This aspect of adaptation is crucial to make these models widely useful.

2.3.2 Challenges and risks.
Despite this exciting vision, multiple challenges need to be overcome. To enable the generalization discussed above, we must collect robotic datasets of sufficient size and diversity. Additionally, we need mechanisms to ensure that we can deploy learned behaviors safely in the real world.

Data needs & challenges. Learning a policy for a robot that perceives the state of its environment via sensors and takes actions to accomplish tasks traditionally requires large datasets of the robot interacting in the real world. On the other hand, many learning tasks in computer vision and natural language processing rely on large and diverse offline datasets that can easily be scraped from the web. Motivated by the advances of existing foundation models in language and vision, we are excited by the possibility of leveraging large offline data sources for training new foundation models for robotics.
One path towards this goal is collecting large datasets for offline learning, for example using teleoperation [Mandlekar et al. 2019], kinesthetic teaching [Sharma et al. 2018], or autonomous methods [Pinto and Gupta 2016; Gupta et al. 2018; Levine et al. 2018; Dasari et al. 2019; Kalashnikov et al. 2021; Chen et al. 2021d], which have shown some promising indications on generalization. While scaling up robot data collection to the size of vision and language datasets [Deng et al. 2009; Krishna et al. 2017; Raffel et al. 2019; Gao et al. 2020a] remains an open challenge, the increasing scale and quality of robotic datasets suggests they can play an important role in learning foundation models for robotics. Moreover, because robots have the ability to actively and autonomously shape their environment, they should be able to generate targeted unlabeled data at scale.

38

Center for Research on Foundation Models (CRFM)

Given the challenging closed-loop nature of learning control, it is possible that collecting datasets of size comparable to those used in vision and language is insufficient for robotics. One exciting option is to additionally leverage external, non-robotic sources of data such as videos of humans or existing vision and natural language datasets. Such data is diverse and exists in large quantities on the web [Deng et al. 2009; Lee et al. 2012; Heilbron et al. 2015; Goyal et al. 2017a; Damen et al. 2018; Gao et al. 2020a; Grauman et al. 2021], affording the possibility of broad generalization if properly leveraged. Elegantly addressing the gap between the robot’s domain and those found in videos or language on the web remains an open challenge; however, recent progress in domain adaptation [Smith et al. 2019; Schmeckpeper et al. 2020] and using pretrained video and language models in robotics [Lynch and Sermanet 2020; Shao et al. 2020; Chen et al. 2021c] present promising directions towards closing this gap.
Finally, simulation presents a boundless source of rich interactive data that robots can learn from, with a range of sensor modalities like rendered visuals, point-clouds, and simulated touch/audio. However, a major challenge lies in bridging the gap between simulation and the real world, both in the underlying physics and in the semantic distribution of environments and tasks. Recent work has shown that by using extensive domain randomization, tasks ranging from flight [Sadeghi and Levine 2017] to contact-rich manipulation [Mahler et al. 2017; OpenAI et al. 2019] and locomotion [Peng et al. 2020; Hwangbo et al. 2019] skills learned in simulation can be transferred to real robots with some success, and that the semantic and visual distribution of the real world can be simulated by scanning the real world into a simulation [Chang et al. 2017; Kolve et al. 2017b; Savva et al. 2019a; Szot et al. 2021; Shen et al. 2021a]. While these are promising steps towards closing the sim-to-real gap, effective and general sim-to-real learning of manipulation and locomotion skills remains an open challenge. Simulation data, real robot data, videos of humans, and natural language data could all be essential to learning robotic foundation models.
Safety & robustness. Further complicating the development of new foundation models for robotics is ensuring their safety and robustness when training or deploying them in the real world. We can expect the safety risks from these models for robotics to be different from their language counterparts given that embodied agents are empowered to manipulate and interact with their surroundings directly in the physical world. One core safety challenge for learning-based systems is the chicken-and-egg problem of needing to specify system constraints for safety prior to collecting data, after which unforeseen unsafe behaviors requiring additional constraints may emerge. For instance, an agent adapting to a new kitchen outside of the training distribution requires sufficient safety guarantees to ensure safe data collection, which may either adversely affect task performance or cause the agent to fail in novel ways. One way to resolve this is restricting the complexity of the environment or increasing the complexity of the robot such that irrecoverable states or unsafe actions are avoided by construction. The robot can also be tasked with autonomously resetting the environment to facilitate uninterrupted learning (or adaptation) from large-scale data collection [Eysenbach et al. 2017; Gupta et al. 2021b]. This would either mean ensuring that nothing in the kitchen is breakable, or ensuring and replacing the items the agent may break while it attempts to collect data.
To address risks posed by robotic foundation models that fail to generalize or produce unexpected behaviors to new stimuli, potential future directions include developing a causal analysis of agents [Déletang et al. 2021], new formal safety evaluation tools, and realistic simulation environments [Corso et al. 2020; Dreossi et al. 2017; Julian and Kochenderfer 2019]. Finally, deriving formal safety guarantees for robotic foundation models, e.g., Hamilton-Jacobi reachability of safe-sets [Chow et al. 2018; Fisac et al. 2019; Herbert et al. 2021] or developing safety boundaries for learning that are interpretable (§4.11: interpretability) to human operators, could help reduce risks posed

On the Opportunities and Risks of Foundation Models

39

by such models [Berkenkamp et al. 2017]. As the development and study of these new types of foundation models progresses, solutions to these challenges will be crucial.
Conclusion. While the promise of robotic foundation models are many — spanning multiple levels of the robotics pipeline from task specification to task learning — the challenges are significant. Collecting data in the physical world that covers diverse environments and embodiments at scale is a sizable hurdle, and ensuring the safety and robustness of such systems is equally exigent. Despite this, our optimism prevails; tackling these challenges now, before developing models offers us the chance to identify ways to collect the right data, from the right sources, at the right scale to build safe and reliable robotic foundation models with the capabilities we desire.
Underpinning this section has been a theme of multimodality. Robotic foundation models — in all possible instantiations — have and will continue to benefit from work in other subfields of AI such as language and vision (§2.1: language, §2.2: vision). Yet as we consider incorporating these extensions from other fields, there are interdisciplinary challenges on the horizon that touch other aspects of foundation models: systems innovation for training and deploying such models for real-time robotics (§4.5: systems), innovation in interfaces for robust human-robot interaction (§2.5: interaction), and lessons to incorporate as we better grasp the safety and robustness of such models (§4.9: ai-safety, §4.8: robustness). Building a reliable ecosystem and thoughtful research practices around foundation models in general, and robotic foundation models in particular, is key to realizing these goals.

40

Center for Research on Foundation Models (CRFM)

2.4 Reasoning and search
Authors: Yuhuai Wu, Frieda Rong, Hongyu Ren, Sang Michael Xie, Xuechen Li, Andy Shih, Drew A. Hudson, Omar Khattab

Fig. 9. Multimodality can allow foundation models to not only reason with formal symbolic language, but also exploit visual aspects of the problem, such as equivalence, symmetry, and Euclidean geometry, to prune the infinite search space and find promising constructions for a solution (§2.4.1: reasoning-tasks), mimicking
the way humans reason about geometry problems.
Reasoning and search have been a central theme throughout the history of AI. Classic tests of intellect, from strategy games to abstract mathematical discovery, served as inspirational goal posts that pushed the limits of “machine intelligence” through a need to devise ever smarter ways of searching for winning solutions. In the early days, symbolic methods were the dominant approach for reasoning [Russell and Norvig 2020], but the involved engineering effort and the need to formalize heuristics to tackle intractable search spaces quickly proved cumbersome. More recently, data-driven methods using neural networks have shown encouraging results — e.g., defeating the best humans in Go [Silver et al. 2016], a board game with a much larger space of actions than the classic challenge of chess — by exploiting statistical structures and learning useful heuristics. This section outlines existing reasoning tasks, ones that require scaling to ever-larger search spaces and understanding the world broadly (§2.4.1: reasoning-tasks). We then argue in §2.4.2: reasoningrole that foundation models should play a central role towards general reasoning as vehicles for capturing the statistical regularities of unbounded search spaces (generativity), allowing positive transfer across tasks and scenarios (universality), and exploiting the grounding of knowledge in multi-modal environments (grounding).
2.4.1 What are the current tasks?
Many reasoning problems pose unbounded search spaces, where systems must deal with numerous kinds of open-ended alternatives. Consider trying to prove that the angles ∠𝐵 and ∠𝐶 are equal for an isosceles triangle △𝐴𝐵𝐶 with 𝐴𝐵 = 𝐴𝐶 (Figure 9). A system can perform any number of actions at each step of reasoning. For instance, the system could add a new auxiliary point with an arbitrary construction, say a perpendicular line, a parallel line, or a tangent circle, and the search space only grows larger as the diagram grows more complicated. One way to prove this theorem is to draw a line 𝐴𝐷 that is the angle bisector of 𝐴, and use the congruence of the two triangles △𝐴𝐵𝐷 and △𝐴𝐶𝐷 to show ∠𝐵 = ∠𝐶, but how can systems find this without extensive search?
More generally, a mathematician is not confined with searching in diagram constructions and Euclidean theorems: mathematicians can apply a vast number of theorems from various branches

On the Opportunities and Risks of Foundation Models

41

Fig. 10. Left: A reaction route for 1,6-Heptadiene-3,5-dione predicted by machine learning-based drug retrosynthesis planner AiZynthFinder [Genheden et al. 2020; Yoshikawa et al. 2021]. Right: A sample proof tree in propositional logic where the formulas outlined in green represent axioms. Although they arise from
different domains, both trees are structurally the same.
of mathematics, make high-level conjectures, formalize new mathematical concepts, or find counterexamples. This contrasts with more structured AI challenges such as the game of Go, whose search space is considered much smaller.17
Besides theorem proving, many real-world problems deal with unbounded search spaces, such as program synthesis [Gulwani et al. 2017], drug discovery [Drews 2000], chemical synthesis [Segler et al. 2018], computer-aided design [Haigh 1985], combinatorial optimization [Bengio et al. 2021], and more. These reasoning problems tend to exhibit similar structure, like the bijection between retrosynthesis in drug discovery and theorem proving in propositional logic, illustrated in Figure 10: in both problems, one is building a tree of synthesis, whose nodes are chemical products on the one side and propositions on the other, and the leaf nodes are the products on the one side, and end axioms on the other. In these problems, a simulated environment is often provided, which allows a solver to run several search threads towards building the solution tree. The simulator often provides intermediate feedback, say, informing the solver with the remaining propositions to establish before the proof is considered complete. The solver in turn needs to select the most promising search thread and proceed based on the intermediate feedback.
Recently, there has been a surge of interest in applying learning-based approaches to tackle reasoning problems. To overcome the unbounded search space challenge, researchers first started with a constrained search space to make the problem tractable [Huang et al. 2018; Bansal et al. 2019]. But such approaches suffered from the limited kinds of actions the solver could issue. For example, the solver could only apply theorems from a known database to prove the target theorem, instead of synthesizing novel theorems and lemmas. Because large language models offered a generic way of modeling the output space as a sequence, they quickly became a more favorable choice, allowing the generation of arbitrary kinds of actions. Researchers have applied these language model-based approaches to various applications, such as predicting protein structures [Senior et al. 2020], proving formal theorems [Polu and Sutskever 2020; Han et al. 2021], conjecturing theorems [Urban and
17Less than the number of grid points on the Go board (i.e., 361 actions for a 19×19 board).

42

Center for Research on Foundation Models (CRFM)

Jakubuv 2020; Rabe et al. 2021; Li et al. 2021b], synthesizing programs from natural language [Chen et al. 2021f; Ling et al. 2016], repairing, generating and understanding code [Yasunaga and Liang 2021; Lu et al. 2021b; Guo et al. 2020; Svyatkovskiy et al. 2020; Kim et al. 2021b; Zügner et al. 2021]. It has also been shown that scaling model size significantly improves reasoning capabilities [Polu and Sutskever 2020], and furthermore standard techniques from language modelling, such as pretraining, can also greatly improve performance on these tasks [Rabe et al. 2021; Polu and Sutskever 2020].
2.4.2 What’s the role of foundation models?
Generativity. We believe that the generative capabilities of foundation models are essential for effective reasoning. Due to the unbounded search space, it becomes intractable to enumerate all kinds of possibilities. Instead, with foundation models, one can model the distribution of the optimal decisions, and generate suitable candidates to proceed to the next step. In particular, as foundation models offer a generic way of modeling the output space as a sequence, the next decision generation is entirely unconstrained and hence universal. Such flexibility is essential for many of the reasoning challenges we discussed, to allow creative generation in domains such as mathematical conjecturing [Li et al. 2021b] and synthesizing novel programs [Chen et al. 2021f]. As one scales up foundation models, the capabilities of capturing such statistical structures also grow immensely [Polu and Sutskever 2020].
Universality. As we mentioned in the last section, many reasoning problems exhibit similar latent structures. We believe that the unifying framework imposed by a foundation model can transfer and share significant heuristics across tasks, ranging from generalizing low-level techniques that work well for one task to new scenarios all the way to directly finding meta-techniques that work well across numerous kinds of problems. In addition, since a foundation model is trained across many domains, it can positively transfer meta-knowledge encoded in the foundation models’ weights across tasks and domains [Papadimitriou and Jurafsky 2020; Wu et al. 2021f; Lu et al. 2021a]. The foundation model training and adaptation framework encourage a separation of concerns, where foundation model training learns meta-knowledge such as the shared search tree structure between drug retrosynthesis and propositional logic proofs, and the adaptation phase can focus on learning the task specific vocabulary. Thus, foundation models can reduce the complexity of the learning problem in the adaptation phase, improving sample complexity and generalization.
Grounding. Reasoning problems are often easily expressed in symbolic languages (e.g., mathematics, code, SMILE representation of molecules). However, these symbols have deep underlying semantic meanings — saying “isosceles triangle” paints a vivid image in the human mind. Foundation models can enable deep groundings and semantic meanings. First, grounding representations in other modalities, such as visual or physical, are essential to grasp abstract concepts in reasoning tasks and endow them with concrete meaning [Larkin and Simon 1987; Jamnik 2001]. Since the models may be trained on multiple modalities, foundation models can assist in understanding a range of data sources (e.g., images, texts). Hence, in the geometry example case, with its understanding of geometrical shapes learned in natural images, a foundation model could effectively utilize the diagrammatic representation of the problem. However, aligned multi-modal data in reasoning is scarce, and it remains an open question whether foundation models can discover connections between different modalities in an unsupervised manner (e.g., discovering of commutative diagram with the corresponding algebraic equations). Furthermore, even within the symbolic domain, symbols can have various levels of interpretation. For example, high-level programming languages can be translated to low-level assembly codes. Foundation models can learn a shared representation that encompasses these various views. Past works have shown that self-supervised tasks [Han et al.

On the Opportunities and Risks of Foundation Models

43

2021; Peng et al. 2021; Li et al. 2021a] allow the model to understand the inner workings behind the high-level code scripts, and further assist downstream tasks.
2.4.3 Future challenges in reasoning.
Due to the intrinsic difficulty of these problems, high-quality annotated data is scarce and harder to collect compared to raw images and text. There have been several attempts towards alleviating this issue. In mathematics, researchers proposed to generate synthetic theorems in the hope of generalizing to realistic theorems [Wang and Deng 2020; Wu et al. 2021a; Firoiu et al. 2021; Zhou et al. 2021c]. Another approach is to design self-supervised tasks to augment datasets [Yasunaga and Liang 2020; Ren et al. 2020; Han et al. 2021; Rozière et al. 2021; Yasunaga and Liang 2021], or better pretraining objectives [Wu et al. 2021f]. However, we still lack general principled approaches in designing self-supervised tasks, as most of the existing works are tailored to specific problem setups [Yasunaga and Liang 2020; Ren and Leskovec 2020; Han et al. 2021]. Building a foundation model will encourage a unifying framework of constructing a suite of self-supervised tasks that can be applied to all reasoning problems. In addition, interactivity (§2.5: interaction) could, with enough scalability, alleviate the data scarcity problem by bringing humans into the loop to minimally guide the learning curriculum or data augmentation process, for example, in selecting axioms to add or conjectures to explore, while interactive tools themselves are a motivating use of foundation models for reasoning [Han et al. 2021; Chen et al. 2021f] in assisting people with the most cognitively demanding or laborious aspects. Interpretation-friendly interactive tools could find further applications in education by assisting humans in learning with the help of highly capable foundation models (§3.3: education).
Improving the high-level reasoning capabilities is a core challenge for existing foundation models. Humans perform abstract reasoning and high-level planning in tackling difficult problem-solving tasks [Miller et al. 1960]. For example, when building a software tool or proving a theorem, we often start with a high-level sketch before delving into the low-level details [Koedinger and Anderson 1990]. Existing foundation models are not trained to generate such high-level plans. Instead, they often focus solely on predicting the next low-level steps [Polu and Sutskever 2020; Han et al. 2021; Chen et al. 2021f]. Unfortunately, to train foundation models to emulate human-like reasoning, we again face a data collection challenge. Although such data does exist in limited settings [Li et al. 2021b], in general, data for high-level reasoning is scarce and difficult to collect. One line of research is to let abstract and modular hierarchy to emerge by itself during learning [Ellis et al. 2021; Hong et al. 2021], but it still remains an open question how to scale these approaches to more general and realistic settings.
Aside from these challenges, there exist many open questions that are also essential to topics discussed in other sections. What constitutes a good architecture for reasoning reliably (§4.1: modeling)? How can we understand and interpret these models theoretically (§4.10: theory and practically §4.11: interpretability)? Can we train robust reasoning models that could generalize to out-of-domain problems (§4.8: robustness and §4.3: adaptation)? We believe research about foundation models on each of these fronts can greatly broaden their impact for the field of reasoning.

44

Center for Research on Foundation Models (CRFM)

2.5 Interaction
Authors: Joon Sung Park, Chris Donahue, Mina Lee, Siddharth Karamcheti, Dorsa Sadigh, Michael S. Bernstein

Fig. 11. Foundation models will bring significant opportunities to developers by lowering the difficulty
threshold for building AI-infused applications, and to the application users by raising the ceiling for what
types of interactions are achievable. In some cases, the line between developers and users will start to blur,
and users may be able to easily develop their own AI applications, for instance with natural language.
The early forms of foundation models such as GPT-3 [Brown et al. 2020] and DALL·E [Ramesh et al. 2021] have demonstrated a high level of versatility both in terms of their ability to let even nonML experts to prototype powerful AI-infused applications, and their ability to seamlessly integrate modalities ranging from texts to images. As the development of foundation models matures, the models’ capacity will continue to expand and their versatility may ultimately lead to fundamental changes in how we interact with AI by allowing us to rapidly prototype and build highly dynamic and generative AI-infused applications. In this section, we discuss the opportunities that these changes present from the perspectives of two important stakeholders: (1) applications developers who will interact with foundation models to design user experience, and (2) end-users who will use or be affected by the AI-infused applications powered by foundation models. Finally, we consider scenarios in which the line that rigidly separates developers and end-users today may start to blur, affording new opportunities for creating AI-infused applications that more closely satisfy users’ needs and values.
2.5.1 Impact on AI-infused application developers’ development process. How will foundation models transform the way developers create AI-infused applications? Despite the monumental progress in machine learning algorithms and systems infrastructure, some point out that designing novel and positive forms of human-AI interaction remains difficult [Dove et al. 2017; Cooper et al. 2014]. The vast amount of data, computing resources, and skills needed to create a powerful task-specific model is frequently in conflict with the iterative prototyping process necessary to elicit and satisfy users’ needs and values [Yang et al. 2016]. This challenge is further

On the Opportunities and Risks of Foundation Models

45

compounded by the fact that AI responses can be unpredictable, and models can produce a vast generative output space, making it difficult for people to build effective mental models of their performance. There has already been some progress on tackling these challenges in the form of work on interactive machine learning (e.g., Crayon [Fails and Olsen 2003], Regroup [Amershi et al. 2012]) and design frameworks for conveying uncertainty in AI to end-users (e.g., principles of mixedinitiative [Horvitz 1999]). However, more work is still needed to overcome these obstacles [Yang et al. 2020].
Foundation models pose important opportunities to address many of the challenges mentioned above. For instance, language-based foundation models’ ability to take natural language as input, and to generalize to many downstream tasks, could significantly lower the difficulty “threshold” [Myers et al. 2000] for application development, i.e., by enabling the development of sophisticated models without having to collect significant amounts of data and train large models from scratch. This could enable even non-ML experts to quickly prototype AI-infused applications. At the same time, the powerful generative and potentially multi-modal capabilities of foundation models could offer a far higher “ceiling” [Myers et al. 2000] of what types of interactions are achievable both in terms of their quality and diversity as we will discuss below. However, how successfully we can leverage these capacities will depend on how effectively we can wrangle foundation models into forms that will be more manageable by application developers.
Unfortunately, the same generalizability and high ceiling that give foundation models their edge can also make these models difficult to work with, as they may be even more unpredictable and complex than single-purpose AI models. Indeed, recent work has shown that it can be difficult to make models like GPT-3 consistently perform the intended task [Reynolds and McDonell 2021], while understanding what it is capable of is still an active area of research [Hendrycks et al. 2021a]. In an effort to improve the reliability and trustworthiness of AI-infused applications, we recommend that future work should continue to investigate how to achieve more predictable and robust behaviors from foundation models (e.g., through fine-tuning, or in cases where the main mode of interaction is natural language prompt, through prompt-engineering [Reynolds and McDonell 2021; Liu et al. 2021d], calibrating [Zhao et al. 2021], or pre-formatting a task-specific endpoint.18 Please see §4.8: robustness for more details).
2.5.2 Impact on end-user interaction with AI-infused applications.
Beyond the new ways developers might create AI-infused applications, what changes will foundation models bring to the experience for end-users interacting with these applications? Existing design frameworks for developing user-facing AI applications focus on augmenting (rather than replacing) users’ abilities as described by Douglas Engelbart [Engelbart 1963] — we expect that these frameworks should and will remain relevant for the development of future AI-infused applications. For instance, maintaining users’ agency and reflecting their values will continue to be a central theme for foundation model-powered applications. Additionally, the benefits of allowing AI agents to take initiatives and automate users’ routines versus the benefits of waiting for users’ direct manipulation [Shneiderman and Maes 1997] will need to be carefully weighed [Horvitz 1999]. Moreover, users’ values should be directly gathered and reflected through processes such as participatory [Lee et al. 2019] and value-sensitive design [Smith et al. 2020] that advocate for actively involving all stakeholders during the designing of the AI-infused applications.
These issues may become especially salient with foundation models because the model may behave in ways that surprise and disappoint users and communities. Generative capabilities might expose biases or points of view that are counter to the communities’ goals, or more insidiously,
18https://beta.openai.com/docs/guides/classifications

46

Center for Research on Foundation Models (CRFM)

draw on such associations in their behavior without the community being aware. This will place a large burden on the groups utilizing foundation models to monitor their models’ behavior, and to the extent possible, adapt them to act in appropriate ways.
While the design frameworks for thinking about AI-infused applications to augment users’ abilities should remain the same, the actual forms of interactions that are attainable may dramatically diversify due to foundation models’ powerful generative and multi-modal capacities. Already, early generations of what can be considered foundation model-powered software tools for multimedia creation and editing have started to drive a new frontier that empowers even novice content creators to generate high-quality multimedia from coarse, intuitive specifications (e.g., collaborative authoring for writers [Lee et al. 2022], text-to-image generation for digital artists,19 mastering for musicians,20 and code completion for programmers).21 Improved foundation models might enable even more ambitious tools (e.g., a fan might provide thematic material for a song which will then be generated in the style of their favorite band, or a business owner might provide simple descriptions of their product which will be used to create a full website). Moreover, foundation models will be used to enrich static multimedia (e.g., automatically remastering legacy multimedia content into new formats, or generating unique experiences for each player in new video games) and may even lead to new forms of multi-modal interactions using interfaces that themselves mix different modalities, such as visual and gesture-based interaction.
We are starting to see glimpses of how foundation models might materialize into concrete interactions in applications ranging from AI Dungeon22 to Microsoft PowerApps23 and CoPilot.24 As we start to envision new forms of interactions, it is of increasing importance for us to think critically about the potential implications these interactions will have on individual users and society to maximize their positive impact. For example, how will foundation model-powered applications change the way we communicate with one another? Will a powerful model write emails in our stead and if so, how will this reshape people’s trust, credibility, and identity knowing that the writers may not have written the emails themselves, and how will this alter our writing styles [Hancock et al. 2020]? Who will own the authorship of the model-generated content and how could the shifting responsibilities and ownership of the consent be misused [Weiner 2018] (see §5.5: economics for a more in-depth discussion)? What are the long-term implications that foundation models will have on our work, language and culture [Hancock et al. 2020; Buschek et al. 2021]? Of particular relevance to this last question is the fact that foundation models are trained on observed data and do not necessarily inform us about causality. Hence, how can we ensure that the use of foundation models leads us to a desired future and not a repetition of the past? Though these issues are not necessarily unique to foundation models, they will be amplified and become more prevalent as foundation models accelerate the creation of effective AI-infused applications.
2.5.3 Blurring the line between developers and end-users.
Today, the line that separates the developers of AI models and end-users is rigid — it is rarely the case that an end-user has the data, computing resources, and expertise to be able to develop a new model that suits one’s values and needs well. While a generic model (i.e., one that is not specific to a specific user or community) could be sufficient in some cases, recent years have seen an increasing number of scenarios in which such models fail to serve users. For instance, a text classification
19https://github.com/nerdyrodent/VQGAN-CLIP 20https://www.landr.com/ 21https://copilot.github.com/ 22https://play.aidungeon.io/main/home 23https://powerapps.microsoft.com/en-us/ 24https://copilot.github.com/

On the Opportunities and Risks of Foundation Models

47

model designed to identify problematic comments for one online community might work well for that community but will fail in others whose norms and cultures may differ significantly (e.g., NSFW communities on Reddit might be more tolerant of certain content, while science communities might reject seemingly mundane anecdotes that are not based on scientific research) [Chandrasekharan et al. 2018]. In another example, AI-powered sensors and robotics tools designed for one target population may fail without the ability to quickly adapt in-context for users with different abilities and needs [Karamcheti et al. 2021]. While recent work has presented promising avenues for future research on how end-users may be able to co-create AI models by manually providing models’ parameters or datasets (e.g., WeBuildAI [Lee et al. 2019]), the results are still preliminary and often focus on rudimentary models.
If foundation models can sufficiently lower the difficulty threshold for building AI-infused applications, they could present an important opportunity to more tightly couple users’ needs and values with the models’ behaviors by allowing users to actively partake in the development process of the models. Recent work has shown that GPT-3, for example, can robustly perform classification tasks in a few-shot or even in zero-shot fashion when given an adequate task description in its natural language prompt [Brown et al. 2020]. An online community trying to moderate its own content might be able to leverage such a capability to create bespoke AI classifiers that filter content based on classification task descriptions that the community has agreed on (of course, this power could also be instead misused to silence the voices of certain members within the community — we point to §5.2: misuse for further discussion on this topic). In addition, the powerful in-context learning capabilities that foundation models will exhibit may allow foundation model-powered applications to more effectively optimize their interfaces on a per-user basis. This could open doors to tackling many salient problems in human-computer and robot interaction such as balancing the power of users’ direct manipulation and automation in mixed-autonomy settings.
Of course, there will still be important challenges that we would need to overcome to truly realize this potential for blurring the line between users and developers. These challenges include mitigating existing biases in foundation models, as well as making the models’ behavior more robust and manageable even for non-ML experts (compared to ML experts, it could be even more difficult for non-ML experts to understand the full capacities and mechanisms of foundation models, which can lead to unexpected pitfalls in the development cycle [Yang et al. 2018]). Future work should explore how foundation models could be situated in the context of interactive machine learning and study how we can support even those with limited experience with machine learning to leverage these models in a robust manner. Nonetheless, the ability for end-users to be involved in developing AI-infused applications is an exciting opportunity that could introduce a new paradigm for how we will interact with these applications in the future.

48

Center for Research on Foundation Models (CRFM)

2.6 Philosophy of understanding
Authors: Christopher Potts, Thomas Icard, Eva Portelance, Dallas Card, Kaitlyn Zhou, John Etchemendy
What could a foundation model come to understand about the data it is trained on? An answer to this question would be extremely informative about the overall capacity of foundation models to contribute to intelligent systems. In this section, we focus on the case of natural language, since language use is a hallmark of human intelligence and central to the human experience.
The best foundation models at present can consume and produce language with striking fluency, but they invariably lapse into the sort of incoherence that suggests they are merely “stochastic parrots” [Bender et al. 2021]. Are these lapses evidence of inherent limitations, or might future foundation models truly come to understand the symbols they process?
Our aim in this section is to clarify these questions, and to help structure debates around them. We begin by explaining what we mean by foundation model, paying special attention to how foundation models are trained, since the training regime delimits what information the model gets about the world. We then address why it is important to clarify these questions for the further development of such models. Finally, we seek to clarify what we mean by understanding, addressing both what understanding is (metaphysics) and how we might come to reliably determine whether a model has achieved understanding (epistemology).
Ultimately, we conclude that skepticism about the capacity of future models to understand natural language may be premature. It is by no means obvious that foundation models alone could ever achieve understanding, but neither do we know of definitive reasons to think they could not.

2.6.1 What is a foundation model?
There is not a precise technical definition of foundation model. Rather, this is an informal label for a large family of models, and this family of models is likely to grow and change over time in response to new research. This poses challenges to reasoning about their fundamental properties. However, there is arguably one defining characteristic shared by all foundation models: they are self-supervised. Our focus is on the case where self-supervision is the model’s only formal objective.
In self-supervision, the model’s sole objective is to learn abstract co-occurrence patterns in the sequences of symbols it was trained on. This task enables many of these models to generate plausible strings of symbols as well. For example, many foundation models are structured so that one can prompt them with a sequence like “The sandwich contains peanut” and ask them to generate a continuation – say, “butter and jelly”. Other models are structured so that they are better at filling in gaps; you might prompt a model with “The sandwich contains __ and jelly” and expect it to fill in “peanut butter”. Both capabilities derive from these models’ ability to extract co-occurrence patterns from their training data.
There is no obvious sense in which this kind of self-supervision tells the model anything about what the symbols mean. The only information it is given directly is information about which words tend to co-occur with which other words. On the face of it, knowing that “The sandwich contains peanut” is likely to be continued with “butter and jelly” says nothing about what sandwiches are, what jelly is, how these objects will be combined, etc. This might seem to suggest an inherent limitation on what a foundation model could achieve. However, we need not restrict the model to seeing only textual input. A foundation model might be trained on a wide range of different symbols: not just language but also computer code, database files, images, audio, and sensor readings. As long as it is just learning co-occurrence patterns of the sequences it is exposed to, then it counts as a foundation model by our definition. As part of this learning, the model might come to represent strong associations between a given piece of text and a particular sensor reading, or between a

On the Opportunities and Risks of Foundation Models

49

sequence of pixel values and a database entry. These associations might reflect important aspects of the world we inhabit and the language we use to talk about it.

2.6.2 What is at stake? Before considering analyses of what understanding is, it is worth reflecting on why we might care about the question of whether a foundation model could achieve it. These models are poised to be deployed for numerous purposes with various functionalities. Some of our goals in deployment may only be met to the extent that the model is capable of understanding. Here we list a few such goals:
• Trust: One might argue that we cannot trust a system’s linguistic behavior unless it understands the language it is using. Of course, we currently trust engineered systems to do things (e.g., manufacturing auto parts) without the question of understanding even arising, but language might be special in this regard, since it is uniquely human. In addition, language can be used to deceive and misrepresent, so understanding alone clearly does not imply trust. On the whole, then, understanding might be taken as a necessary condition for trust in the context of language use.
• Interpretability: If genuine natural language understanding in some way involves maintaining and updating an internal model of the world (including, e.g., the speech context), and if we (as engineers) are able to analyze how linguistic input and output interface with this internal model, that could afford substantial gains in interpretability, predictability, and control of these systems.
• Accountability: Not unrelated to the previous points, in the future we may find it desirable to hold artificial agents in some way accountable for the language they produce [The HAI Adaptive Agents Group 2021]. Depending on how we think about concepts like accountability, responsibility, agency, and the like, language understanding may emerge as a prerequisite.
The mere possibility that understanding will play an indispensable role in any of these matters provides strong motivation to develop a framework for theorizing about it.

2.6.3 What is understanding?
Our central question is whether a foundation model could come to understand a natural language. With the above, we can now sharpen it: is self-supervision sufficient for understanding, keeping in mind that there are no constraints on the data used for this supervision? In order to address this question, we first need to define what we mean by understanding.
As a start, we find it helpful to make explicit a distinction that is sometimes conflated in discussions of the topic. The distinction is between the metaphysics and the epistemology of understanding. Metaphysics concerns what it would mean (“in principle”) for an agent to achieve understanding. Epistemology, by contrast, concerns how (“in practice”) we could ever come to know that an agent has achieved the relevant type of understanding. In short, metaphysics is more about our ultimate target, whereas epistemology is more about how (if at all) we could know when we have reached it. Our epistemology thus depends to some extent on our metaphysics.

50

Center for Research on Foundation Models (CRFM)

Metaphysics of understanding. Philosophy of language offers a number of alternatives for what it is to understand natural language.25 Simplifying the landscape for the sake of brevity, the following three broad classes of views all have connections with research lines in AI and NLP:26
• Internalism: Language understanding amounts to retrieval of the right internal representational structures in response to linguistic input. Thus, language understanding is not even a possibility without a rich internal conceptual repertoire of the right kind.
• Referentialism: Roughly, an agent understands language when they are in a position to know what it would take for different sentences in that language to be true (relative to a context). That is, words have referents and (declarative) utterances are truth-evaluable, and understanding involves a capacity to evaluate them relative to presentation of a situation or scenario.
• Pragmatism: Understanding requires nothing in the way of internal representations or computations, and truth and reference are not fundamental. Rather, what matters is that the agent be disposed to use language in the right way. This might include dispositions toward inference or reasoning patterns, appropriate conversational moves, and so on. Crucially, the relevant verbal abilities constitute understanding.27
While this is a simplified picture of the space of possibilities, we already see how they relate in quite different ways to the goals mentioned above. On the pragmatist view, for instance, achieving language understanding does not imply anything about our ability to trust or interpret the system, insofar as it guarantees nothing about the agent’s internal structure or its relation to the (non-linguistic) world. On the internalist view, by contrast, a fairly robust kind of internal/causal interpretability is at least strongly suggested. The question of whether or not a foundation model could understand language in principle takes on a very different character depending on which of these metaphysical characterizations we adopt.
Internalism and referentialism can both be cast as defining a mapping problem: to associate a linguistic sign with a “meaning” or a “semantic value”. For internalism this will be a representation or concept, a program for computing a value, or some other type of internal object. For referentialism, it might be a mapping from a word to an external referent, or a mapping from a situation to a truth value (all relative to a context). Could self-supervision suffice for achieving the desired mapping in a foundation model? Here, the nature of the training examples might be relevant. If the model receives only linguistic inputs, then its capacity to learn this mapping might be fundamentally limited in ways that prevent it from learning to refer in the relevant sense. (Indeed, Merrill et al. [2021] identify some theoretical limits, albeit under very strong assumptions about what it means to learn the meaning of a symbol.) However, if the input symbol streams include diverse digital traces of things in the world – images, audio, sensors, etc. – then the co-occurrence patterns might contain enough information for the model to induce high-fidelity proxies for the required mapping.28 For

25Relatedly, there is a sizable literature in philosophy of science focused on the concept of understanding, mainly as it relates to scientific explanation. See Grimm [2021].
26We are leaving aside other questions that may be relevant to the metaphysics of understanding, such as whether or not consciousness or some form of subjective experience may be necessary. These are pressing philosophical issues, but they are not easily connected to research in AI and NLP.
27For an accessible introduction to internalist as well as referential views, we recommend Elbourne [2011]. This version of pragmatism arguably finds its roots in Wittgenstein [1953], but it is expressed most succinctly by Turing [1950], in which Turing suggests replacing the question of whether a machine can think with questions about a specific behavioral test (which came to be known as the Turing Test).
28To the extent that the mapping embodies causal information, we must also contend with theoretical limitations concerning the possibility of drawing causal inferences from correlational (or even experimental) data (see Spirtes et al. 2001; Bareinboim et al. 2020).

On the Opportunities and Risks of Foundation Models

51

referentialism, there is still a further question of how these proxies relate to the actual world, but the same question arises for human language users as well.
Bender and Koller [2020] give an interesting argument that combines referentialism with pragmatism. They imagine an agent O that intercepts communications between two humans speaking a natural language L. O inhabits a very different world from the humans and so does not have the sort of experiences needed to ground the humans’ utterances in the ways that referentialism demands. Nonetheless, O learns from the patterns in the humans’ utterances, to the point where O can even successfully pretend to be one of the humans. Bender and Koller then seek to motivate the intuition that we can easily imagine situations in which O’s inability to ground L in the humans’ world will reveal itself, and that this will in turn reveal that O does not understand L. The guiding assumption seems to be that the complexity of the world is so great that no amount of textual exchange can fully cover it, and the gaps will eventually reveal themselves. In the terms we have defined, the inability to refer is taken to entail that the agent is not in the right dispositional state for understanding.
Fundamentally, the scenario Bender and Koller describe is one in which some crucial information for understanding is taken to be missing, and a simple behavioral test reveals this. We can agree with this assessment without concluding that foundation models are in general incapable of understanding. This again brings us back to the details of the training data involved. If we modify Bender and Koller’s scenario so that the transmissions include digitally encoded images, audio, and sensor readings from the humans’ world, and O is capable of learning associations between these digital traces and linguistic units, then we might be more optimistic – there might be a practical issue concerning O’s ability to get enough data to generalize, but perhaps not an in principle limitation on what O can achieve.29
We tentatively conclude that there is no easy a priori reason to think that varieties of understanding falling under any of our three positions could not be learned in the relevant way. With this possibility thus still open, we face the difficult epistemological challenge of clarifying how we could hope to evaluate potential success.
Epistemology of understanding. A positive feature of pragmatism is that, by identifying success with the manifestation of concrete behaviors, there is no great conceptual puzzle about how to test for it. We simply have to convince ourselves that our limited observations of the system’s behavior so far indicate a reliable disposition toward the more general class of behaviors that we took as our target. Of course, agreeing on appropriate targets is very difficult. When concrete proposals are made, they are invariably met with objections, often after putative success is demonstrated.
The history of the Turing Test is instructive here: although numerous artificial agents have passed actual Turing Tests, none of them has been widely accepted as intelligent as a result. Similarly, in recent years, a number of benchmark tasks within NLP have been proposed to evaluate specific aspects of understanding (e.g., answering simple questions, performing commonsense reasoning). When systems surpass our estimates of human performance, the community’s response is generally that the test was flawed, not that the target was reached. There may be some suite of behaviors that is our real target, but it is just hard to circumscribe or turn into a practical test.30 Then again, this might reveal that internalism or referentialsm are what we had in mind all along.

29On our reading, Bender and Koller [2020] allow that multimodal data might change the scenario, especially if O is allowed to have cooperative interactions with the humans about shared scenarios and topics.
30Part of the difficulty may also relate to the fact that typical humans make frequent errors in many of these domains, but not necessarily the same types of errors that are made by current systems. Characterizing the target behaviours may thus involve more than just identifying the “correct” behaviour.

52

Center for Research on Foundation Models (CRFM)

If we take internalism or referentialism as the ultimate target – our gold standard for what understanding is – then behavioral tests will always be at best imperfect as a means of assessing whether understanding has been achieved. The imperfections are two-fold. First, behavioral tests will always have gaps that could allow unsophisticated models to slip through. Second, a system might have achieved the mapping that these views require, but we may be unable to show this with behavioral testing. Recent experiences with the model GPT-3 show how challenging this might become: depending on the prompt one uses, one can see surprisingly coherent outputs or utter nonsense, and so prompt engineering requires deep expertise [Rong 2021].
Thus, both internalism and referentialism call for structural evaluation methods that allow us to study their internal representations, probing them for information [Tenney et al. 2019; Manning et al. 2020], studying their internal dynamics [Sundararajan et al. 2017], and perhaps actively manipulating them according to specific experimental protocols supporting causal inference [Vig et al. 2020; Geiger et al. 2020]. There may be fundamental limitations on what we can learn from practical experiments about the inner workings of a complex foundation model, but it is clear that these methods will be useful whenever our target aligns with internalism or referentialism.
2.6.4 Moving the discussion forward.
It seems clear that there are no easy answers to the question of whether foundation models will ever understand language. To even begin to address the question, one must resolve a difficult metaphysical question about which there are a number of substantively distinct views. The metaphysical question then feeds into an epistemological question that poses many practical challenges. Nonetheless, the above discussion does invite one practical conclusion: if foundation models are pursued as a path to language understanding in artificial agents, then multimodal training regimes may well be the most viable strategy, as they would seem the most likely to provide the model with the requisite information. Whether self-supervision then suffices is a completely open question.

On the Opportunities and Risks of Foundation Models

53

3 APPLICATIONS
The capabilities (§2: capabilities) of foundation models indicate that they have the potential to transform various sectors and industries, extending the role AI plays in society (§5: society). Among the myriad applications where foundation models may be applied, we will focus on three disciplines — healthcare (§3.1: healthcare), law (§3.2: law), and education (§3.2: law) — that are all foundational to societal function. Within each, we discuss the opportunities that foundation models pose for this domain alongside challenges (e.g., interpretability; §4.11: interpretability) and concerns (e.g., privacy; §4.7: security).

54

Center for Research on Foundation Models (CRFM)

3.1 Healthcare and biomedicine
Authors: Michihiro Yasunaga, Jing Huang, Camilo Ruiz, Yuhui Zhang, Giray Ogut, Saahil Jain, William Wang, Yusuf Roohani, Hongyu Ren, Antoine Bosselut, Ehsan Adeli, Jure Leskovec, Russ Altman

Fig. 12. Foundation models in healthcare and biomedicine. We visualize an interactive framework where
foundation models enable various tasks across healthcare and biomedicine when trained on multimodal
data generated by various sources in the healthcare ecosystem. The first column lists several sources of data,
including care providers, payers, institutions (universities, non-profits, and governments), pharma, wearables,
and medical publications/forums. The second column shows several data modalities generated by the data
sources. They include images (e.g., chest X-rays), videos (such as ultrasounds), graphs of chemical compounds,
tables for electronic health records (EHRs), text such as clinical notes, time series such as ECGs, and genetic
data. The third column visualizes a foundation model trained on such data and then applied to healthcare
and biomedicine downstream tasks listed in the fourth column. This process can generate new data that will
further improve the foundation model, hence the bidirectional relation between the foundation models and
the tasks.
Healthcare and biomedicine are an enormous application area in society, for instance, with expenditures accounting for 17% of gross domestic product (GDP) in the US [Swensen et al. 2011; van Hartskamp et al. 2019; Keehan et al. 2020]. Both healthcare (which focuses on the delivery of care to patients via diagnosis, treatment, and health administration) and biomedical research (which focuses on the scientific understanding of disease and the discovery of new therapies) demand significant expenses, time, and comprehensive medical knowledge [Yu et al. 2018; Korngiebel and Mooney 2021]. We envision that foundation models can be a central storage of medical knowledge that is trained on diverse sources/modalities of data in medicine [Krumholz et al. 2016; SoltanianZadeh 2019; Suresh et al. 2020] (Figure 12 left), and can be queried/updated interactively by medical professionals (e.g., healthcare providers and biomedical researchers access published findings and upload new publications) [Ionescu et al. 2020] and queried by the public. As foundation models have strong adaptation capabilities (e.g., fine-tuning, prompting [Brown et al. 2020]), they can be efficiently adapted to various individual tasks in healthcare and biomedicine (e.g., question

On the Opportunities and Risks of Foundation Models

55

answering app used by patients [Klasnja and Pratt 2012; Zhu et al. 2019; Daniel et al. 2019; Liu et al. 2020a], clinical trial matching system [Ni et al. 2015; Harrer et al. 2019; Beck et al. 2020] accessed by researchers and patients; Figure 12 right). This way, foundation models can be a central interface that supports various interactions between data, tasks, and people in healthcare and biomedicine, thereby advancing the efficiency and accuracy of healthcare/biomedical applications [Elbattah et al. 2021]. We elaborate these opportunities in §3.1.1: healthcare-tasks and §3.1.2: biomed-tasks.
At the same time, healthcare/biomedical applications pose unique challenges that motivate further research in foundation models, such as integrating multimodal data in healthcare/biomedicine [Miura et al. 2021; Liu et al. 2021a] and observing ethical and legal regulations in medicine (privacy, safety and explainability) [Guan 2019; Xu et al. 2019]. We elaborate these challenges in §3.1.3: healthcare-biomed-challenge.
3.1.1 Opportunities in healthcare.
Foundation models may improve the delivery of care to patients through healthcare providers and hospitals. Currently, healthcare cost increases every year [Keehan et al. 2020], and studies estimate that 30% of healthcare spending may be wasteful due to administrative inefficiency and preventable medical errors [Kocher 2021]. Moreover, as the demand for healthcare increases, the society faces a serious shortage in healthcare providers [Kirch and Petelle 2017]. This inefficiency and shortage in healthcare necessitate developing fast and accurate interfaces for healthcare providers and patients, such as automated aid systems for diagnosis/treatment, summarization of patient records, and answering of patient questions [Davenport and Kalakota 2019; Nie et al. 2018; Wang et al. 2021b]. In particular, in an urgent pandemic crisis such as COVID-19, fast diagnosis/screening (e.g., automatic analysis of chest X-ray images) as well as automated question answering for patients (e.g., symptom checking and care) and the public (e.g., disease prevention) are vital to reduce the spread of diseases and allocate healthcare resources for critical patients, saving more lives [Lalmuanawma et al. 2020]. As foundation models have a strong capability to serve as an integrated knowledge reservoir, they can be queried and adapted to various individual tasks in healthcare. Below are examples of important tasks in healthcare that would benefit from foundation models.
Interface for healthcare providers. Foundation models can improve the efficiency and accuracy of care by providers. Healthcare providers spend unnecessary time editing electronic heath records (EHRs) [Kocher 2021], and preventable medical errors (e.g., hospital readmissions, surgical errors) cause wastes in healthcare [Shrank et al. 2019; Shah et al. 2020]. Foundation models can be adapted as an efficient and accurate interface into EHRs (clinical notes, lab value histories and imaging files) [Li et al. 2020c; Steinberg et al. 2021; Percha 2021], helping healthcare providers create summaries of patient visitation [Krishna et al. 2020], retrieving relevant cases and literature, and suggesting lab tests, diagnosis, treatments and discharges [Zhang et al. 2019b; Rasmy et al. 2021]. Foundation models can also be adapted to help a surgical robot monitor and achieve accurate surgeries [Diana and Marescaux 2015; Agrigoroaie and Tapus 2016; Yu et al. 2019]. See §2.3: robotics for more discussions on foundation models for robotics.
Interface for patients. Foundation models can be adapted to serve as an interface to patients, providing relevant information about clinical appointments [Bates 2019], answering patient questions related to preventive care [Demner-Fushman et al. 2020], along with relevant medical explanatory information (e.g., text and graphics that explain conditions) [Chaix et al. 2019], and helping assistive-care robots for patients [Jeong et al. 2015; Abdi et al. 2018]. See §2.5: interaction for more discussion on foundation models for user interaction. Foundation models can also serve as an interface with the general public to answer questions related to public health and pandemic prevention (such as the COVID-19 case) [Bharti et al. 2020; Herriman et al. 2020]. At the same time,

56

Center for Research on Foundation Models (CRFM)

we note that the interface must guarantee factual accuracy to ensure public trust in medical advice [Kreps and Kriner 2020] (see §3.1.3: healthcare-biomed-challenge).
3.1.2 Opportunities in biomedicine.
Foundation models may facilitate biomedical research such as discovery of drugs and understanding of diseases, which ultimately translates to improved healthcare solutions [Hanney et al. 2015]. Currently, biomedical discovery requires significant human resources, experimental time and financial costs. For instance, drug development involves a complex process, from basic drug research of protein target identification and potent molecule discovery to clinical development (e.g., clinical trials) to the final drug approval, which typically takes over 10 years and costs more than one billion dollars [Wouters et al. 2020]. Facilitating and accelerating biomedical discovery using existing data and published findings is an imperative problem in biomedicine [Yu et al. 2018]. In particular, a novel disease outbreak such as COVID-19 costs millions of lives and trillions of dollars [Lalmuanawma et al. 2020; McKibbin et al. 2020]; if we can speed up drug development for new diseases, that would be very helpful. Foundation models can be particularly helpful for biomedical discovery in two aspects. First, foundation models have a strong generative capability (e.g., coherent text generation in GPT-3), which can help generative tasks in biomedical research such as generating experimental protocols (clinical trials) and designing molecules that work (drug discovery) given existing data [Kadurin et al. 2017; Harrer et al. 2019]. Second, foundation models have a potential to integrate diverse data modalities in medicine, which enables investigating biomedical concepts (e.g., disease) from multiple scales (using molecule-, patient- and population-level data) and multiple knowledge sources (using imaging, textual and chemical descriptions). This facilitates biomedical discoveries that are difficult to obtain if using single-modality data [Lanckriet et al. 2004; Aerts et al. 2006; Kong et al. 2011; Ribeiro et al. 2012; Wang et al. 2014, 2015c; Ruiz et al. 2020; Wu et al. 2021h]. Foundation models also enable transfer knowledge across modalities. Lu et al. [2021a] showed how a transformer model trained on natural language (a data-rich modality) could be adapted for other sequence-based tasks such as protein fold prediction, which is a long-studied predictive task in biomedicine [Jumper et al. 2020]. Below are examples of important tasks in biomedicine that will benefit from foundation models.
Drug discovery. To discover a drug or a therapeutic that treats a disease, researchers must first identify a target (e.g., proteins, genes, RNA causally implicated in the disease) and must then search for molecules (e.g., chemical compounds, antibodies) that bind to the target and treat the disease. Typically, identifying the appropriate target and generating a corresponding molecule requires years of expensive wet lab experiments [Hughes et al. 2011; Schenone et al. 2013; Schneider 2018]. Foundation models’ generativity can improve the search space and efficiency (see §2.4: reasoning), which not only reduces the amount of experiments but also helps to discover new and better drugs [Jin et al. 2018; You et al. 2018; Walters and Barzilay 2020; Stokes et al. 2020]. Moreover, the simultaneous solution of related drug discovery problems (i.e., target identification, efficacy prediction, side effect prediction, and others) by a single foundation model may improve the solutions to each of them [Ramsundar et al. 2015; Camacho et al. 2018; Duran-Frigola et al. 2020; Huang et al. 2021a]. As an example, one area where foundation models have shown significant potential for impacting therapeutic design is the modeling of proteins using language models. Successful applications range from predicting viral mutations that can escape a vaccine-induced immune response to predicting protein docking potential for better design of therapeutic antibodies [Bepler and Berger 2021; Hie et al. 2021; Tsaban et al. 2021; Wu et al. 2021b; Rives et al. 2021].
Personalized medicine. Personalized medicine aims to select the optimal treatment for individual patients based on their health history, genetics, imaging, and other personal measurements [Collins

On the Opportunities and Risks of Foundation Models

57

and Varmus 2015; Ashley 2016]. For instance, given a set of drugs and a patient genome, foundation models may help predict which drug is likeliest to treat the patient with minimal side effects [Whirl-Carrillo et al. 2012; Tatonetti et al. 2012; Gerstung et al. 2017; Grinfeld et al. 2018; Adam et al. 2020]. Foundation models are uniquely powerful in their ability to integrate multimodal patient data ranging from the EHR [Rajkomar et al. 2018] to medical imaging [Bera et al. 2019; Ouyang et al. 2020] to drug and molecular measurements [Gottlieb et al. 2011; Ruiz et al. 2020] to make an optimal prediction.
Clinical trials. Clinical trials study efficacy and safety of treatment or drug candidates. Conventional clinical trials are inefficient and costly: 80% of trials fail due to inability to show efficacy/safety or problems with patient matching [Ali et al. 2020; Liu et al. 2021c]. Foundation models can help in the following: predicting potential failures and design promising clinical trial protocols (e.g., patient eligibility criteria) based on existing studies; and automating matching of eligible patients based on patient individual profiles, which are multimodal data including EHRs, gene sequence, etc. [Harrer et al. 2019].
3.1.3 Challenges and future research in foundation models.
While there are potential opportunities for foundation models to help, healthcare/biomedical applications also pose unique challenges that motivate further research in foundation models.
Multimodality. Medical data are highly multimodal, with various data types (text, image, video, database, molecule), scales (molecule, gene, cell, tissue, patient, population) [Kong et al. 2011; Ruiz et al. 2020], and styles (professional and lay language) [Lavertu and Altman 2019; Li et al. 2019]. Current self-supervised models are developed for each modality (e.g., text [Lee et al. 2020b], image [Chaitanya et al. 2020], gene [Ji et al. 2021], protein [Jumper et al. 2020]), and do not jointly learn from diverse modalities. To learn the inter-modality and cross-modality information from these diverse multimodal medical data, we need to investigate both feature-level and semantic-level fusion strategies in the training of foundation models. If done effectively, this has a potential to unify biomedical knowledge and facilitate discoveries as discussed in §3.1.2: biomed-tasks.
Explainability. Explainability — providing evidence and logical steps for decision making — is crucial in healthcare and biomedicine [Holzinger et al. 2019], and is made obligatory under the General Data Protection Regulation (GDPR). For instance, in diagnosis and clinical trials, patient symptoms and temporal relevance must be explained as evidence. This helps the resolution of potential disagreement between the system and human experts. Explainability is also needed for informed consent in healthcare [Amann et al. 2020]. However, current foundation models’ training objectives do not include explainability, requiring future research in this direction [Linardatos et al. 2021]. Incorporation of knowledge graphs may be a step to further improve model explainability [Roberts et al. 2020; Xu et al. 2020; Jin et al. 2021]. Readers are refered to §4.11: interpretability for more discussion on explainability.
Legal and ethical regulations. Healthcare applications must observe legal and ethical regulations with guarantees, such as patient safety, privacy and fairness. For instance, regarding safety, predictions made by foundation models must be factually accurate with established medical knowledge, and must quantify uncertainty or choose to defer to an expert when uncertain [Challen et al. 2019; Mozannar and Sontag 2020]. For privacy, the use of patient health records must observe the privacy laws, such as HIPAA [Act 1996] in the case of the US. Federated learning is one potential solution to keeping the raw, sensitive data private in the training of foundation models [Chamikara et al. 2021]. For fairness, researchers will need to be mindful of common pitfalls or otherwise risk exacerbating existing social inequalities [Chen et al. 2019; Wiens et al. 2019; Chen et al. 2020b]. They must

58

Center for Research on Foundation Models (CRFM)

ensure that the training and evaluation data for foundation models is sufficiently representative of different sexes, races, ethnicities and socioeconomic backgrounds; an area where medical datasets and clinical trials have had a long history of bias [Martinez-Martin et al. 2020; Kaushal et al. 2020]. Research is also needed to debias and regularize models to ensure fairness when representative data is scarce [Zhao et al. 2020a]. Foundation model developers also need to consult with ethics and law researchers, and observe regulations in the specific circumstances (e.g., country, region) where they are deployed. We also refer readers to §4.7: security, §4.8: robustness, §5.1: fairness, §5.4: legality for details on privacy, robustness, fairness and legality.
Extrapolation. The process of biomedical discovery involves extrapolation. For instance, foundation models must be able to quickly adapt to new experimental technologies (e.g., new assays, new imaging techniques such as high resolution microscopy) or new settings (e.g., new target diseases such as COVID-19) [Jaroch et al. 2018; Benam et al. 2019]. The ability to leverage existing datasets and extrapolate to new settings is a key machine learning challenge in biomedicine [Snell et al. 2017; Ma et al. 2021b]. While GPT-3 exhibits some extrapolation behaviors (e.g., generating new text not seen before), its mechanism is unclear and still in its infancy. Further research is needed for improving the extrapolation capability of foundation models, especially when considering the diverse range of data modalities and tasks that is inherent to healthcare and biomedicine but is not commonly studied in current GPT-3 and related models. Also see §4.8: robustness.

On the Opportunities and Risks of Foundation Models

59

3.2 Law
Authors: Peter Henderson, Lucia Zheng, Jenny Hong, Neel Guha, Mark Krass, Julian Nyarko, Daniel E. Ho

Fig. 13. An example of various steps of a civil case in the United States and where foundation models might
help. At each stage different modalities might need to be processed and adaptation is needed to a new court
or legal perspective.
From family court to criminal justice and from environmental policy to corporate transactions, the reach of the law is vast. In the United States,31 there are over 1.3M lawyers [American Bar Association 2021] and annual revenues for legal services exceed $300B [MarketLine 2021]. Yet “access to justice” remains far out of reach for most. Legal services can be prohibitively expensive. Roughly 86% of low-income individuals with civil legal problems in the United States, for instance, report receiving inadequate or no legal help [Legal Services Corporation 2017]. Even when counsel is appointed, lawyers might be strained by increasingly large caseloads. Studies have shown that public defenders, for example, are often overworked and underfunded [Lefstein and Spagenberg 2009; Schumm 2012; American Bar Association 2004]. The U.S. Department of Justice reported that in 2007, 73% of county-based public defender offices exceeded the maximum recommended limit of cases received per attorney and 15 of 19 reporting state public defender programs exceeded the maximum recommended limit of felony or misdemeanor cases per attorney [Farole and Langston 2010; Langston and Farole 2010]. Even in a country with one of the highest per capita rates of attorneys, justice can appear out of reach. U.S. President Jimmy Carter once opined, “Ninety percent of our lawyers serve ten percent of our people. We are overlawyered and underrepresented” [Carter 1978]. According to a leading voice in access to justice, technology may provide a path forward [Rhode 2014], a view echoed by many others [Cabral et al. 2012].
What role might foundation models play in the law?32 A major promise is that foundation models can improve access to justice and government services by leveling procedural and financial barriers to legal services. The challenges posed by legal applications can, in turn, motivate basic research questions for foundation models. Many legal applications pose unique challenges to computational solutions. Legal language is specialized and legal outcomes often rely on the application of ambiguous and unclear standards to varied and previously unseen fact patterns. At the same time, due to its high costs, labeled training data is scarce. Depending on the specific task, these idiosyncrasies
31We restrict our discussion to legal applications in the United States because of the expertise of the authors. Some discussion here may apply to legal venues globally, however.
32We note that for the purposes of this section we consider foundation models to be any self-supervised pretrained model that is used to quickly adapt to new contexts with little supervised learning. See also the discussion in §1: introduction and §2.6: philosophy for an expanded definition.

60

Center for Research on Foundation Models (CRFM)

can pose insurmountable obstacles to the successful deployment of traditional models. In contrast, their flexibility and capability to learn from few examples suggest that foundation models could be uniquely positioned to address the aforementioned challenges.
Throughout this section, foundation models may take as context many modalities as evidence: audio during trial proceedings, video and images during discovery, and text in conducting legal research. Yet, the majority of legal tasks in which reliance on foundation models will be beneficial involve text-based inputs and outputs. As such, we mainly focus on text-based domains while only briefly discussing others. To ground the discussion, Figure 13 describes the stages of a civil lawsuit in the United States and where foundation models might come into play in this process. Figure 14 shows the logic flow required to generate just part of one paragraph of a legal brief, which might serve as a concrete example of a task that foundation models might one day be used for.
An Important Consideration. Before proceeding, we note that the ethical, legal, and fairness considerations expanded on in §5.6: ethics, §5.4: legality, and §5.1: fairness are particularly important to examine before using foundation models in an applied legal or government context, as these applications often have important, real-world consequences to those affected [Surden 2020]. Foundation models must also be thoroughly scrutinized before deployment, as discussed in §4.4: evaluation. For example, the legal system places particular emphasis on — and may even mandate — transparency, accountability, and explainability. Consequently, it is questionable whether current models are positioned to solve many of the most pressing, legal problems. Nonetheless, the need to expand and improve access to legal and government services provides a worthy goal for foundation models.
3.2.1 Opportunities in law.
Legal applications can range from the use of machine learning in government contexts [Engstrom et al. 2020; Coglianese and Ben Dor 2020; Re and Solow-Niederman 2019] to aiding lawyers in their provision of legal services [Zheng et al. 2021; Huang et al. 2021b; Ostendorff et al. 2021; Vold and Conrad 2021]. We note that prior work has also surveyed machine learning-assisted legal tasks in text-based domains [Zhong et al. 2020; Chalkidis et al. 2020], although it has been noted that recent legal AI research has focused on geographic regions outside of the U.S. [Zheng et al. 2021]. While many of the topics we discuss here may be applicable to different legal systems, due to the expertise of our team we focus primarily on the U.S. In particular, we concentrate on three broad categories of legal applications that may benefit from foundation models in the U.S. legal system: private law or civil justice (claims between private individuals, arising out of, for instance, contracts, property or torts), criminal law (i.e., the prosecution of individuals for criminal behavior), and (non-criminal) public law (e.g., the regulation of private behavior by government agencies).
Civil law. In U.S. civil proceedings, parties must typically find and pay attorneys to be represented. As a result, many individuals, especially those with low income, struggle to secure adequate legal representation [Rhode 2004]. Foundation models have the potential to improve access to justice by reducing the cost, improving the quality, and extending the reach of legal services. In Figure 13, we describe the process by which a civil lawsuit is filed in a U.S. court and where foundation models may play a role in aiding both attorneys and judges.
Even before an attorney is involved in the legal process, clients may benefit from the deployment of foundation models. Recent work has used machine learning models to identify the relevant legal issues contained in a plain-language description of facts presented by a client.33 Tools like these can help provide a recommendation for the type of legal action needed to address the issue at hand or to recommend a specialized attorney. A number of other similar efforts have sought to increase
33https://spot.suffolklitlab.org/

On the Opportunities and Risks of Foundation Models

61

access to justice by providing information tailored to a client’s particular needs [Cabral et al. 2012; Brescia et al. 2014; Queudot et al. 2020; Westermann et al. 2019].
Once a client speaks with an attorney, prior to civil litigation, the attorney may seek to avoid a costly trial. At this stage, they can rely on foundation models to evaluate contracts, review terms of service, find relevant patents, and conduct other pre-litigation processes in order to ensure that their clients are at an advantage [Betts and Jaep 2017; Elwany et al. 2019; Lippi et al. 2019; Lee and Hsiang 2019; Hendrycks et al. 2021c; Hegel et al. 2021]. Notably, recent work has both described the challenges and benefits of using foundation models for contract review [Leivaditi et al. 2020; Hegel et al. 2021; Hendrycks et al. 2021c]. In addition to reviewing and drafting legal documents, client interactions and documents can be translated to reduce costs and barriers to the provision of legal services [Cuéllar 2019]. But translation of legal documents requires precision and an understanding of highly technical language, which makes collecting training data costly. Additionally, translating client statements or trial proceedings often requires an understanding of local dialects and language. This, too, makes it difficult to collect enough ground truth translation data to train on. As a result, traditional supervised methods rarely achieve the level of accuracy required in the legal domain [Vieira et al. 2020]. Foundation models may improve performance in this area over fully supervised mechanisms by adapting quickly in these low-resource contexts.
During litigation, foundation models can help lawyers to conduct legal research, draft legal language, or assess how judges evaluate their claims [Zheng et al. 2021; Huang et al. 2021b; Ostendorff et al. 2021; Vold and Conrad 2021; Chalkidis et al. 2020, 2019]. This could potentially reduce the costs of and improve legal services. For example, recent work has utilized pretrained models for the recommendation of relevant citations and holding statements when writing legal texts [Zheng et al. 2021; Huang et al. 2021b; Ostendorff et al. 2021]. Other work uses pretrained models for improved legal question answering to power commonly used legal search engines and help lawyers conduct legal research [Vold and Conrad 2021]. A wide variety of work has also examined automated contract drafting and review, a task that could similarly benefit from foundation models [Hendrycks et al. 2021c; Betts and Jaep 2017]. Perhaps most compelling, foundation models may help assist lawyers generate legal briefs (written arguments). The models might find novel arguments or identify problems in attorney-written portions of the brief. For example, Tippett et al. [2021] predict the outcome of a legal proceeding based on features extracted from the filed briefs. Foundation models can be leveraged to use raw language as inputs rather than extracted features. This might provide attorneys with more informative recommendations as to how their brief could be improved to ensure a favorable outcome.
After opening and reply briefs are filed, parties then begin the discovery process, which has already used simple machine learning models for the better part of a decade [Grossman and Cormack 2010]. Attorneys use these systems to label whether a document should be produced to the opposing party. The documents are multi-modal in nature, often containing video, images, audio, and text. Current systems are costly because they used supervised learning and active learning to label the documents as responsive [Grossman and Cormack 2010; Oard et al. 2018; Yang et al. 2021]. Instead, few-shot or zero-shot document retrieval capabilities that might be possible with foundation models would help ease concerns about the large costs of the current process.34 To avoid the possibilities of gamesmanship in the discovery process, Cui [2018] has proposed a zero-shot (or few-shot) adaptation process that can only be operationalized through the use of foundation models.
After discovery, once the trial begins, foundation models could help parties prepare for trial by predicting what the judge might focus on during questioning [Dickinson 2018], adapting to
34https://www.kirkland.com/publications/article/2020/04/technology-assisted-review-framework

62

Center for Research on Foundation Models (CRFM)

the current context from judges’ prior published opinions. In the courtroom, foundation models might be used to examine audio and video of courtroom proceedings to determine if outcomes were biased against the defendant because of their race or dialect.35
Once the trial concludes, foundation models could help judges and law clerks to properly evaluate legal claims from both parties using similar technologies, or the use of contextual embeddings from foundation models might assist in statutory interpretation [Nyarko and Sanga 2020; Choi 2020]. Recent work (without reliance on foundation models or NLP) has examined whether an appeals decision can be predicted from a set of extracted features, like citation counts and the appearance of key words [Katz et al. 2017; Boniol et al. 2020]. It is possible that such models could be improved using foundation models and applied to help judges draft decisions by flagging obvious mistakes in their opinion, as has been discussed in the context of adjudicative agencies [Engstrom et al. 2020; Ray and Lubbers 2014]. They can also be used to identify racial biases in legal opinions and help judges revise their opinions accordingly [Rice et al. 2019].
Criminal law. One particularly contentious area has been the use of risk scores in government settings, particularly in criminal law. Some may want to use language-based foundation models to aid in making charging decisions or parole decisions based on a given text-based narrative of the events. Careful consideration must be taken before using foundation models for risk scoring due to the potential for biases, especially when language data is included [Bender et al. 2021; Berk et al. 2021; Laufer 2020]. But foundation models may play a role in many other dimensions of criminal justice. The same tools as in civil litigation, above, can also be used by prosecutors and defense attorneys. This can help appointed attorneys perform their job more efficiently and reduce unnecessary overhead. As a result, they may be able to balance already heavy caseloads more effectively. For example, public defenders are often viewed as being overworked and underfunded, which would lead to avoidable procedural errors.36 Foundation models can help reduce some of these resource constraints by identifying errors and automating simple tasks. However, they are not a solution on their own.
In other areas, foundation models can act as an oversight mechanism to reduce structural inequities. Pretrained models have been used for processing parole hearing transcripts to find instances of anomalous outcomes [Bell et al. 2021]. Recent work has also removed linguistic cues for a suspect’s race in police reports to promote race-blind charging decisions and avoid racially biased prosecutions [Chohlas-Wood et al. 2020]. Other work has helped identify disrespectful police communications [Voigt et al. 2017]. In these contexts, it is very costly to label data since annotators must be given access to sensitive data and appropriate background checks are often required. To reduce these costs, foundation models can be used to pretrain and adapt quickly to downstream tasks where labels are scarce.
Public law. Government agencies regulate vast parts of society, and foundation models have wide potential applicability across public law. This includes: analyzing public comments in the noticeand-comment process, assisting patent examination, retrieving relevant documents in response to Freedom of Information Act requests, aiding in mass adjudication, among many others. Recent work has surveyed these government applications in a variety of contexts and we refer the reader to the relevant sources for in-depth discussion [Engstrom et al. 2020; Coglianese and Ben Dor 2020]. In many of these applications, foundation models can improve the quality, efficiency, utility, and
35For example, speaking African-American Vernacular English dialects in the courtroom has been shown as a potential source of bias during trial. https://www.nytimes.com/2019/01/25/us/black-dialect-courtrooms.html
36See, for example, in People v. Superior Court (Vasquez), 27 Cal.App.5th 36 (2018) a defendant did not receive a trial for 17 years because the public defender’s office had severe budget cuts and understaffing. The court ruled that the systemic breakdown in the public defender’s office constituted a due process violation and the defendant’s case was dismissed.

On the Opportunities and Risks of Foundation Models

63

accessibility of government services: labels are scarce, resources are constrained, and contexts are constantly shifting. As such, the adaptability and flexibility of foundation models are often required to improve efficiency and performance. To give an illustrative example of just one such application, existing work has leveraged NLP for facilitative moderation in public comment forums. In this use case, predictive models help lay-users improve arguments and identify misstatements in their comments. Such a system has already been deployed in the U.S. Department of Transportation rulemaking process [Park et al. 2012], although it can likely be improved through the linguistic reasoning capabilities of foundation models. But government agencies must comply with constitutional, statutory, and administrative obligations (see §5.4: legality), so additional care is needed in these settings.
3.2.2 How can foundation models uniquely help?
The above examples of legal applications are unique in several ways. First, the cost of annotating data is very high. Often, the expertise to create high-quality labels can only be found in attorneys, who may charge hundreds of dollars per hour. Even after labels are obtained, certain data may be sensitive and cannot be pooled together to training a large language model. Given recent progress in few-shot learning [Brown et al. 2020], foundation models are among the most promising paths for learning models with limited annotations.
Second, legal decision-making requires context at various scales: knowledge of all historical decisions and standards, knowledge of the case law that remains relevant in the present, and knowledge of the nuances of the individual case at hand. Foundation models are uniquely poised to have the potential to learn shared representations of historical and legal contexts, as well as have the linguistic power and precision for modeling an individual case.
3.2.3 What are foundation models lacking that requires more research?
To illustrate the deficiencies current foundation models need to overcome in order to be realistically deployed, we consider as an example the automatic creation of a legal brief to submit to a court.
A brief lays out the arguments to a judge before a hearing. Once a party has filed an opening brief, the opposing party files a response. The judge then evaluates the briefs and asks questions of both parties at a hearing before making a decision. Figure 14 visualizes the structure of such a legal brief with some of its characteristic features.
An automated brief generation mechanism might take as context relevant documents and facts of a case (as specified by an attorney) as well as a rough sketch of the desired outcome. It would then generate a legal brief with complex legal arguments to submit to the court.
Long Documents and Narratives. To achieve this goal, the model must be able to read long contexts and produce long narratives. Legal documents tend to be far longer than documents in any other context. The average U.S. Supreme Court opinion contains around 4,700 words,37 a brief on the merits to the Supreme Court can have as many as 15,000 words,38 a law review article often contains 20,000 to 30,000 words,39 parole transcripts can be hundreds of pages long [Bell et al. 2021], and trial records can be even longer. Current foundation models have struggled with such long contexts and outputs (see §4.1: modeling for more discussion).
Retrieval, Concept Drift, Argument Formation, and Logical Reasoning. In addition to reading casespecific documents, the foundation model must retrieve the relevant case law and understand which
37https://www.americanbar.org/groups/public_education/publications/teaching-legal-docs/how-to-read-a-u-s– supreme-court-opinion/
38https://www.supremecourt.gov/casehand/courtspecchart02162010.aspx 39https://www.stanfordlawreview.org/submissions/article-submissions/

64

Center for Research on Foundation Models (CRFM)

Fig. 14. An extract from a fictional brief written by one of the authors of this work. The prototypical form that law students are instructed to write a brief involves: (1) introducing the argument; (2) stating the legal rule in a persuasive manner; (3) applying the legal rule to the facts of the case; (4) persuasively concluding the argument. This often involves information retrieval and paraphrasing from both prior cases and the facts of the current case.
case law is still valid and which has been overruled, taking into account potential concept drift since it was trained. More work in editing grounded information in foundation models will be required as case law evolves [De Cao et al. 2021]. Using retrieved legal standards, the foundation model must then understand how to weave them into a persuasive argument. Emerging research has studied ways of using foundation models to measure, detect, and generate persuasive texts [Duerr and Gloor 2021; Li et al. 2020a; Longpre et al. 2019], which may provide stepping stones towards this goal.
But legal briefing also requires the ability to identify a relevant legal rule and determine how it applies to a new situation. For example, Holzenberger et al. [2020] provide a dataset and methodology for evaluating this ability in current models based on the tax code. Given a set of tax code rules and a description of someone’s situation, the model must determine how the rules apply to the individual in order to determine what their tax liability is.40 Holzenberger et al. [2020] found that domain experts can express the relevant tax law as a prolog solver to determine an individual’s tax liability with perfect accuracy. However, they found that simply using a BERT model, even when pretrained on in-domain legal text, was not sufficient to allow the model to deduce and apply the relevant rules, a process also referred to as “legal reasoning.”
40For example, a rule might look like, “In the case of an individual who is not married and is not a surviving spouse, paragraphs (1) and (2) shall be applied by substituting ‘$750’ for ‘$600’.” US Internal Revenue Code (IRC) §63(f )(3).

On the Opportunities and Risks of Foundation Models

65

These challenges can further be illustrated through a real exchange with GPT-3, demonstrating that current models are unable to perform even comparatively simple tasks involving legal reasoning.
Legal Reasoning with GPT-3.
“Liquidated damages” are a form of monetary compensation for a breach of contract that the parties specify in their agreement. In general, courts in the U.S. will not enforce liquidated damages provisions if the amount stipulated in the contract is so high that it acts as a form of punishment, rather than to be merely compensatory. First, we see if the model encoded the correct rule about the enforcement of liquidated damages provisions.
q: Are liquidated damages clauses enforceable? a: Liquidated damages clauses are generally enforceable unless the sum stipulated
is exorbitant or unconscionable.
The rule appears correct. But then we apply the rule to a set of facts. Here, the model needs to infer that one million in damages is exorbitant for a Toyota Corolla and thus, the liquidated damages provision should not be enforced.
q: X and Y contract over the sale of a Toyota Corolla. Their contract specifies that X has to pay Y one million dollars in damages if X does not deliver the car on time. X does not deliver the car on time. Is Y entitled to damages? If so, in what amount?
a: Yes, Y is entitled to $1,000,000 in damages. Damages are meant to compensate the injured party for the loss suffered due to the breach. Y has suffered a loss of $1,000,000 because X has not delivered the car on time.
The model is unable to infer that $1M for a Toyota Corolla is exorbitant. Even if the model is provided with both the abstract rule and its precise translation to the context in the prompt — and the application should thus be trivial — GPT-3 is still unable to conduct the logical reasoning required.
q: X and Y contract over the sale of a Toyota Corolla. Their contract specifies that X has to pay Y the exorbitant amount of one million dollars in damages if X does not deliver the car on time. X does not deliver the car on time. The law requires that liquidated damages clauses are enforced unless the sum stipulated is exorbitant or unconscionable. Is Y entitled to damages? If so, in what amount?
a: Yes, Y is entitled to the liquidated damages. The amount in this case is not exorbitant since the price was set by the parties themselves.

Precision. The foundation model also cannot create fake facts in the process, a problem that has come up in current models [Gretz et al. 2020; Zellers et al. 2019b]. Specificity and truthfulness are of heightened important in legal contexts, where imprecise statements can have drastic, unanticipated consequences, and false statements can lead to sanctions against attorneys.
Few-shot Learning. Some challenges facing foundation models beyond those described in the above example include few-shot learning, which is still in its infancy [Perez et al. 2021]. Research on few-shot learning techniques and increasing access to legal corpora can work in tandem. Because foundation models need to be precise, not only in factual truth, as discussed above, but also in technical legal language, it remains unclear to what extent information obtained from one corpus can be utilized in another corpus of a different domain. Few-shot learning thus remains important.

66

Center for Research on Foundation Models (CRFM)

Adaptation. Some gains have been observed from domain-adaptive pretraining on unlabeled legal corpora. These gains appear to be most pronounced when the pretraining corpus is highly relevant to the downstream task and labeled training data is limited (a setting which is common in the law) [Zheng et al. 2021]. It has not yet been comprehensively studied whether this extends to a diverse set of legal tasks, but leveraging unlabeled domain-specific corpora for self-supervised training of foundation models may provide complementary improvements to few-shot methods.
Access to Clean In-Domain Data. Some recent efforts have sought to create large labeled datasets for more challenging legal benchmark tasks through automation [Zheng et al. 2021] or manual annotation by volunteer legal experts [Hendrycks et al. 2021c]. These efforts have demonstrated that larger language models that are pretrained on more data achieve performance gains on certain challenging tasks, compared to more limited gains observed in other settings [Chalkidis et al. 2020; Elwany et al. 2019; Zhong et al. 2020]. This work suggests that larger legal benchmark datasets may be necessary to observe further gains from applying transfer learning techniques to foundation models. However, creating benchmark datasets for tasks that are legally meaningful and difficult from an NLP perspective can itself be challenging, as human expert annotation can be costly and automated methods that utilize conventional tokenization and sentence segmentation techniques can fail to account for unique aspects of legal text, such as the structure of legal citations [Bommarito et al. 2018; Savelka et al. 2017]. As a consequence of these challenges, many existing legal domainspecific labeled datasets are small, not publicly available, or reflect simpler tasks that have been solved by methods often pre-dating the development of foundation models.41
Much available legal data may also be unrepresentative. Since only a fraction of cases end up in legal opinions, it is unclear whether the disputes in publicly available data are representative of the typical disputes presented to a model in practice [Priest and Klein 1984]. Costly training data for more representative scenarios may be concentrated in the biggest law firms. These law firms may have the ability to retain and accumulate data across many cases and clients. One concern then is that foundation models could concentrate power even more among the few actors that have the resources to train models on in-domain data — unless the models can generalize sufficiently well.
Reliability. Finally, we again note that even if foundation models could successfully perform all tasks in the legal domain, deployment remains a major challenge: a failure of a foundation model in the law will have real, damaging consequences to both clients and attorneys (see also discussion on fairness, legality, and ethics in §5.1: fairness, §5.4: legality, and §5.6: ethics). For this reason machine translation software has already been deemed unreliable for use as evidence in some courts,42 although it continues to be relied upon in others.43
Given all of these complexities, legal briefing and reasoning is likely beyond the capabilities of current models, but appears to be within the future realm of possibilities. As such, these serve as a potential lode star for the ongoing development of foundation models.

41For law firms and legal technology companies, tasks for which high performance can already be achieved, and can
therefore be more immediately productized, may be considered more worthwhile to invest costly manual labeling efforts
towards. 42See discussion by Vieira et al. [2020]. 43For example, in Vasquez v. United States, No. 3: 16-cv-2623-D-BN (Dist. Court, ND Texas 2019), counsel relied on Google
Translate to prove that the previous (native speaker) attorney has mistranslated a plea deal.

On the Opportunities and Risks of Foundation Models

67

3.3 Education
Authors: Ali Malik, Dorottya Demszky, Pang Wei Koh, Moussa Doumbouya, Drew A. Hudson, Allen Nie, Hamed Nilforoshan, Alex Tamkin, Emma Brunskill, Noah Goodman, Chris Piech

Fig. 15. Foundation models in education could be trained on multiple data sources to learn the capabilities
necessary for education: an understanding of various subject matter and different pedagogical techniques.
These foundation models can be applied in a general-purpose way across a range of tasks and goals such as
understanding students, assisting teachers, and generating educational content.
In the year 2000, the largest gathering of world leaders convened at the United Nations Millennial Summit to reflect on an ideal vision for the future. Delegates concluded that a primary focus should be education, declaring it “a foundation for human fulfillment, peace, sustainable development, economic growth, decent work, gender equality and responsible global citizenship." This discussion was ultimately recodified into the United Nations Sustainable Development goal to “ensure inclusive and quality education for all and promote lifelong learning" [United Nations General Assembly 2015]. However, providing high quality, inclusive education at a large scale poses difficult societal and economic challenges. The price of education per student is growing faster than economy-wide costs [Bowen 2012], limiting the resources available to support student learning. In the United States, one symptom is that private education debt held by students has reached $1.6 trillion, surpassing total credit card debt [Friedman 2020]. Considering the rising need to provide adult retraining, the gap between the demand for education and our ability to provide it is alarmingly large and has concerning achievement disparities across protected demographics.
With the advent of the digital age and the rapid growth in digital learning, computational approaches to education have shown promise in increasing the effectiveness of learners and teachers. Several core directions have emerged as potentially impactful applications of AI for education [Woolf et al. 2013], such as systems that can provide meaningful feedback to students [Malik et al. 2021], help teachers improve [Jensen et al. 2020; Demszky et al. 2021; Suresh et al.

68

Center for Research on Foundation Models (CRFM)

2021], or even create personalised and adaptive learning experiences that tailor the learning process to individual students’ needs and dispositions [Connor 2019].
Despite this potential, building technical solutions to effectively scale inclusively and quality of education has proven to be exceptionally difficult. One particular challenge is that existing work has focused on custom solutions to highly specific tasks for which large amounts of training data has to be collected from scratch. Due to the difficulty and cost of creating large datasets, using this approach to solve every educational task independently is fundamentally limited. Instead, is it possible to create general-purpose approaches that are reusable across various tasks and subjects?
Foundation models have already started to boost the performance of some specific flagship tasks in education. Recent examples include using MathBERT [Shen et al. 2021b] to power “knowledge tracing" — the challenge of tracking a student’s understanding over time given their past responses — and the “feedback challenge", where an algorithm has to interpret a student’s answer to a structured open-ended task, such as a coding question [Wu et al. 2021e]. Can foundation models lead to even more transformative changes in this domain? And what are the known and imagined risks of foundation models applied to education? In this section, we first frame the conversation around the ethical considerations. We then ground our discussion in two concrete tasks: (1) understanding student misconceptions, and (2) improving student understanding through instruction.
3.3.1 Important concerns for centering foundation models in education research.
The future of AI for education is exciting, especially in the context of foundation models. However, we caution the reader to be especially thoughtful about the impact of any AI research applied to education.44 The goal of education are deeply interwoven with complex, long term social impact. While we actively work to improve digital education, it is imperative that we put in substantial thought to try and imagine the complexities of any disruption in this space [Piech and Einstein 2020]. Ethical challenges range from issues such as data bias, legal constraints, and the impact of digital socialization. These issues are not unique to foundation models, but they are worth reflecting on regularly as research makes substantial progress in AI for education. Reflection on impact is especially important when research starts by asking “what can new AI technology afford?"
Many of the issues in §5.6: ethics apply to education. For example, as in many other domains, small biases in foundation model training data could be hard to track down [Dixon et al. 2018; Bolukbasi et al. 2016], but have important implications for equity of educational access. Moreover, these systems may experience a high degree of “feedback", where the collected data continually reinforces the model’s decisions. This issue of bias goes beyond what data is collected and includes concerns over the applications that researchers choose to work on. Below, we discuss other education-specific issues. Many of the issues revolve around the question: “who benefits?" and for whom is new technology created?
Removing teachers from the loop One of the goals of digital education, especially based on AI, is to increase the productivity of the learning experience so that more learning happens per unit time or unit cost. One can imagine that decision makers could use this increased productivity to remove human teachers from the loop. The long term implications of such decisions are hard to know a priori. Could interacting with an education system optimized to maximize “learning” have adverse effects on socioemotional skill development? Could it create fewer opportunities for
44In 2013, Facebook initiated Free Basics, a project to provide free internet to the world and thus spread opportunity and interconnection. Now, the United Nations Human Rights Council reports that, in Myanmar, Facebook’s efforts to follow through on such aspirations without proper human moderation accelerated hate speech, instigated division, and incited offline violence in the Rohingya genocide. Free Basics now serves as a warning of the complexities of technological impact on society.

On the Opportunities and Risks of Foundation Models

69

Fig. 16. The figure illustrates a system that embeds signals from various modalities (image, speech, sign,
text) and languages into a universal feature space. Such a feature space allows ideas to be linked across
modalities and languages. Pedagogically relevant link types include analogies (similarities across languages)
and contrasts (distinct concepts across languages), both of which can occur in the same modality or across
different modalities.
interacting with others? Loneliness is on the rise in younger generations [Cigna 2018], and teachers are a modulating force for pressures that AI researchers might not envision.
Was this work done by the learner or a foundation model? Another challenge is how to effectively teach students who have access to foundation-model-based tools. For example, it will be much more complex for teachers to understand the extent of a student’s contribution if the student worked together with a powerful generative model, or to regulate ineffective collaborations and detect plagiarism. Visual Studio has recently released GitHub CoPilot, an AI pair-programmer built upon GPT-3 [Chen et al. 2021f]. How will this change computer science education? Many challenges for beginner programmers might be trivial to CoPilot or its technical successors, which could undermine the learning experience for novices. It would be instructive to study other examples of technological advances that disrupted education for certain subjects, such as calculators in math classrooms and Google Translate in language courses, both of which now coexist with traditional instruction.
Privacy and security. One important ethical issue in the use of AI in education is highlighted by the strict legal guidelines concerning privacy in student work. For example, in the United States, student information is protected by the Family Education Rights and Privacy Act (FERPA). These laws and regulations are especially important for children under 13, who have their data privacy and security additionally protected by the Children’s Online Privacy Protection Act. Among other things, FERPA limits teachers from sharing personally identifiable student work. This could directly impact initiatives to share data used both for training and for evaluating foundation models. Moreover, there is an open question as to whether the weights of a foundation model could somehow leak the (possibly private) data it was trained upon [Nasr et al. 2018; Song et al. 2017]. These issues, and their corresponding approaches, are similar to the challenges described in §3.1: healthcare.
This list is not exhaustive and the ethical considerations will vary from project to project.

70

Center for Research on Foundation Models (CRFM)

3.3.2 Foundation models of student thought.
When building AI tools for inclusive, and joyful education, there are many tasks where foundation models could be useful. Many of those tasks require us to first understand the learners whom we are trying to help, especially in the context of open ended work.
What would it take for a foundation model to be able to reason about student understanding? It is easy to imagine a foundation model which has been adapted to answer a math question correctly, but it is less clear how to build a model that can diagnose mistakes in student understanding based on the student’s answers. To explore this theme, we consider the case study of providing feedback to students who are working on open-ended tasks such as writing a short paragraph, drawing a physics diagram, or writing code. This “feedback challenge” exemplifies how foundation models can be helpful off-the-shelf for learners, and also demonstrates open areas for foundation model research.
To effectively provide feedback to students, two central capabilities are required: (1) understanding the subject matter of the task (e.g., physics or coding), and (2) the diagnostic ability to “notice": a technical term in education for inferring why a student made a mistake. For typical student interactions in a typical classroom, there is not enough data for an AI model to learn, from scratch, both of these central capabilities. Even for massive courses with millions of students, supervised algorithms barely understand the complex student reasoning behind even short, four-line programs [Malik et al. 2021]. As such, the feedback task inherently requires a transfer of understanding from external data and experience.
Foundation models, as they currently exist, are directly helpful for the first of these capabilities: understanding a specific subject matter. For example, when learning to provide feedback on short programming questions, a foundation model such as GPT-3 can efficiently understand what fluent code looks like with a few examples. Some research in this direction has already started exploring foundation models that can quickly adapt to questions in new subject matter domains [Wu et al. 2021e; Condor et al. 2021]. Similarly, foundation models could also integrate multiple modes of information such as the text of a task’s prompt, diagrams in the question, or even the content of a grading rubric provided to teaching assistants. This unified representational ability can help foundation models comprehend a subject matter through richer sources of information. As a concrete case study, many of these insights were leveraged as core components of an algorithm which was able to grade an introductory Computer Science midterm at Stanford University, with the same effectiveness as human teaching assistants [Wu et al. 2021e]. In this case, subject matter encoding was built on a foundation model that had been adapted on GitHub code and a corresponding small dataset for each question’s subject matter. In general, we can imagine leveraging various sources of data to adapt foundation models to different subject matter. For example, math adaptation could use mathematical websites or textbooks [Shen et al. 2021b] or historical student answers on platforms such as Gradescope; spoken language understanding could leverage radio archives or podcasts; and domains like creative writing could look to large digital archives like Project Gutenberg.
In contrast to subject matter, adapting a foundation model to the task of mapping observed mistakes to flaws in a student’s thought processes is much less well-explored. The ability for an instructor to “notice” the reasons behind why a student makes a specific mistake is a critical component of the feedback challenge. Imagine, for example, a student learning two digit addition who answers the question “what is 26 + 19?" with the response “315." Take a moment and try to guess why they gave that answer and what misconceptions they have.45. This ability to notice could be posed as an adaptation task for foundation models (§4.3: adaptation) or perhaps even as a reasoning task (§2.4: reasoning).
45This student has made the common mistake of concatenating the results of adding the one’s digit and ten’s digit

On the Opportunities and Risks of Foundation Models

71

While difficult, training an AI system to notice is an achievable goal. Across classrooms, and across learning tasks in a given domain, there are generalizable patterns in how students arrive at their answers. The labeled data that can directly be used for this adaptation task, such as instructorwritten feedback to student work in [Wu et al. 2021e], are often held privately by instructors in disparate datasets. However, publicly accessible data, such as StackOverflow interactions, might also be creatively used to adapt a foundation model to notice. Some research has also explored effective ways of extracting, from instructors, generative descriptions of how students make mistakes [Malik et al. 2021; Gulwani and Singh 2013] — these hand-written generative models could also be used to generate adaptation data to help foundation models diagnose student mistakes.
3.3.3 Foundation models for instruction.
Reasoning about student understanding is an essential step towards a second objective: provide inclusive, high quality instruction. Computational approaches to instruction focus on different tasks like content personalization [Connor 2019], question generation [Guo et al. 2016; Willis et al. 2019; Srivastava and Goodman 2021], adaptive curriculum design [Mandel et al. 2014; Doroudi et al. 2017], and predicting instructor intervention [Chandrasekaran and Kan 2019; Alrajhi et al. 2021]. In this subsection, we discuss how foundation models could be useful in the act of teaching students.
Since effective teaching requires reasoning about student understanding, the previous discussions on understanding subject matter and “noticing” are extremely relevant. However, providing effective instruction requires an additional capability: that of understanding pedagogy [McKenzie 2003]. This encapsulates an effective understanding of techniques to guide a student, such as asking Socratic questions or providing analogies/contrasting cases; using encouraging or supportive language; tailoring the difficulty of questions to the student; and generating examples that are relevant to a student’s interests and background.
How can foundation models be adapted to understand good pedagogy for instruction? One idea is to consider adaptation using data source where instruction is the primary role. For example, data from question answering forums like StackOverflow could potentially be used to build a tutor which can parrot common Socratic questions. Similarly, a foundation model adapted on encyclopedias such as Wikipedia might be able to give answers to student questions which are (often) factually correct. There are also public data sources like textbooks, lecture videos, lesson plans, and graded feedback that collectively contain important pedagogical behaviours which could be adapted by foundation models (Figure 15).
Another adaptation challenge for instruction based on foundation model is to learn how to speak to students like teachers. The language used by teachers is often different from the language used by the general population. Teachers are ideally trained to speak to students with respect and in a way that intentionally helps them form a positive identity with the subject being learned [Truax 2018]. Cautionary examples like Microsoft’s 2016 Twitter bot “Tay," a chatbot that started generating hate speech within 24 hours of being deployed live, show us the importance of explicitly accounting for this factor in education. To train a language model which is more heavily influenced by professional teachers in classrooms, we could perhaps adapt foundation models to data sources like lecture videos or recorded office hour videos.
The adaptation problem above is compounded by the fact that different education contexts vary significantly in the kind of language that would be appropriate: for example, effective instruction in a 5th-grade science class would look quite different from that in a college physics class, much less a college literature class. This presents technical challenges beyond what would be faced in typical NLP domain shift settings (e.g., question answering based on news articles vs. Reddit posts), as the foundation model would need to be fluidly adaptable in terms of its tone and language, and not just the factual content that it generates.

72

Center for Research on Foundation Models (CRFM)

Beyond sound pedagogical techniques and instructional language, how might foundation models provide even more insightful forms of instruction? §2.1: language of this paper highlights the fact that remarkably complex language can be acquired by babies in a short amount of time. As the authors point out, a salient difference between foundation model training and human language acquisition is that “human language is grounded to the real world: for example, a baby’s caretakers point to objects while they talk about them." This same insight can also inspire ideas as to how foundation models can be used for generative education. Humans seem to learn well when presented with real-world analogies and contrasts which may be cross-cutting between their current context and past experiences. For example, when teaching sign language, an instructor might use an analogy such as "the hand shapes for the word ‘morning’ looks like the sun rising" or note that “the hand shape you just made look very similar to another word, so let us focus on the differences." As another example, when teaching Swahili to a learner who already knows Arabic and English, an instructor could point out that the Swahili word for 8 (pronounced nane) is a “false friend” that is phonetically similar to English word for 9 (pronounced nine). Foundation models that can integrate multi-modal data have the potential to make these kinds of rich analogies and comparisons that are typical in childhood language learning (Figure 16).

On the Opportunities and Risks of Foundation Models

73

4 TECHNOLOGY
The technological foundations of foundation models give rise to the capabilities (§2: capabilities) that determine their potential. To understand the technology used in development, we consider the data (§4.6: data), model architectures (§4.1: modeling) and systems (§4.5: systems) used to train (§4.2: training), and further adapt, (§4.3: adaptation) these models alongside the theory (§4.10: theory) that should be developed to understand this paradigm. To then understand the resulting models, we discuss how to evaluate (§4.4: evaluation) and interpret (§4.11: interpretability) alongside the importance of robustness (§4.8: robustness), security and privacy (§4.7: security), and long-term AI safety (§4.9: ai-safety) for ensuring the reliability of these models when deployed in society (§5: society).

74

Center for Research on Foundation Models (CRFM)

4.1 Modeling
Authors: Drew A. Hudson, Antoine Bosselut, Alex Tamkin, Omar Khattab, Jared Quincy Davis, Jiaxuan You, Trevor Gale

Fig. 17. The five key properties of a foundation model: expressivity — to flexibly capture and represent rich information; scalability — to efficiently consume large quantities of data; multimodality — to connect together various modalities and domains; memory capacity — to store the vast amount of accumulated knowledge; and compositionality — to generalize to new contexts, tasks and environments.
The emerging paradigm of foundation models has attained impressive achievements in AI over the last few years, as models such as BERT [Devlin et al. 2019] shine at a wide spectrum of language understanding tasks: from textual classification and entailment to question answering and reading comprehension, while GPT-3 composes rich and fluent tales about unicorns [Brown et al. 2020] and DALL-E shows signs of visual creativity, generating from scratch strikingly-realistic pictures of avocado chairs [Ramesh et al. 2021].
These and other instances of recent foundation models not only achieve remarkable performance across a multitude of diverse downstream tasks and applications [Rajpurkar et al. 2018; Wang et al. 2019a], but also manifest noteworthy behaviors of interpretability [Karras et al. 2020], robustness [Devlin et al. 2019], controllability [Patashnik et al. 2021] and generalization [Brown et al. 2020]. What does it take for a model to demonstrate such qualities? What architectures are capable of consuming large quantities of potentially multimodal information and translate them into rich knowledge of the world? And overall, what desirable properties should a network possess to give rise to a foundation model?
Here, we identify and discuss five such properties, spanning expressivity, scalability, multimodality, memory capacity, and compositionality, that we believe are essential for a foundation model in order to: (1) distill and accumulate knowledge from various sources and domains, (2) organize it in an effective and scalable representation, and (3) flexibly generalize it towards novel contexts. For each of these properties, we motivate their necessity, provide examples of contemporary models that incorporate them, and explore key challenges and promising avenues for future research and development. See Figure 17 for an overview diagram.

On the Opportunities and Risks of Foundation Models

75

4.1.1 Expressivity.
Expressivity concerns with the theoretical and practical capacity of a network to model the data distribution it is trained over and represent it in a flexible manner. Prior works have proposed formal expressivity measures to characterize the complexity of functions a network can compute, or more precisely, approximate, which is essentially affected by its depth, width, connectivity, and structural patterns [Raghu et al. 2017].
As the No Free Lunch theorem suggests, there is no single model or algorithm that suits best for all cases [Wolpert and Macready 1997], and so, for our purposes, we are particularly interested in identifying which models could effectively capture the facets of natural information, such as human language or real-world images [Goodfellow et al. 2016]. These modalities are either continuous (as in vision) or discrete (as in language), are distinctly hierarchical and high-dimensional, and present a complex set of relations and interactions among their constituent elements, whether these are pixels, words or physical objects.
Indeed, recent breakthroughs in generative modeling provide strong evidence for the high expressivity of neural networks, as they successfully express distributions of textual [Brown et al. 2020; Devlin et al. 2019; Lieber et al. 2021; Wang and Komatsuzaki 2021], auditory [van den Oord et al. 2016], and visual [Karras et al. 2020; Brock et al. 2018] domains, and generate samples of high fidelity, diversity and realism.

Inductive Biases. Much of the success of neural networks over the last decade in modeling natural data is owed to the networks’ high depths, as could be roughly measured by the number of stacked non-linear layers they are composed of, or the number of computational steps they take during their chain-of-reasoning. Great depths play a crucial role in enhancing networks’ expressivity, allowing them to form powerful hierarchical and distributed representations that could generalize from the training data to new unseen examples [He et al. 2016b; Levine et al. 2020].
The universal approximation theorem [Lu et al. 2019b] indeed states that even simple multilayer perceptrons (MLPs) can represent a broad set of functions, while different inductive biases, as those implemented in Recurrent Neural Networks (RNNs) or Convolutional Neural Networks (CNNs) [Goodfellow et al. 2016], can improve the learning efficiency and enhance the capacity of a given network to model different forms of information: sequential data, common to language, speech and time-series, for the former, or spatially-invariant information, as in images or videos, for the latter.

Transformer Networks & Attention. Meanwhile, transformer networks [Vaswani et al. 2017], introduced more recently, demonstrate the importance of capturing long-range dependencies and pairwise or higher-order interactions between elements. They build on the self-attention mechanism [Vaswani et al. 2017; Bahdanau et al. 2014] that enables shorter computation paths and provides direct means to compare elements far-across the input data (such as a pronoun and its antecedent in a sentence, or two sentences that refer to the same topic).
From another perspective, the multiplicative interaction embodied in both attention as well as gating structures (as in LSTMs [Hochreiter and Schmidhuber 1997] or Mixture-of-Experts [Shazeer et al. 2017]) offers a more flexible alternative to the rigid fixed-weight computation of MLPs and CNNs, dynamically adapting the computation to the input at hand. This proves especially useful for language modeling, where, for instance, given a sentence like “She ate the ice-cream with the X", while a feed-forward network would always process it in the very same manner, an attentionbased model could adapt its computation to the input — updating the contextual representation of the word “ate” if the prepositional phrase (PP) attachment X is “spoon”, or instead link it to the “ice-cream" if X refers e.g., to “strawberries" [Zavrel et al. 1997].

76

Center for Research on Foundation Models (CRFM)

General-Purpose Computation. A final notable advantage of attention over prior architectures stems from its stronger generality, where it is not strongly tied to a particular task or domain, as is the case for the local receptive field of convolution or the sequential assumption of recurrent networks, both reflecting inherent properties specific to the vision and language modalities respectively. We hypothesize that the general-purpose nature of attention and transformers contributes to their broad applicability for a wide range of research problems and applications [Liu et al. 2019; Dosovitskiy et al. 2020; Hudson and Zitnick 2021].
This contrast captures a more general trade-off between task-specialization and expressivity: models with stronger structural priors can leverage them to improve sample efficiency on the particular tasks that benefit from these assumptions; while conversely, models that integrate weaker inductive biases learn more slowly, but can in turn scale to higher volumes of data and adapt to a diverse set of domains, since they do not rely on restrictive or task-specific suppositions. As both data and compute turn more accessible, we observe that the exploration of models with a minimal set of inductive biases that can “let the data speak for itself" seems to serve as a more promising approach for future research in the field.

Challenges & Future Directions. Notwithstanding the stellar progress and accomplishments of neural networks in general, and foundation models in particular, in terms of expressivity, notable challenges still remain. Leading approaches [Choromanski et al. 2020; Dosovitskiy et al. 2020] keep struggling with modeling of extremely long-range dependencies, such as those occurring in books, movies, or even DNA sequences, which may be attributed to the quadratic computation of contemporary transformer-based approaches [Wang et al. 2020c; Lin et al. 2021].
This challenge essentially reflects the trade-off between efficiency and expressivity: where explicit modeling of long-distance interactions through short and direct computation paths improves expressivity on the one hand, but comes at the expense of scalability due to computation entailed by the increased connectivity on the other [Child et al. 2019; Kitaev et al. 2020; Choromanski et al. 2020]. Models such as the GANformer [Hudson and Zitnick 2021] and the Perceiver [Jaegle et al. 2021b,a] explore ways to balance these two properties and propose transformers with linear complexity that rely on bipartite or bottleneck attention, so to improve computational efficiency while maintaining high-expressivity. We believe that identifying an effective equilibrium between these two objectives offers an interesting avenue for future research.
Another important research direction relates to the expansion of foundation models, which, so far, have mainly focused on the language domain [Peters et al. 2018; Devlin et al. 2019; Brown et al. 2020], to different modalities, such as the structural [Scarselli et al. 2008; Veličković et al. 2017] and perceptual [Tolstikhin et al. 2021; Jaegle et al. 2021b; Tan and Le 2021], each involving a unique set of associated challenges. Likewise, we believe that exploring architectures for reasoning (§2.4: reasoning), which demands iterative computation chains and interaction with symbolic information, constitutes a valuable goal for future foundation models research.

4.1.2 Scalability.
Closely connected to model’s expressivity is the notion of scalability. As rich data from varied sources becomes more readily available, and computational resources get stronger and more efficient (§4.5: systems), we should look for ways to match this rate of progress and harness it to improve AI competency and versatility. For foundation models to effectively fit the complex and high-dimensional distribution of images or text, they should thereby be scalable across all dimensions: including both models’ depth and width as well as their training time, number of parameters, and amount of data they could process.

On the Opportunities and Risks of Foundation Models

77

Optimization. Specifically, foundation models should both be: (1) easy-to-train (§4.2: training), by being resilient to noise or imperfections in the data, and robust against instabilities like vanishing [Helfrich et al. 2018; Glorot and Bengio 2010] or exploding gradients [Hochreiter and Schmidhuber 1997; Nair and Hinton 2010], but also (2) easy-to-adapt (§4.3: adaptation), by overcoming phenomena of catastrophic forgetting [Kirkpatrick et al. 2017] and supporting few-shot learning [Sung et al. 2018]. We are still in the early days of understanding the principles that drive the scalability of learning algorithms, and while recent works have started to shed some light on these themes [Liu et al. 2020c; Kuditipudi et al. 2019; Nakkiran et al. 2019], much work remains to be done.
Hardware Compatibility. Moving beyond aspects of robustness and optimization, foundation models should also be practically efficient (§4.5: systems), and take advantage of contemporary and future hardware [Hooker 2020]. One example of that is parallelizablity, an important property that characterizes the computation supported by GPUs. Indeed, much of the transformers’ great success over the previously dominating recurrent approach was driven by their higher degree of parallelism.
Looking forward, given the fast-pace progress of systems development, we should further ensure that models are designed to co-adapt to future hardware advances. Consequently, foundation models should ideally be amenable to schemes such as distributed training, which is gaining popularity, as is the case for e.g., Mixture-of-Experts, and possibly leverage properties such as sparsity of the computation or representation, as is the case for the Longformer [Beltagy et al. 2020], BigBird [Zaheer et al. 2020], and Sparse Transformer [Child et al. 2019] approaches, and which likely will become more central in future hardware and processors.
4.1.3 Multimodality.
Traditionally, the fields of computer vision, robotics, and NLP have made progress in an independent manner, with separate communities developing specific approaches suitable for each modality. A conducive consequence the rise of deep learning has brought about was the bridges it helped forming among the various communities and research areas within AI, as seemingly different problems could now be tackled by closely-related approaches, and studies of originally remote topics began converging to a common ground. This breakthrough opened up a new range of possibilities, fostering pioneering exploration into the theme of multimodality, encompassing areas as varied as language grounding [Lynch and Sermanet 2020], visual semantics [Conser et al. 2019], embodied environments [Savva et al. 2019a] and interactive agents [Gray et al. 2019].
Essentially, multimodality serves as a key component of intelligence, and is a crucial factor for the development of both thorough and broad comprehension of the world. Concretely, language learning is more effective when occurring in a grounded environment rather than in a vacuum. And inversely, from the vision perspective, language encourages the emergence of abstractions that link between low-level perceptual signals and statistics to semantic concepts of objects, properties, agents and motivations, thereby enriching and elevating visual representations.
In light of these observations, we argue that foundation models should ideally connect together the different modalities, distill their embodied information into a shared multifaceted representation, and capture the full range of inter-connections and relations among them so as to furnish a wide range of capabilities (see §2.1: language, §2.2: vision,§2.3: robotics, §2.4: reasoning).
Generality and Specialization. An important design choice for multimodal foundation models is the degree of specialization, or the structural sharing between the modules responsible for each modality. Naturally, data of different domains exhibits diverse kinds of structures and properties — where, for instance, language is discrete while vision is continuous. At first sight, this variation hints that specialized inductive biases tailored for each modality could be of aid. Yet, as training scales

78

Center for Research on Foundation Models (CRFM)

upwards and models are provided with the opportunity to base their learning less on structural priors and more on the data itself, general approaches that maintain only a handful of broad general assumptions prove in fact a lot more successful than task-specific alternatives. And so, as corroborated by recent success of general-purpose models like transformers across different modalities — both linguistic [Liu et al. 2019; Lan et al. 2019] and visual [Dosovitskiy et al. 2020; Hudson and Zitnick 2021], we see that generality is critical for improving AI capabilities.
Multimodal Interactions. Another key consideration for multimodal models relates to weight sharing: do the various modalities benefit from using the same or different parameters for their respective components? Prior works have shown that fruitful transfer could certainly occur across modalities, but the ideal degree of sharing remains unclear, so is the existence of principled ways for discovering it.
Finally, a major design question concerns with the forms of the multimodal interactions supported by the model, which vary widely between concrete cases and examples: Cross-modal or late-fusion models such as ConVIRT [Zhang et al. 2020a] and CLIP [Radford et al. 2021] maintain fully separate encoders for each data source, and compare their spaces only at the ultimate computation stage, using e.g., a simple dot product. Meanwhile, early-fusion models, such as ViLBERT [Lu et al. 2019a; Cho et al. 2021], jointly reason over multiple modalities necessary for tasks of visual reasoning and question answering. Identifying the optimal stage and form for merging the respective vector spaces [Nagrani et al. 2021] remains an open research question.
Overall, while there seems to be a consensus within the community about the importance of multimodality, models that go beyond shallow alignment of vision and language are yet to exist, and the theme of grounded language learning in embodied environments still has much room for exploration.
4.1.4 Memory.
So far, we have discussed the foundation models’ goal to gather and accumulate information from varied modalities at large scales. This knowledge encompasses both broad understanding of the world as well as specific mastery of niche subjects or particular facts. Representing such a large body of learned information is by no means trivial, and is leading to interesting questions about effective mechanisms for access, storage, retrieval and manipulation of particular items or memories.
Explicit Storage. An important design principle that could achieve these desiderata is to separate out computation from memory [Weston et al. 2014; Graves et al. 2016; Hudson and Manning 2018, 2019a] in order to enhance models’ ability to transfer knowledge by applying previously acquired abstract skills to new concrete settings.
In this context, it is important to distinguish between explicit facts — that can be stored in an external memory storage, and implicit knowledge — that is reflected through the networks’ trainable weights. Such decoupling of explicit and implicit knowledge enjoys multiple advantages compared to the alternative of implicitly encoding all information together through the network weights. The separation mitigates the inflation in models’ size and number of parameters needed to store the growing quantities of knowledge [Guu et al. 2020], improves models’ trust and reliability by increasing their knowledge provenance [Cheney et al. 2009], and most notably, is key for memory update, manipulation or adaptation [Lewis et al. 2020b] (§4.3: adaptation), which could in turn enable generalization to novel contexts and downstream tasks.
Indeed, disentanglement between memory and computation has been a recurring goal in deep learning and NLP research over the last years, including models such as Memory Networks [Weston et al. 2014; Sukhbaatar et al. 2015], the Neural Turing Machine [Graves et al. 2014, 2016], the Neural State Machine [Hudson and Manning 2019a], and MAC [Hudson and Manning 2018]. Furthermore,

On the Opportunities and Risks of Foundation Models

79

using key-value structures [Miller et al. 2016] for accessing external memories has been shown to be very effective for modeling long-term dependencies [Henaff et al. 2016; Bosselut et al. 2018; Lample et al. 2019]. Transformers, the celebrated architecture underlying most foundation models to date, likewise exhibits operations that involve key-value memory-access and computation among the contextual word representations they gradually build [Geva et al. 2020].
Information Retrieval. Once a model completes gathering the information after training, there are multiple ways to retrieve particular facts or memories necessary for downstream applications and tasks. Some employ explicit prompting techniques that query the model’s knowledge through input sequences [Petroni et al. 2019; Kassner et al. 2021; Jiang et al. 2020] while other approaches involve implicit recollection and reshaping of the prior knowledge through an adaption phase [Bosselut et al. 2019; Hwang et al. 2021]. A third category of methods goes a step further and combines neural-based computation with symbolic aggregation and retrieval of information from either unstructured textual repositories [Karpukhin et al. 2020; Lewis et al. 2020b; Khattab et al. 2020] or even structured resources such as knowledge graphs [Zhang et al. 2019a; Peters et al. 2019; Liu et al. 2020e; Verga et al. 2020; Yasunaga et al. 2021].
However, there is trade-off between the strong memorization skills offered by retrieval mechanisms on the one hand and the richer representations learned when there is an information bottleneck on the other. Indeed, over-reliance on retrieval reduces the opportunities to learn how to represent information in compact and abstract manners, distill key insights and concepts out of the vast amounts of input information the model is exposed too, and, basically, separate the wheat from the chaff. For instance, the in-context learning abilities of GPT-3 possibly emerge as a by-product of enforcing the network to represent the input sequential data through its bounded memory architecture [Brown et al. 2020]. Overall, While they certainly have some merits [Guu et al. 2020], models that rely on external retrieval mechanisms may not learn to generalize as effectively as bounded, compact and abstract representations.
Knowledge Manipulation. Finally, when considering large-scale learning over long durations, it is crucial to note the dynamic nature of knowledge, where facts’ correctness and validity can change over time as the world keeps evolving — and what was true or relevant yesterday may not be so tomorrow. It is therefore crucial for a model to represent its knowledge in a manner that supports efficient update or manipulation of facts as part of its lifelong learning.
4.1.5 Compositionality.
Compositionality can be defined as the principle according to which the meaning of the whole is derived from the meaning of its constituent parts, and the rules applied to combine them [Janssen and Partee 1997; Bottou 2014]. It is a crucial ingredient of human intelligence [Lake et al. 2017], underlying our capabilities to plan, reason and learn readily and efficiently from a handful of examples. Compositionality may hold the key to achieve out-of-distribution — or specifically — combinatorial generalization. Drawing on classic ideas from symbolic AI, it encourages and enhances desirable properties within neural networks, such as interpretability, controllability and dataefficiency [Lake et al. 2017], and can take different forms, characterizing variety of elements:
Model. Compositionality can be reflected at the model level, in terms of its architectural properties, structure, and degree of modularity — which can increase training and inference efficiency of large neural models [Shazeer et al. 2017]. It also links to themes of interpretability and multimodality, as it relates to the interfaces between the different modules the model is composed of, what modes of interactions they employ, and how transparent they are.

80

Center for Research on Foundation Models (CRFM)

Computation. Models such as Module Networks [Andreas et al. 2016] and Mixture-of-Experts [Shazeer et al. 2017] go further along this direction, exhibiting not only structural modularity, but also compositional computation, supported by the specialization of sub-networks to different operations, in a manner that adapts and tailors the model behavior to the input at hand. While some methods rely on concatenation of hand-engineered modules [Andreas et al. 2016], alternative approaches enable the network specialization to naturally emerge through learning [Shazeer et al. 2017]. Other models, such as MAC [Hudson and Manning 2018] and Dynamic Memory Networks [Xiong et al. 2016] perform an explicit iterative computation, where a given task is decomposed into multiple reasoning steps, performed one by one, manifesting sequential progression from a set of initial facts to novel inferences and conclusions.
Training & Data. Not only can the model or its computation be compositional, but so can be the data or training processes too [Andreas 2020]. Instead of training one model over a complete dataset, one could split, or decompose it into subsets, train different models on each one independently, and ultimately recombine them at test time through various ensemble techniques [Dietterich 2000]. Such approaches could have far-reaching implications on the training and deployment procedures of foundation models, in both practical and even societal regards.
Representation. We have discussed compositionality of different elements, such as the model, the computation, the training schemes or the data. But most notably, the learned representation itself, which emerges over the course of the model training and adaptation, can also be compositional [Andreas 2019]. Indeed, a promising manner to represent knowledge is through structured, potentially graph-based, object-oriented representations [Zhang et al. 2019a; Wang et al. 2021a], that center around identifying entities and event nodes and forming connections, analogies and relation edges among them. It reflects a natural way to organize information about the world, where inputs from different modalities can be channeled and aggregated around semantic multi-faceted concepts. Such representations could support multi-hop reasoning and inference [Washington et al. 1995; Sun et al. 2020b; Yu et al. 2020c], and potentially also enable stronger out-of-distribution generalization through recombination.
However, compositionality can also hinder the expressivity of the representation, and impede its capacity to account for idiosyncrasies, exceptions, and contextual correlations [Misra et al. 2017a]. In other words, the whole can sometimes be greater than the sum of its parts, where for instance, red wine is not the same as red onion. But while many approaches that have dominated over the last decade tend to focus mostly on one end of the spectrum, and learn monolithic distributed representations, we believe that exploring manners to reach a better balance between contextuality and compositionality is a promising avenue for future research.
4.1.6 Summary.
We have introduced five properties that we believe are essential for the next generation of foundation models, in order to effectively distill the large amounts of information around us so to successfully address downstream tasks: expressivity — to flexibly capture and assimilate real-world information, scalability — to adeptly handle high volumes of high-dimensional data, multimodality — to consume, process and potentially produce content from different sources and domains, memory capacity — to effectively store and retrieve the acquired knowledge, and finally, compositionality, to foster successful generalization to novel tasks, settings and environments. We believe that the realization of the full potential of foundation models, as is envisioned and discussed in detail throughout this report, will rely on research of new architectural and modeling advances to fulfill these desiderata.

On the Opportunities and Risks of Foundation Models

81

4.2 Training
Authors: Alex Tamkin
Training objectives are mathematical functions describing how to transform a model architecture and large amount of broad data into a foundation model. For example, GPT-3 was trained with a language modeling objective, which rewards the model for predicting the next word correctly [Shannon 1948]. We begin by laying out some goals of these training approaches, describe important design trade-offs in current approaches, and outline important goals for the path ahead.
4.2.1 Goals of training objectives.
Here we outline some key goals for training algorithms in light of the recent rapid progress in these methods and models.46
Leveraging broad data. The rise of self-supervised learning algorithms has unlocked the power of internet-scale datasets which would be intractable to annotate by hand. This kind of broad data comes in many forms, including images, audio recordings, and video (§2.2: vision); robotic and sensor data (§2.3: robotics); and text, either in isolation or paired with other modalities like images (§2.1: language). Because this data lacks external annotations, a major focus for researchers is designing bespoke self-supervised algorithms that leverage the unique structure within each kind of data to produce a training signal for a foundation model.
Domain completeness. An important goal for foundation model training algorithms is to be domain complete, in the sense that solving the training task requires capabilities that are broadly useful for downstream tasks in the domain (see §2.1: language, §2.2: vision, §2.3: robotics). This property is crucial for the generality of a foundation model. For example, language modeling may require models to acquire capabilities as wide-ranging as coreference, sentiment and translation as the model learns to predict the next word in a document. In contrast, a supervised learning task like sentiment classification may lead to a more narrow set of capabilities (see §2.1: language). As important as this quality is, it is not obvious a priori what tasks will result in a domain complete capabilities, or even how to evaluate the full breadth of a model’s capabilities (see §4.4: evaluation and §4.10: theory).
Scaling and compute efficiency. Procedures for training foundation models must reliably convert data, a model architecture, and compute into a broadly capable model. To maximize the capability of a foundation model, we can identify the bottlenecks to this process and propose new training algorithms which remove them. The rise of self-supervised algorithms has made model size and compute resources increasingly salient bottlenecks [Kaplan et al. 2020; Henighan et al. 2020], leading to a shift where models are evaluated not solely on their capabilities but rather on the amount and kind of compute needed to reach those capabilities (§4.4: evaluation). The efficiency of training objectives can vary tremendously,47 laying in sharp relief how important the design of a training approach is to the emergence of powerful capabilities given a fixed compute budget. Thus, a major goal for training researchers is to design training objectives with a richer training signal, resulting in models which learn faster and attain stronger capabilities.48 One force aiding this development is the surprising predictability of how capabilities scale with different kinds of architectures, data
46We use “training" instead of pretraining to emphasize the primacy of the foundation model itself, and because some methods for adapting foundation models to downstream tasks do not involve any later stage of training.
47e.g., 4x for ELECTRA [Clark et al. 2020] vs BERT [Devlin et al. 2019], 12x for contrastive vs generative approaches to CLIP training [Radford et al. 2021]
48Of course, a key goal for computer systems designers is to alleviate compute as a bottleneck for training (see §4.5: systems) And the choice of a training method is ultimately also constrained by the availability of diverse, high-quality data

82

Center for Research on Foundation Models (CRFM)

sizes, and compute [Hestness et al. 2017; Kaplan et al. 2020], a striking phenomenon which enables model developers to make choices based on clearer trends instead of more costly random searches.
4.2.2 Design trade-offs in current SSL methods.
Current self-supervised learning (SSL) methods for training foundation models are diverse, but what unites them is that they produce prediction problems from unlabeled data without the need for human annotators. SSL objectives manufacture a rich training signal from this data through carefully-designed constraints, either on the data itself (e.g., redacting or noising) or on the way the model is able to represent or process the data (e.g., latent bottlenecks). At some level, these constraints “bake in" the kinds of capabilities desired when adapting models to downstream tasks (§4.3: adaptation).49
Here, we describe three important design choices that current models explore, along with their respective tradeoffs in terms of their resulting capabilities.
At what level of abstraction should we model? A fundamental question is what the input representation of a foundation model should be. One option is to model the input at the level of raw bytes. However, this high dimensionality may cause the model to focus on predicting less semantic aspects of the input,50 slowing the rate at which it acquires more generally-useful capabilities. These approaches also become intractable when training models like transformers [Vaswani et al. 2017] whose compute costs grow quadratically with the input size.51 Another option is to use domain knowledge to reduce the input space of a model — such strategies include patch embeddings [Dosovitskiy et al. 2020] as well as fixed or learned tokenization [Schuster and Nakajima 2012; Sennrich et al. 2016; Kudo and Richardson 2018; van den Oord et al. 2017; Ramesh et al. 2021]. These methods may alleviate some challenges facing generative approaches, but have the trade-off that they may jettison possibly-useful information in the input.52 The choice of a continuous vs discrete input also has trade-offs for adaptation (§4.3: adaptation); more work is needed to capture the benefits of both approaches.
Generative vs discriminative models Generative training approaches are conceptually elegant yet powerful — they train models to learn joint or conditional distributions over training inputs. Two major families of generative foundation models include autoregressive foundation models [van den Oord et al. 2016; Radford and Narasimhan 2018; Chen et al. 2020d; Yang et al. 2019; Ramesh et al. 2021], which generate inputs piece by piece, and denoising foundation models [Devlin et al. 2019; Raffel et al. 2019] which corrupt and then recover the inputs. The specific kind of generation performed in the training process determines what kind of interactivity is available

(§4.6: data), which continues to be a major challenge for many domains, including robotics (§2.3: robotics) and low-resource
languages (§2.1: language) 49For example, the causal language modeling objective used to train GPT-3 [Brown et al. 2020] enabled conditioning it
via prefixes. And the color jitter augmentations used during contrastive learning [Chen et al. 2020c] encourage invariance
to properties not thought to be useful for downstream tasks. Better understanding how the particular choice and structure
of these constraints influences the capabilities acquired by the model is an important area for future work (§4.10: theory). 50e.g., blades of grass, audio compression artifacts, or spellings of words 51See §2.2: vision and §4.1: modeling for discussions of training costs for high-dimensional sequences, such as images
and video 52For example, tokenizing text may make it harder to learn rhymes, puns, or other tasks that benefit from character-level
information [Branwen 2020]

On the Opportunities and Risks of Foundation Models

83

during adaptation53 (see §2.5: interaction and §4.3: adaptation), and future models may enable an even richer set of interactions.54
While generative training approaches have their benefits, several discriminative approaches have also recently gained traction. These methods do not enable generation-based interaction, yet they may enable more efficient learning for classification- or regression-based tasks in high-dimensional continuous settings like images, audio, and video. Most of these methods output vectors for (parts of) inputs, which are trained to be similar for different “views” of an input [Wu et al. 2018; van den Oord et al. 2018; Chen et al. 2020c; He et al. 2020; Grill et al. 2020; Caron et al. 2021; Zhang et al. 2020a; Radford et al. 2021] or used to predict whether parts of inputs are real or fake [Clark et al. 2020; Iida et al. 2021]. Better understanding the trade-offs between generative and discriminative training, as well as capturing the best of both approaches, remain interesting avenues for future study.
Capturing multimodal relationships. Another increasingly important research area is capturing the relationships between multiple kinds of data. What this means may differ based on the context and the goals of a modeler. For example, CLIP [Radford et al. 2021] and ViLBERT [Lu et al. 2019a] are both multimodal vision-language, but differ in the precise way they are multimodal.55 The former encodes images and text separately into vectors, enabling users who have examples from a single modality to retrieve, score, or classify examples from the other modality. The second processes images and text jointly at an early stage of the model, enabling downstream applications like visual question answering where reasoning over pairs of related images and text (e.g., images and questions about them) are provided. Multimodal foundation models remain a nascent research area; much is still unexplored about the different ways a model can be multimodal as well as better understanding the capabilities these additional modalities bring.
4.2.3 Paths forward.
We close with some important goals for the future of foundation model training.
Out-of-the-box SSL Right now, SSL objectives are highly domain-specific: different methods currently prevail in natural language processing, computer vision, and speech processing. This has two major disadvantages: First, these different techniques make it challenging to grasp the common threads and scientific principles underlying why each of these methods work. Second, this domain-specificity requires developing new foundation model training methods from scratch for each new field, including medical, scientific, and new multimodal settings. A more general objective for efficiently training foundation models on any kind of data would represent a significant milestone for the foundation model training community [Tamkin et al. 2021b].
Obtaining a rich training signal It is clear that not all training objectives are made equal — some are radically more efficient than others, translating into far more capable foundation models for a given compute budget. Are there training methods orders of magnitude more efficient than those currently known? If so, how can we find them? These investigations will be shaped by many forces, including what future software and hardware advances (§4.5: systems) make possible. We also need not view data (§4.6: data) and training algorithms as independent factors: not only does the
53For example, autoregressive models like GPT-3 enable prefix-based conditioning, while denoising models like T5 or BERT facilitate the use of bidirectional context to replace arbitrary-length spans or fix typos.
54Other kinds of generative approaches less studied in a foundation modeling context include diffusion and score-based models [Sohl-Dickstein et al. 2015; Song and Ermon 2019; Ho et al. 2020], VAEs [Kingma and Welling 2014], flow models [Dinh et al. 2015; Kingma and Dhariwal 2018], and GANs [Goodfellow et al. 2014] — it remains to be seen whether these or other future approaches can also enable learning as diverse variety of capabilities as autoregressive or denoising approaches.
55See §2.2: vision and §2.1: language for more discussion of multimodality in vision and language specifically

84

Center for Research on Foundation Models (CRFM)

quality and availability of the data influence the training signal,56 but the training algorithm itself could adaptively seek out or construct richer training examples as the model improves to accelerate learning [Tamkin et al. 2021c].
Goal-directed training of foundation models. Adaptation methods such as prompting (§4.3: adaptation) draw on emergent properties that result almost as an afterthought of training. Can we train foundation models where the ability to understand and reliably carry out goals in a complex world is part of the model’s training objective? A focus on developing general capabilities distinguishes this direction from the goal of adapting an existing foundation model to a specific task via reinforcement learning (e.g., Stiennon et al. [2020]). Instead, one might imagine more sophisticated versions of current methods which acquire a diverse range of real-world capabilities from raw online [Klyubin et al. 2005; Singh et al. 2005; Salge et al. 2013; Shakir Mohamed 2015; Florensa et al. 2017; Pathak et al. 2017; Haber et al. 2018] or offline [Precup et al. 2000; Lange et al. 2012; Ajay et al. 2021; Yang and Nachum 2021; Schwarzer et al. 2021] interactions, without the need for human annotations or task construction. Such methods might use techniques quite similar to existing SSL algorithms: e.g., training sequence models in goal-directed contexts where they can be directly asked to carry out certain tasks via conditioning (e.g., UDRL [Schmidhuber 2019; Srivastava et al. 2019] or Decision Transformer [Chen et al. 2021b]; also see §2.3: robotics). The complex behaviors that have already emerged in simple interactive environments [Baker et al. 2020] suggest multitask, multiagent, and multimodal goal-directed training of foundation models as an interesting avenue for future study.

56Including any undesirable or biased capabilities (§5.1: fairness)

On the Opportunities and Risks of Foundation Models

85

4.3 Adaptation Authors: Xiang Lisa Li*, Eric Mitchell*, Sang Michael Xie, Xuechen Li, Tatsunori Hashimoto

Fig. 18. During adaptation, a foundation model is converted into an adapted model (bottom row) in order to reflect updated information, desired behaviors, or deployment constraints.
While foundation models provide a powerful general-purpose engine for processing multi-modal information, adapting a foundation model before use is necessary for some applications. Broadly, an adaptation procedure produces an adapted model by conditioning a foundation model on additional information, either by priming the foundation model through the inclusion of new data or a prompt in its input or by updating some or all of the foundation model’s parameters to reflect the new information. For example, in text summarization, appending a prompt such as TL;DR to the input article can improve foundation model performance [Radford et al. 2019] by acting as a task specification for the foundation model. Alternatively, fine-tuning the parameters of a foundation model with an organization’s internal, domain-specific data could improve the model’s accuracy by adding information relevant to the organization’s use case. In this section, we describe existing approaches to adaptation and several factors that determine whether a particular adaptation procedure is appropriate for a particular setting. We additionally describe various use cases for foundation model adaptation, including relatively well-studied settings such as specialization of a foundation model to a particular task or domain as well as more speculative settings like test-time data removal [Bourtoule et al. 2019] and editing model behavior on particular inputs [Sinitsin et al. 2020]. We conclude by presenting a long-horizon goal for future research in foundation model adaptation.
4.3.1 Methods for foundation model adaptation.
Many methods for adapting foundation models have been proposed, making the decision of which adaptation procedure to use for a particular problem or compute environment difficult. We emphasize three factors of particular importance for practitioners to consider when selecting an adaptation procedure: (1) the compute budget (specifically storage and memory); (2) the amount of task-specific data available; and (3) extent of access to foundation model gradients.

86

Center for Research on Foundation Models (CRFM)

Factor 1: Compute budget. For foundation models with billions or trillions of parameters, finetuning all model parameters may demand prohibitively large memory. Also, separately fine-tuning for many tasks can incur unacceptable storage costs. There are many works that propose methods to reduce the storage for adapting foundation models, and we refer to this class of lightweight adaptation methods as low-storage adaptation. Typically, methods in this class freeze most of the pretrained foundation model parameters and only learn a relatively small number of task-specific parameters (either by fine-tuning some pretrained parameters or by adding altogether new modules), reducing the storage overhead for each task.
The key design decision for such algorithms is the choice of parameters to be adapted. Perhaps the simplest approach is to tune only the final layer of the pretrained model, while other works tune only the model’s bias vectors [Zaken et al. 2021], low-rank residuals to model weight tensors [Hu et al. 2021], or masks over model parameters [Zhao et al. 2020b]. Another line of research tunes “soft” prompts [Li and Liang 2021; Qin and Eisner 2021; Liu et al. 2021e; Lester et al. 2021; Hambardzumyan et al. 2021], corresponding to sequences of arbitrary parameter vectors rather than embeddings of the model’s vocabulary, and conditions the foundation model on these prompts by concatenation with input activations, either at the input layer or at all layers. Another approach freezes all model parameters and interleaves new MLP modules with trainable parameters between existing model layers [Houlsby et al. 2019]. While these lightweight adaptation techniques seem to trade off parameter efficiency and performance on downstream tasks, they sometimes achieve comparable performance to full fine-tuning, despite updating 1000× fewer parameters [Zaken et al. 2021; Li and Liang 2021; Hu et al. 2021]. Lester et al. [2021] shows an instance when the performance gap between full fine-tuning and lightweight adaptation vanishes as the model size increases. We remain uncertain how lightweight adaptation techniques scale as model size increases [Aghajanyan et al. 2020]. Because GPU memory is typically a more limiting resource than disk storage, low-memory adaptation procedures are perhaps even more critical to democratizing foundation models than low-storage adaptation procedures. Various techniques for low-memory model training have been proposed, which can be directly applied to fine-tuning-based adaptation procedures (§4.5: systems). However, some low-memory procedures such as gradient checkpointing [Chen et al. 2016] trade off computation and memory, potentially exacerbating the significant energy consumption of foundation models [Bender et al. 2021].
Factor 2: Data availability. Task specialization mostly demands task-specific labeled data as training signals.57 However, the cost of annotation varies greatly across tasks and languages; for example, annotating MRI data requires expert medical knowledge, whereas labeling sentiment for English texts requires only common sense judgement. When adaptation data is plentiful, we might apply traditional fine-tuning approaches or their lightweight counterparts. In low-resource language-based settings, combining prompting and fine-tuning has been shown to be a promising direction [Schick and Schütze 2021a,b; Gao et al. 2020c; Perez et al. 2021; IV et al. 2021; Min et al. 2021]. Le Scao and Rush [2021] shows that a well-tuned prompt can be worth around 100 training examples, and fine-tuning a carefully prompted foundation model is significantly more data-efficient than fine-tuning an unconditioned foundation model.
Factor 3: Access to foundation model gradients. Despite the significant impact of foundation models on some research communities, little standardization of distribution practices exists for large scale foundation models (with more than 50 billion parameters). As we gradually become aware of the potential hazards from the misuse of foundation models (see §5.2: misuse), providing access to all
57Prompts are an exception, although we might consider prompts to implicitly represent the information contained in a batch of labeled data [Le Scao and Rush 2021].

On the Opportunities and Risks of Foundation Models

87

of a foundation model’s parameters for fine-tuning might result in ethical concerns. Moreover, most users do not have enough compute resources to exploit their full access. For example, the memory requirements of foundation models might preclude their direct fine-tuning for many organizations and institutions. Consequently, future foundation model providers would likely restrict access to the full parameters of the model and instead provide surrogate API access, as exemplified by an early foundation model, GPT-3. On one extreme, if a foundation model provider enables access only to the model output (e.g., the textual continuation of a prompt, the generated image, or a score evaluating the alignment between an image and a textual description), foundation models can be adapted using in-context learning [Brown et al. 2020]. In-context learning freezes the foundation model parameters, and steers the output of the foundation models by conditioning on a (typically natural language) prompt, which might be composed of task instructions or demonstrations. To improve the performance of in-context learning, the prompt needs to be carefully designed, either by manual search or an automated procedure [Jiang et al. 2020; Shin et al. 2020], and validated on the adaptation data. At the other extreme, if the foundation model provider grants access to gradients with respect to model parameters, full fine-tuning can be applied, where all model parameters are updated to improve performance on a downstream task. As a middle ground, we might obtain gradient access only to foundation model inputs,58 which are considerably lower dimensional than foundation model parameters. In this case, we could deploy lightweight adaptation techniques [Liu et al. 2021e; Li and Liang 2021; Lester et al. 2021], which freeze the model parameters and optimize a continuous prefix or prompt for each task.
4.3.2 Use cases for adaptation.
Adaptation is useful whenever the desired use case of a model differs from the relatively general training objective used for foundation model training (§4.2: training). Most commonly considered is the case in which a foundation model is adapted to perform a specific task (e.g., text summarization or animal classification from images), narrowing the scope of the model. Indeed, the vast majority of existing approaches described earlier in this section have targeted this setting. However, other forms of adaptation are useful, such as making local model edits to correct undesirable predictions for particular inputs or adding privacy constraints to the trained foundation model, which are task-agnostic. In this subsection, we describe a variety of use cases for adaptation, the methods that are most applicable to them, and remaining challenges in addressing these settings.
Task specialization. The most widely-studied case of foundation model adaptation is that of task specialization, in which a foundation model is adapted to optimize performance for a specific task or set of tasks. For example, specializing for summarization tasks would induce foundation model behavior to extract key ideas from the input document(s) and re-organize them in short summary sentences. Various adaptation procedures have proven effective for task specialization, showing significant improvement over performance of the unadapted model [Howard and Ruder 2018; Brown et al. 2020]. In addition to the relatively widely-studied setting of specializing foundation models to specific tasks, other task-agnostic adaptation problems become increasingly challenging (but no less important) for foundation models owing to their size and computational demands. For example, the cost of training foundation models makes continual training over time to keep a model’s predictions up to date with current events particularly expensive. Additionally, the challenge of collecting massive anonymized datasets used to train foundation models (§4.6: data) make the likelihood of personal information leakage into training sets non-trivial; mechanisms to efficiently remove training data from a foundation model post-facto are therefore desirable.

58Assuming the foundation model provider enables the input space to be continuous.

88

Center for Research on Foundation Models (CRFM)

Temporal adaptation. Ideally, foundation models store knowledge that closely represents the state of the world, independent of modality. However, the world is constantly changing; new heads of state are elected, clothing styles change, social norms and beliefs shift (§5.6: ethics), and the use of language evolves, causing a shift in the input distribution, target predictive distribution, or both. This temporal shift presents a challenging statistical problem due to the induced distribution shift, as discussed in §4.8: robustness. For foundation models, temporal shift also presents a particularly difficult computational problem; due to the computationally demanding nature of training foundation models [Shoeybi et al. 2019; Brown et al. 2020], frequent re-training from scratch might carry unacceptable financial or environmental impacts [Bender et al. 2021] (§5.3: environment), or simply take too long to be a viable method for keeping models up to date. In visual domains, gradual self-training on unlabeled data across intermediate time points can bridge the temporal shift across a long time period, but remains an expensive retraining procedure [Kumar et al. 2020a].
In the context of language models, temporally-partitioned diagnostic datasets help quantify the rate at which large language models become outdated [Lazaridou et al. 2021; Hombaiah et al. 2021; Dhingra et al. 2021], showing that classic techniques like re-weighting training data and dynamic evaluation (updating model parameters with new data at production time [Mikolov et al. 2010]) can partially alleviate, but not fully solve, this problem. Explicitly conditioning a language model on the time period it is intended to model is another technique that has shown some promise [Dhingra et al. 2021]. An alternative strategy to addressing temporal shift is to design retrievalbased (semi-parametric) models, which augment the model input with additional context retrieved from a large, human-interpretable database (e.g., Wikipedia articles) [Karpukhin et al. 2020; Lewis et al. 2020b; Guu et al. 2020; Khandelwal et al. 2020; Khattab et al. 2020]. For retrieval-based models, adaptation corresponds to updating individual units of information in the database (e.g., single paragraphs of encyclopedia articles) without re-training the model. While promising, challenges for retrieval-based approaches remain both in training an accurate retrieval mechanism and in accurately conditioning on the retrieved information [Lewis et al. 2020b]. We revisit the problem of temporal adaptation in the more general context of continual learning later in the section.
Domain specialization. In addition to task specialization, it is often necessary to specialize a foundation model to a particular domain (such as legal documents), without limiting the breadth of tasks the foundation model can accomplish. This specialization induces a mismatch between the foundation model training and adaptation data distributions (§4.8: robustness) which may require new adaptation methods to handle. Prior works have found that diverse and general pretraining data can cause negative transfer for current adaptation methods. For example, Cole et al. [2021] shows that fine-tuning a model pretrained only on the iNaturalist animal classification dataset provides better downstream performance than fine-tuning a model pretrained on iNaturalist along with 750K other images; similarly LegalBERT [Chalkidis et al. 2020], which is pretrained only on legal documents, improves over BERT [Devlin et al. 2019], which is trained on a much more diverse training set on the downstream task of text classification and sequence tagging in legal documents. One approach to domain specialization is to include an intermediate adaptation step, where the foundation model continues training on unlabeled data from the specialized domain. For instance, this approach improves the downstream performance significantly for satellite images and specialized text topics [Reed et al. 2021; Gururangan et al. 2020]. However, continual foundation model training may perform worse than re-training from scratch in certain domains such as legal documents [Chalkidis et al. 2020]. Elucidating the scenarios in which continual training does or does not benefit performance is an important direction for future work.
Local model editing. In some settings, it is useful to adapt a foundation model locally, meaning that the model’s predictive distribution should be adapted only for a single input or a local

On the Opportunities and Risks of Foundation Models

89

neighborhood around a single input, without changing the model’s behavior for unrelated inputs. For example, when a foundation model produces an especially problematic mistranslation for a particular input phrase and target language, it is desirable to correct this mistranslation without affecting the model’s behavior for unrelated phrases. Past work has studied the problem of applying approximately localized updates to large neural networks through new pretraining objectives that enable easy editing with standard gradient descent [Sinitsin et al. 2020], higher-order networks that predict parameter edits for an underlying model [Cao et al. 2021; Mitchell et al. 2021; Hase et al. 2021], and constrained fine-tuning procedures [Zhu et al. 2020]. However, existing methods vary in the reliability with which they can perform model edits without damaging global model performance. Furthermore, scaling these methods to massive foundation models is not straightforward due to their size and the computational cost of training objectives that require computing higher-order gradients [Sinitsin et al. 2020; Cao et al. 2021; Mitchell et al. 2021].
Applying constraints. There are settings in which foundation models need to be adapted to satisfy privacy constraints. For instance, Carlini et al. [2021] demonstrated that existing foundation models are able to memorize sensitive information in the training data and can regurgitate such data when queried via standard APIs. While this phenomenon calls for improved data curation, developing adaptation procedures which eliminate or reduce the influence of specific data examples on the trained model would be a complementary solution. Improved adaptation strategies (along with better pretraining methods) in this direction will also benefit institutions working with foundation models under the General Data Protection Regulation (GDPR), as the mandate gives users the right to be forgotten. While research on the topic of machine unlearning [Bourtoule et al. 2019; Cao and Yang 2015] has started to gain traction, the problem has not yet been studied in depth for foundation models. In addition, foundation models trained on less curated internet data have been shown to exhibit harmful biases targeting specific groups (e.g., gender and racial bias) [Bender et al. 2021; Basta et al. 2019; Kurita et al. 2019; Sheng et al. 2019] and can produce toxic outputs [Gehman et al. 2020] (§5.2: misuse). While strategies such as further fine-tuning the foundation model on carefully curated datasets (for potentially multiple generations) [Solaiman and Dennison 2021] or applying controllable generation techniques [Keskar et al. 2019] have shown some success in mitigating harmful behavior, a framework for training equitable and safe foundation models (§5.1: fairness) will likely require further research with a collective effort across the data collection, training, and adaptation phases as well as consultation with domain experts.
4.3.3 A long-term goal for foundation model adaptation research.
To the extent that adaptation is concerned with efficiently integrating a model’s existing knowledge with new data or objectives, a natural extension of adaptation is continual learning [McCloskey and Cohen 1989; Parisi et al. 2019] or continual adaptation. The ability to adapt a foundation model continually is desirable, whether to keep a model’s knowledge continually up-to-date with world events or cultural developments, continually add data from completely new domains or modalities as they become available, or continually edit a model’s memories to comply with privacy or legal constraints as a society’s values or laws evolve. However, continual learning problems typically induce catastrophic forgetting [McCloskey and Cohen 1989; Ratcliff 1990; Kirkpatrick et al. 2017] in neural networks, where old tasks or data are rapidly forgotten as the training distribution changes.
We consider continual adaptation of a foundation model as a grand challenge for future foundation model adaptation research. Tackling this challenge requires closing the performance gap between a foundation model trained continuously on a non-stationary stream of data from different tasks, domains, or time periods and the same foundation model trained from i.i.d. data from the aggregate mixture [Lopez-Paz and Ranzato 2017]. Innovations in both model architectures and training

90

Center for Research on Foundation Models (CRFM)

objectives are likely to be necessary in order to do so. For example, while memory mechanisms have long been speculated as key to successful continual learning [French 1999] and have shown some promise for foundation models [Lewis et al. 2020b; Guu et al. 2020; Borgeaud et al. 2021], effectively leveraging updated memories remains a challenging problem [Zhang and Choi 2021]. In addition, techniques for localizing knowledge in a foundation model in order to make targeted parameter updates [Dai et al. 2021a] or learning such update rules [Cao et al. 2021; Mitchell et al. 2021] may help prevent forgetting, but repeated application of such updates still induces significant forgetting [Hase et al. 2021]. Continual foundation model adaptation may also require new understanding of how the problem of catastrophic forgetting manifests at the scale of foundation models as well as developing new meta-learning techniques [Schmidhuber 1987; Santoro et al. 2016; Finn et al. 2017] to improve forward transfer of already-learned information to new settings.
Continually training on experiences gathered by deployed foundation models, or even training on aggregated experiences gathered by many different models, may speed the rate of progress of foundation model development, but incur risks of feedback loops and eroded alignment of model behavior and stakeholder values. Despite the aforementioned challenges, continual foundation model adaptation holds the promise of more rapidly responding to shifts in socio-cultural values, better leveraging existing knowledge to learn new concepts, lessening the environmental impact and increasing the accessibility of foundation models by eliminating the computational burden of training from scratch, and reducing the extent that previously-learned concepts must be re-learned due to forgetting.

On the Opportunities and Risks of Foundation Models

91

4.4 Evaluation Authors: Rishi Bommasani, Kawin Ethayarajh, Omar Khattab

4.4.1 Introduction.
Evaluation gives context to machine learning models: it serves as a means for (1) tracking progress — how do we we measure the performance of models and how do we design improved models (§4.1: modeling); (2) understanding — what behaviors do models exhibit (§4.11: interpretability) and how do they perform on different slices of data (§4.8: robustness); and (3) documentation — how do we efficiently summarize model behavior and communicate this to diverse stakeholders. For foundation models, each of these purposes for evaluation are critical but the nature of foundation models introduces new challenges that are not generally encountered in other AI or ML settings:
(1) Tracking progress requires relative comparison, but comparing foundation models is complicated by the fact that foundation models must be adapted (potentially in different ways) to perform tasks.
(2) Understanding requires specified in-advance knowledge (e.g., taxonomies) of what is being evaluated for, but foundation models acquire emergent skills (e.g., in-context learning) that will be difficult to anticipate in designing evaluations.
(3) Documentation requires clear desiderata to meaningfully inform decision-making, but foundation models can be adapted for myriad applications, which makes comprehensive documentation challenging.
To orient the discussion of evaluating foundation models, we distinguish two classes of evaluation that arise from the abstraction of foundation models: intrinsic evaluation of the foundation model, which is inherently divorced from a specific task due to the task-agnosticity of these models, and extrinsic evaluation of task-specific models, which is necessarily dependent on both the foundation model and the adaptation mechanism. Further, we recognize that due to the anticipated impact and scope of foundation models, a variety of stakeholders (e.g., foundation model providers and application developers, auditors and policymakers, practitioners and researchers) will require evaluation of both foundation models and task-specific derivatives, with these evaluations serving different purposes and involving different desiderata based on the stakeholder. With this in mind, standard paradigms for the evaluation of machine learning models are not designed explicitly for the setting of foundation models. Therefore, we emphasize intrinsic evaluation (§4.4.2: evaluationintrinsic), the importance of adaptation in extrinsic evaluation (§4.4.3: evaluation-adaptation), and evaluation design (§4.4.4: evaluation-design) as clear steps towards an evaluation framework that is better suited to foundation models. This discussion contributes to broader dialogue surrounding the role of evaluation of machine learning systems [Galliers and Spärck Jones 1993; Lipton and Steinhardt 2019; Ribeiro et al. 2020; Linzen 2020; Kiela et al. 2021; Milli et al. 2021; Jacobs and Wallach 2021; Bowman and Dahl 2021; Dehghani et al. 2021; Ma et al. 2021a, inter alia] and, given the complexities of evaluation, may benefit from drawing upon theories of measurement and evaluation that exist beyond machine learning [Messick 1987; Jackman 2008; Loevinger 1957; Messick 1988; Hand 2010; Brewer and Crano 2014].
4.4.2 Intrinsic evaluation.
Evaluation of machine learning systems has traditionally been grounded in tasks, often ones that are envisioned as functions specifically useful for applications (e.g., translation, object recognition). In contrast, since foundation models are intermediary assets that must be further adapted or specialized to perform useful tasks, the standard evaluation paradigm must be altered to facilitate the direct understanding and comparison of foundation models.

92

Center for Research on Foundation Models (CRFM)

One approach is to evaluate foundation models in terms of the task associated with the training objective. For example, a language model like GPT-3, which was trained by predicting the next word given the preceding context, may be evaluated based on the probabilities it assigns words given their preceding context in held-out test data (i.e., perplexity on language modelling benchmarks like LAMBADA [Paperno et al. 2016]). This approach has shown promise in NLP thus far, but we identify two fundamental limitations it exhibits. First, relying on the training objective for evaluation lacks generality: foundation models trained using different incompatible objectives cannot be readily compared or understood in a consistent frame. Second, evaluation in this way relies upon a proxy relationship to be meaningful, i.e., measurements in terms of the training objective should correlate with other more meaningful and intelligible quantities (e.g., the quality of content generated via a foundation model). While this proxy relationship has proven to be robust in the past in some contexts, it likely will break down when assessing more diverse capabilities of foundation models, their behavior in more diverse environments or domains, and considerations beyond in-domain accuracy (we discuss this more extensively in §4.4.4: evaluation-design). In light of these limitations, we anticipate that two approaches will need to be considered, offering complementary benefits.
Imputing intrinsic evaluation from broad extrinsic evaluation. One route towards evaluating foundation models is to adapt them to a wide range of tasks and measure the performance of the resulting task-specific models. As the foundation model is the shared basis across all of these models, the performance in aggregate reflects on the nature, and quality, of this shared basis. At present, many subareas of AI have begun to construct meta-benchmarks, i.e., a single evaluation that consolidates individual evaluations across a number of different tasks or domains [Wang et al. 2019b,a; Hu et al. 2020; Santurkar et al. 2020; Gehrmann et al. 2021; Hendrycks et al. 2021b; Koh et al. 2021; Tamkin et al. 2021b]. Given the growing adoption of this paradigm and its established strengths, here we note why it is likely insufficient to fully satisfy the goals of evaluations with respect to foundation models. Meta-benchmark evaluation requires adaptation (minimally to specialize the foundation model to each of the tasks in the meta-benchmark), which makes reasoning about the foundation model itself challenging given the addition process (i.e., adaptation) involved. Specifically, this complicates matters of progress, both in terms of tracking (e.g., is performance attributable to potent foundation models or well-designed adaption practices) and in terms of identifying improvements in the process used to learn foundation models (e.g., fundamental improvements in data selection (§4.6: data), training objectives (§4.2: training), and model architectures (§4.1: modeling) may be difficult to identify by comparing the performance on a meta-benchmark between two foundation models). In addition, this evaluation paradigm makes it difficult to understand or document properties and capabilities specific to the foundation model, which may make it unwieldy to convey to certain stakeholders (e.g., SuperGLUE performance may not be sufficiently informative, or may be misleading, for policymakers) or use as grounds for anticipating their behavior for new tasks or domains.
Direct evaluation of intrinsic properties. To complement the use of meta-benchmarks, we also argue for why measuring the properties (e.g., specific capabilities or biases) of foundations models directly is valuable, divorced from specific tasks.59 For example, we may endeavor to directly measure the linguistic capabilities of foundation models to identify syntactically valid and invalid sentences. To motivate the value of this approach, we return to the purposes for evaluation. Notably,
59Strictly speaking, these direct evaluations may still involve formulation as a task and foundation model specialization to perform the task, but the objective is more akin to probing (see §4.11: interpretability) of trying to measure the foundation model as directly as possible.

On the Opportunities and Risks of Foundation Models

93

articulating the presence and intensity of capabilities, skills, and biases identifies concrete areas for improvement (progress), elucidates the current potential (understanding), and expresses relevant aspects efficiently (documentation). Such an approach also is in service of broadly comprehensible evaluation, i.e., evaluation that can be understood by both technical experts, non-technical experts (e.g., policymakers or social scientists) and the general purpose. For example, characterizing the persuasive or rhetorical capabilities of these models may especially intuitive for internalizing their potential for disinformation and misuse (§5.2: misuse) [Buchanan et al. 2021].
Direct evaluation of properties also serves as an important pathway towards better handling of the emergent properties of foundation models; to demonstrate this, we take in-context learning as a case study. In particular, Brown et al. [2020] not only demonstrated GPT-3’s signature capability of robust in-context learning, but also were the first to specifically identify in-context learning as a specific way to adapt and interact with models (through their exploration of GPT-3). Traditional task-based extrinsic evaluation does not provide a clear means by which in-context learning could have been identified; directly interacting with the foundation model appears to be necessary in this case. More generally, while it appears inevitable that many unanticipated phenomena like in-context learning will be recognized through the unstructured or loosely structured exploration of these models and their capabilities, we believe new approaches to evaluation should be sought out that structure this exploration or, more ambitiously, suggest new properties that can then be more rigorously tested for. Intrinsic evaluation may also lower the threshold for demonstrating the potential of foundation models; new approaches for foundation models may be sufficiently promising if they demonstrate improvements in intrinsic evaluation, even if they are not immediately accompanied by corresponding well-suited adaptation methods for eliciting these capabilities in extrinsic evaluation.
There is a significant open question of how intrinsic evaluation should be implemented; the mechanics of such evaluation are unclear. We enumerate a few general principles and considerations that may help inform the design and execution of intrinsic evaluation.

(1) Inspiration from evaluation of humans. Many of the relevant properties, capabilities, and biases we are interested in for foundation models are also of interest for humans, which suggests that methods for measuring these properties in humans may prove to be instructive, or even directly translatable, for evaluating foundation models. For example, psycholinguistic measures of human linguistic competencies can be modified to evaluate foundation model linguistic competencies [Levy 2008; Frank et al. 2013; Linzen et al. 2016; Ettinger and Linzen 2016; Marvin and Linzen 2018; van Schijndel and Linzen 2018; Futrell et al. 2019; Prasad et al. 2019; Ettinger 2020] or psychological measures of human social biases can be modified to evaluate foundation model social biases [Greenwald et al. 1998; Caliskan et al. 2017; May et al. 2019; Guo and Caliskan 2021].
(2) Human-in-the-loop evaluation. Human-in-the-loop evaluation may prove to be critical to provide a more exploratory means for understanding foundation models, including assessing their generative or interactive capabilities. In particular, human interaction with foundation models directly may better identify their emergent capabilities and limitations and direct auditing of foundation models [e.g., Raji and Buolamwini 2019, §5.6: ethics] may advances goals for documentation and transparency.
(3) Validity of intrinsic measures. While intrinsic measures allow for direct measurement at the source, i.e., measurement and evaluation of the properties of a foundation model independent of adaptation and specific tasks, they pose challenges for building trust in the validity [Messick 1987, 1988] of the evaluation. In particular, extrinsic evaluation outcomes may also be important in validating intrinsic measure design, e.g., the predictive validity of intrinsic

94

Center for Research on Foundation Models (CRFM)

measures (i.e., their ability to (statistically) predicted related downstream outcomes) may prove to be a central criterion.
4.4.3 Extrinsic evaluation and adaptation.
Evaluating task-specific models has historically involved reporting the performance (generally meaning the accuracy) of the model on a specific held-out test set. While this paradigm may partially suffice to understand or document a model, it often amounts to unfair comparisons between taskspecific models produced with different (and, potentially, unequal) resources, making it difficult to gauge how much progress has been made. The concern of unfair comparisons is exacerbated in the foundation model regime: different foundation models (e.g., BERT and GPT-3) may form the foundation for different task-specific models, and these foundation models may involve vastly different amounts of training data and computation.
To account for the resources required to achieve specific levels of performance, Linzen [2020] argues that (pre)training resources should be acknowledged and tracked in evaluation. We believe this is a scientifically principled proposal; comparing different approaches for training foundation models without accounting for training resources is likely to be misleading. However, given that the process for creating foundation models is especially expensive (e.g., requiring significant human and financial capital), and often governed by societal factors (e.g., commercial incentives) in addition to scientific factors, it may be the case that the foundation models in practice will vary greatly in the training resources afforded, making controlled comparison difficult. Here, we consider an alternative, which may be more pervasively viable, to partially account for the resources involved to complement the proposal of Linzen [2020]. In particular, we consider why extrinsic evaluation should acknowledge adaptation resources, which is critical for ensuring that extrinsic evaluation is able to identify the most performant adaptation methods (which intrinsic evaluation, fundamentally, cannot do). We draw attention to the fact that adaptation resources often are construed as the data used to adapt models, but additional resources [e.g., data used to choose adaptation methods; Perez et al. 2021] and constraints (e.g., the level of access required to adapt the foundation model; see §4.3: adaptation and §5.6: ethics for further discussion) should also be accounted for.
Accounting for adaptation resources. Accounting for the resources expended to adapt foundation models for specific tasks requires a complete understanding of what resources or constraints are used for different adaptation methods, i.e., evaluations that endeavor to account for these resources must evolve alongside developments in what resources are used in adaptation (§4.3: adaptation). In existing task-specific evaluations, most evaluations specify the amount of data that can be used to adapt a (foundation) model to the task. However, Perez et al. [2021] identify a key nuance here that has been discounted in past work, in that this should encapsulate all data used to inform adaptation, i.e., both the data used to adapt the foundation model and the data used to choose the adaptation method. Further, in the foundation model regime, the notion of access requirements for different adaptation methods is also a new consideration that should be factored into evaluation. Concretely, some adaptation methods may generally outperform others but may require greater ability to access or modify the foundation model compared to others (e.g., fine-tuning requires foundation model gradients to modify a foundation model, whereas prompting may only require blackbox access in specifying inputs).
Accounting for the resources involved in adaptation enriches what conclusions can be reasonably drawn from evaluation of task-specific models. At present, task-specific evaluation may provide sufficient clarity for certain types of understanding or documentation of particular task-specific artifacts (i.e., the exact models being evaluated) but do not provide clear signal for how different adaptation methods perform and how to select a specific adaptation method in a given context. In

On the Opportunities and Risks of Foundation Models

95

contrast, by accounting for the resources and access requirements involved in adaptation, evaluation better enables research to identify which adaptation methods or processes make best use of the resources provided, i.e., signal is offered not just for the specific artifacts being evaluated but the more general processes by which they were derived. The proposed evaluation protocol, therefore, clearly works towards identifying which adaptation methods should be used; we note that all of these conclusions should always be taken as specific to a given foundation model, as evaluation in this form does not provide sufficient evidence to conclude an adaptation method is uniformly the best across foundation models.60
4.4.4 Evaluation design.
In theory, the goal of evaluation is to measure and characterize various theoretical constructs (e.g., accuracy, robustness (§4.8: robustness), fairness (§5.1: fairness), efficiency (§4.5: systems), environmental impact (§5.3: environment)) in service of various purposes (i.e., progress, understanding, documentation). However, in practice, the utility of evaluation will be determined by how evaluations are designed and executed. For example, automated measurements of the generative capabilities of foundation models (e.g., their factual correctness) may poorly capture the nature of these qualities and, instead, human-in-the-loop evaluation may better contextualize these capabilities.
In considering the evaluation design we envision for foundation models and their adapted derivatives, we begin with the mechanics of evaluation. Traditionally, the evaluation of machine learning models has involved a large training set that is used to learn the model, an optional validation set that is used to set hyperparameters, and a test set to evaluate the generalization of the learned model to held-out data [Bishop 2006]. As a result, creating benchmarks to evaluate models has historically required large amounts of data, most of which is allocated towards training, which complicates the design of certain diagnostic or nuanced evaluations when data is scarce or expensive to attain [Rogers 2020, 2021]. In contrast, because the benefits of foundation models will often coincide with the sample efficiency of adaptation (i.e., few-shot or zero-shot capabilities) and the diversity of possible applications, we instead envision a regime where benchmarks for individual tasks are much smaller (since far less data needs to be provided as “training", i.e., adaptation, data) and are far more diverse (both to capture various capabilities in intrinsic evaluation and more strongly ground evaluation in ecologically valid ways [Bronfenbrenner 1977; de Vries et al. 2020] during extrinsic evaluation). This suggests that the nature of foundation models may cause a shift in nature of benchmarks (and the mentality of those constructing benchmarks), de-emphasizing quantity as a key priority in benchmarks as opposed to quality and diversity. The NLP community has begun to see the beginnings of such a regime with expansive and diverse benchmarks like BIG-Bench61 and FLEX [Bragg et al. 2021]; this paradigm lowers the barrier for benchmark design, thereby enabling the broader community to partake in evaluation design.62
Alongside the mechanics of evaluation, the presentation of and interface to the evaluation results informs how these results will be used inform decision-making (e.g., new modelling approaches, model selection, auditing). Leaderboards have become the de facto paradigm in machine learning, whereby models are ranked by a specific and singular criterion (generally a form of accuracy). This approach has generally led to significant and rapid progress in system quality over time [e.g., Wang
60Current results, instead, suggest that different adaptation methods are better-suited to different types of foundation models and training objectives [Liu et al. 2021e; Lester et al. 2021].
61https://github.com/google/BIG-bench 62Traditionally, the design of benchmarks like ImageNet [Deng et al. 2009] and SQuAD [Rajpurkar et al. 2016] has been conducted by high-resourced research labs that can afford to pay for the creation of these datasets through crowdsourcing [Rogers 2020].

96

Center for Research on Foundation Models (CRFM)

et al. 2019a], but significant concerns have been raised of whether this yields more general improvements [e.g., Linzen 2020; Bowman and Dahl 2021].63 As is true for all machine learning models, it is rarely the case that the desiderata for foundation models and their derivatives will be singular; instead, we anticipate the breadth of their application and societal impact necessitates heightened consideration of criteria beyond accuracy (e.g., robustness, fairness, efficiency and environmental impact). To this end, we note that evaluation of foundation models should report measurements across these diverse fronts; existing benchmarks are increasingly designed to reflect more than just accuracy (e.g., robustness [Koh et al. 2021; Goel et al. 2021], fairness [Nadeem et al. 2021; Nangia et al. 2020], efficiency and environmental impact [Coleman et al. 2017]). Further, we note that if the reporting of performance across this different categories is done in the form of a leaderboard, mechanisms to disambiguate potential trade-offs (to induce a ranking) will be especially necessary [Ethayarajh and Jurafsky 2020]. In particular, since different stakeholders will have different preferences (e.g., the weight they ascribe to different properties) and values [Birhane et al. 2020], leaderboard design should allow stakeholders to interact and manipulate how the ranking is done to align with their values; Ma et al. [2021a] presents an early attempt to enable this by comparing the utility of models using an economic framing based on a user’s specified utility function.
4.4.5 Takeaways.
Evaluation performs several roles (i.e., progress, understanding, documentation) that are vital for all machine learning paradigms, including the foundation model paradigm. Foundation models introduce new challenges for existing evaluation frameworks; designing evaluations that directly target the foundation model regime will better serve not only the multiple purposes of evaluation, but also the myriad of stakeholders involved.
(1) While machine learning evaluation traditionally has considered task-specific models, evaluating foundation models involves engaging with the fact that these models are not specific to a task. Evaluation of these models likely will involve integrating two complementary approaches: (a) imputing the properties of foundation models from broad evaluation of task-specific derivatives and (b) direct measurement of these properties in foundation models.
(2) Existing evaluation frameworks often do not account for the resources required to create the models being evaluated, leading to unfair comparisons. For foundation models, we discuss an evaluation paradigm that emphasizes accounting for adaptation resources (e.g., all data used in adaptation, access requirements for the foundation model), which appears to lead to more informative evaluations that better shape how adaptation is conducted.
(3) Existing evaluation design often is limited in the diversity of metrics considered and requires large adaptation datasets. For foundation models, we echo growing calls for evaluation to consider a broader range of desiderata (e.g., robustness, fairness, efficiency, environmental impact) to capture the wide range of stakeholder values/preferences, as well highlight how the sample efficiency of adapting adaption models may allow for more diverse evaluations by re-allocating resources involved in designing evaluations.

63We note the connection to Strathern’s Law [Strathern 1997] (sometimes referred to as Goodhart’s Law [Goodhart 1984]): “When a measure becomes a target, it ceases to be a good measure."

On the Opportunities and Risks of Foundation Models

97

4.5 Systems
Authors: Deepak Narayanan, Trevor Gale, Keshav Santhanam, Omar Khattab, Tianyi Zhang, Matei Zaharia

Fig. 19. Plot showing the growth of number of parameters and number of training operations (FLOPs) of
transformer-based language models (shown in blue), and memory capacity and peak device throughput
of NVIDIA P100, V100, and A100 GPUs (shown in red) with time. The rate of growth (slope of each line) of state-of-the-art language models (roughly 10× a year) far exceeds the rate of increase in computational capacity of hardware (roughly 10× in four years), motivating the need for parallelism across a large number of accelerators and co-design of algorithms, models, software, and hardware to drive further progress. Number
of parameters and number of training operations are obtained from relevant papers [Brown et al. 2020], and
memory capacities and peak throughputs are obtained from GPU specification sheets.
Computer systems are one of the largest bottlenecks to developing foundation models. Foundation models are frequently too large to fit in the main memory of a single accelerator (e.g., GPU) and require an immense amount of computation to train (e.g., > 1000 petaFLOP/s-days for GPT-3 [Brown et al. 2020]). Additionally, these models will likely get larger over time: for instance, the compute and memory requirements of state-of-the-art language models have grown by three orders of magnitude in the last three years, and are projected to continue growing far faster than hardware capabilities (Figure 19). Once trained, these large models are expensive to perform inference with and difficult to debug, monitor, and maintain in production applications. We believe that further advances in the performance and usability of foundation models will require careful co-design across algorithms, models, software, and hardware systems, as well as new interfaces for programming and deploying ML applications. In this section, we discuss the key computer systems challenges in developing and productionizing large-scale foundation models.
4.5.1 Improving performance through co-design.
Today, training large-scale foundation models [Brown et al. 2020; Rae et al. 2021; NVIDIA and Microsoft 2021; Wang and Komatsuzaki 2021] can often require custom software systems such as Megatron, DeepSpeed, or Mesh Transformer JAX [Shoeybi et al. 2019; Rasley et al. 2020; Wang 2021], built on top of standard frameworks like PyTorch, TensorFlow, and JAX [Paszke et al. 2019; Abadi et al. 2016; Bradbury et al. 2018]. These software systems rely on a number of innovations across the stack to train models efficiently at scale: new parallelization dimensions such as pipeline

98

Center for Research on Foundation Models (CRFM)

parallelism [Huang et al. 2019; Narayanan et al. 2019] that limit communication while keeping devices busy, state-sharding optimizers to reduce memory usage [Rajbhandari et al. 2020], just-intime (JIT) compilers to optimize the computation graph [PyTorch 2021], and optimized libraries like cuDNN and NCCL [NVIDIA 2021]. Megatron and DeepSpeed are efficient to a particular scale; for example, Megatron can extract up to 52% of the theoretical peak throughput of modern hardware with approximately 3000 GPUs on a model with a trillion parameters [Narayanan et al. 2021b]. However, scaling to larger models with more GPUs still is challenging, since existing parallelization strategies break down at larger GPU counts. Data parallelism is limited by the batch size [Li et al. 2020e], pipeline parallelism by the number of layers in the model [Huang et al. 2019; Narayanan et al. 2019], and tensor model parallelism by the number of GPUs in a single server [Shoeybi et al. 2019].
While we will continue to realize performance gains from new hardware, growth in the resource requirements of large models far outstrips generational hardware improvements [Brown et al. 2020]. To facilitate the next major leap in model capacity and to democratize the advances in model quality, it will be increasingly critical to co-design training algorithms, models, software, and hardware, because many of the avenues to dramatically increase performance alter the semantics of the training computation. For example, executing operations in lower precision (such as fp16) can help increase throughput on modern hardware (e.g., the V100 and A100 GPUs have dedicated tensor core units for lower-precision matrix multiplication), but also affect the numerics of the optimization procedure [Micikevicius et al. 2017]. Similarly, exploiting weight sparsity can significantly improve training and inference times [Elsen et al. 2020; Gale et al. 2020] by only performing mathematical operations on the non-zeros in the model, but requires different training algorithms [Jayakumar et al. 2021; Evci et al. 2020; Dettmers and Zettlemoyer 2019]. Other examples of co-design include model architectures that map more efficiently to hardware [So et al. 2019; Child et al. 2019; Wang et al. 2020c; Lee-Thorp et al. 2021; Kitaev et al. 2020; Beltagy et al. 2020; Tay et al. 2020; Ren et al. 2021], efficient optimizers [Anil et al. 2020; Shazeer and Stern 2018], novel tokenization alternatives [Xue et al. 2021; Tay et al. 2021], specially architected hardware training platforms [Jouppi et al. 2017; Mudigere et al. 2021; Selene 2021], and distributed parallelization strategies with relaxed weight update semantics [Narayanan et al. 2019, 2021a].

Case study: efficient knowledge representation. As a concrete case study of successful codesign, retrieval-based models such as REALM, RAG, ColBERT-QA, and RETRO [Guu et al. 2020; Lewis et al. 2020b; Khattab et al. 2020; Borgeaud et al. 2021] take a different approach to model design than simply increasing the number of model parameters. Instead of trying to accumulate implicit knowledge from ever-larger datasets directly into a DNN model with billions of parameters (like GPT3), retrieval-based models store knowledge outside the model parameters in the form of text passages, capturing knowledge within the passages with dense vector representations. These models then use scalable top-𝑘 search mechanisms to extract knowledge pertinent to each input, while keeping the DNN model itself small (§4.1.4: modeling-memory). This design improves computational efficiency as well as maintainability of the model in production: for example, developers can update the knowledge of the model just by replacing a text passage, without needing to retrain a large DNN.
Retrieval-based models have achieved promising initial results by leveraging several new crossfunctional ideas, including backpropagating the loss through the retriever during training [Guu et al. 2020] (which requires approximating the gradient through a knowledge store consisting of millions of passages) and modeling fine-grained interactions between queries and passages [Khattab and Zaharia 2020; Khattab et al. 2020] (which requires decomposing the computation into vector-level nearest-neighbor search operations). These techniques allow retrieval-based models to be accurate

On the Opportunities and Risks of Foundation Models

99

and efficient, but demand functionality not readily supported by popular ML frameworks and nearest-neighbor indexes such as FAISS [Johnson et al. 2019].
4.5.2 Automated optimization.
Another important challenge in systems is to automate the application of optimizations that straddle algorithms, models, software, and hardware. While many optimizations and parallelization strategies are complementary, identifying the most effective combination of optimizations is challenging since the joint search space grows combinatorially and optimizations interact in non-trivial ways [Narayanan et al. 2021b]. Foundation models heighten the need for automated optimization as manual experimentation is extremely expensive and time-consuming at the scale of thousands of GPUs.
Recent work in this area has focused on systems targeting semantics-preserving optimizations. In particular, systems have been proposed to automatically discover mathematically-equivalent graph substitutions [Jia et al. 2019a; Wang et al. 2021c], facilitate the distributed execution of operator graphs through both high-level APIs and low-level compilers [Rasley et al. 2020; Mandeep Baines 2021; Bradbury et al. 2018; Shazeer et al. 2018; Lepikhin et al. 2020], and automate the selection of hybrid distribution strategies [Jia et al. 2019b; Santhanam et al. 2021]. These systems have helped deploy many foundation models in industry [Fedus et al. 2021; M2M-100 2020; Turing-NLG 2020].
Unfortunately, automated optimization becomes much harder when composing semanticsaltering optimizations (§4.5.1: systems-co-design), as it is often unclear how to jointly model the statistical impacts of these techniques (e.g., how many training iterations are needed to reach a specific accuracy?). We will therefore need new software tools, libraries, and compilers to automatically identify compositions of optimizations that target comprehensive metrics like timeto-accuracy [Coleman et al. 2017; Mattson et al. 2020]. Building such tools will require tight collaboration between systems and machine learning experts.
4.5.3 Execution and programming models.
The unique multi-task nature of foundation models provides an opportunity to amortize training and inference costs over many applications. In particular, paradigms such as adaptation mean more sharing across model instances [Raffel 2021]. For example, two models prefix-tuned [Li and Liang 2021] from the same pretrained model can share the same model “stem,” reducing the storage footprint (the shared stem only needs to be stored once), while also making it possible for execution to be shared and batched across the prefix-tuned models [Shen et al. 2019; Narayanan et al. 2018]. Consequently, the specific adaptation mechanism used informs system optimization (§4.3: adaptation).
It is an open question as to what programming interface should be used to specify that various adapted models are derived from the same pretrained model (e.g., models 𝑌 and 𝑍 are derived from the same pretrained model 𝑋 ), or that various components of two models share parameters (e.g., two models 𝐴 and 𝐵 share the same stem up to layer 𝑖). Ludwig [Molino et al. 2019] and PyTorch’s Module offer easy ways to compose functionality within a model, but no system today supports cross-model dependencies. Giving users the opportunity to provide annotations will allow training and inference systems to optimize and orchestrate computation more efficiently; without such annotations, systems will not have visibility into what computation and parameters can be shared across model instances. A model’s “adaptation history” (what models is this particular model adapted from) can also be used for debugging: an adapted model’s errors on particular types of inputs could originate from the pretrained model, pointing to issues in the pretraining process versus adaptation process. Frameworks like PyTorch, as well as software libraries for

100

Center for Research on Foundation Models (CRFM)

training foundation models such as HuggingFace Transformers [Wolf et al. 2020], do not allow for fine-grained lineage information across entire model instances to be specified.
Building and maintaining a cluster of thousands of accelerators also requires tremendous effort. New training paradigms like Learning@Home [Ryabinin and Gusev 2020; Diskin et al. 2021] explore leveraging volunteer compute over the internet to train foundation models collaboratively. Such fundamentally new execution models can decrease the cost of training for any one entity, but require collaboration across a number of different areas like security (to ensure that a malicious volunteer cannot significantly alter the training process), distributed systems (to deal with fault tolerance issues as volunteers drop), and crowdsourcing.
4.5.4 Productionization of foundation models.
As the community continues to push the capabilities of foundation models, realizing their potential will require addressing the challenges associated with deploying these resource-intensive models in production. These challenges include performing model inference with tight latency targets, and ensuring that models and data are monitored in an automated way.
For applications with strict cost and latency constraints, model compression techniques like distillation [Hinton et al. 2015; Li et al. 2020d; Sanh et al. 2019], quantization [Polino et al. 2018; Gholami et al. 2021; Zhou et al. 2018], pruning [LeCun et al. 1990; Gordon et al. 2020; McCarley et al. 2019; Wang et al. 2019c; Sajjad et al. 2020], and sparsity [Gale et al. 2020; Elsen et al. 2020] could aid deployment by transforming larger models to obtain desired inference-time properties. These techniques were originally intended for smaller models (e.g., BERT-L) in low-memory environments (e.g., mobile phones), but are now necessary to handle the extreme scale of modern foundation models in datacenter deployments. Parallelization techniques like tensor model parallelism [Shoeybi et al. 2019], traditionally used for training, might also be useful to reduce inference latency, and also provide additional memory capacity across GPUs to fit the model’s parameters.
In addition to these practical constraints, increases in the size and complexity of foundation models and the datasets used to train them pose new challenges to model and dataset lifecycle management. Since models with a large number of parameters are hard to manually inspect by humans, we need better systems for automated dataset curation (§4.6: data) and model quality assurance. Techniques like behavioral testing [Ribeiro et al. 2020] and model assertions [Kang et al. 2020] facilitate easier model maintenance in production by providing analogs to unit tests, runtime monitoring (in the form of test-time assertions), and continuous model improvement (as new inputs come in) for models deployed in end applications. These tools can help address issues of fairness and bias (§5.1: fairness), and reduce model mispredictions.

